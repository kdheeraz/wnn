{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6801955,"sourceType":"datasetVersion","datasetId":3909889}],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline,make_pipeline\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Bidirectional\nfrom matplotlib import pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras import backend as K","metadata":{"execution":{"iopub.status.busy":"2024-07-06T04:03:58.565126Z","iopub.execute_input":"2024-07-06T04:03:58.565472Z","iopub.status.idle":"2024-07-06T04:04:07.422974Z","shell.execute_reply.started":"2024-07-06T04:03:58.565445Z","shell.execute_reply":"2024-07-06T04:04:07.422003Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"CFG = [\n    '/kaggle/input/software-effort-estimation-datasets/albrecht.csv',\n    '/kaggle/input/software-effort-estimation-datasets/china.csv',\n    '/kaggle/input/software-effort-estimation-datasets/desharnais.csv',\n    '/kaggle/input/software-effort-estimation-datasets/finnish.csv',\n    '/kaggle/input/software-effort-estimation-datasets/isbsg10.csv',\n    '/kaggle/input/software-effort-estimation-datasets/kemerer.csv',\n    '/kaggle/input/software-effort-estimation-datasets/kitchenham.csv',\n    '/kaggle/input/software-effort-estimation-datasets/maxwell.csv',\n    '/kaggle/input/software-effort-estimation-datasets/miyazaki94.csv'\n]","metadata":{"execution":{"iopub.status.busy":"2024-07-06T04:04:18.975680Z","iopub.execute_input":"2024-07-06T04:04:18.976806Z","iopub.status.idle":"2024-07-06T04:04:18.981654Z","shell.execute_reply.started":"2024-07-06T04:04:18.976767Z","shell.execute_reply":"2024-07-06T04:04:18.980650Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"code","source":"def data_albrecht():\n    df = pd.read_csv(CFG[0])\n    df_for_training = df.astype(float)\n    #LSTM uses sigmoid and tanh that are sensitive to magnitude so values need to be normalized\n    # normalize the dataset\n    scaler = MinMaxScaler()\n    scaler = scaler.fit(df_for_training)\n    df_for_training_scaled = scaler.transform(df_for_training)\n    return df\n\n\ndef data_china():\n    \n    df = pd.read_csv(CFG[1])\n    df = df.drop(columns=['id','ID'])\n    df_for_training = df.astype(float)\n#     print(df.columns)\n    #LSTM uses sigmoid and tanh that are sensitive to magnitude so values need to be normalized\n    # normalize the dataset\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    scaler = scaler.fit(df_for_training)\n    df_for_training_scaled = scaler.transform(df_for_training)\n    \n    trainX = []\n    trainY = []\n    n_future = 2   # Number of days we want to look into the future based on the past days.\n    n_past = 2 # Number of past days we want to use to predict the future.\n\n    for i in range(n_past, len(df_for_training_scaled) - n_future +1):\n        trainX.append(df_for_training_scaled[i - n_past:i, 0:df_for_training.shape[1]])\n        trainY.append(df_for_training_scaled[i + n_future - 1:i + n_future, df_for_training.shape[1] - 1])\n\n    trainX, trainY = np.array(trainX), np.array(trainY)\n\n    return trainX, trainY\n\n\ndef data_desharnais():\n    df = pd.read_csv(CFG[2])\n    df = df.drop(columns=['Project','YearEnd', 'Language'])\n    df_for_training = df.astype(float)\n    #LSTM uses sigmoid and tanh that are sensitive to magnitude so values need to be normalized\n    # normalize the dataset\n    scaler = StandardScaler()\n    scaler = scaler.fit(df_for_training)\n    df_for_training_scaled = scaler.transform(df_for_training)\n    return df\n\n\ndef data_finnish():\n    df = pd.read_csv(CFG[3])\n    df_for_training = df.astype(float)\n    #LSTM uses sigmoid and tanh that are sensitive to magnitude so values need to be normalized\n    # normalize the dataset\n    scaler = MinMaxScaler()\n    scaler = scaler.fit(df_for_training)\n    df_for_training_scaled = scaler.transform(df_for_training)\n    return df\n\n\ndef data_isbsg10():\n\n    df = pd.read_csv(CFG[4])\n    df = df.drop(columns = ['id', 'ID', 'Data_Quality', 'Year','AG', 'N_effort_level1',\n           'N_PDR1', 'SDR', 'PET', 'PIT', 'I_Date', 'PAS',\n           'Recording_Method', 'Resource_Level', 'MTS', 'ATS', 'R_PWE_NPA',\n           'P_UWE', 'CASE_Tool', 'UM', 'HMA', 'Hardware1',\n           'IDE', 'DT1', 'DBS1', 'CS1', 'WS1', 'MS1', 'OP1', 'RTA',\n           'SP_CMMI', 'SP_ISO', 'SP_TICKIT', 'MIN_Defects', 'MAJ_Defects',\n           'X_Defects', 'TOT_Defects', 'UB_BU', 'UB_L', 'UB_DU', 'UB_CU',\n           'IMarket', 'T_Platform', 'D_Embedded', 'SE', 'SEA', 'SEM', 'E_Estimate',\n           'E_Estimate_Method', 'DDE', 'DDEM', 'C_Estimate', 'CEC', 'CEM',\n           'E_Tool', 'E_Comments', 'EC_Date', 'SR?', 'SR', 'R_FPC', 'R_FPA',\n           'P_Defects', 'D_Defects', 'MIN_B_Defects', 'MAJ_B_Defects',\n           'X_B_Defects', 'TOT_B_Defects', 'MIN_T_Defects', 'MAJ_T_Defects',\n           'X_T_Defects', 'TOT_T_Defects','S_Defects', 'MIN_I_Defects', 'MAJ_I_Defects',\n           'X_I_Defects', 'TOT_I_Defects'], axis = 1)\n    # Replace \"?\" with numpy.nan\n    df.replace('?', np.nan, inplace=True)\n#     print(df.columns)\n    \n    # imputation transformer\n    trf1 = ColumnTransformer([\n        ('impute',SimpleImputer(strategy='constant', fill_value='Missing'),[14,15,16,17,18])\n    ],remainder='passthrough')\n    \n    trf2 = ColumnTransformer([\n    ('ohe',OneHotEncoder(sparse_output=False,drop= 'first'),[0,1,2,3,4,5,6,7,8,9,10,11,12,13,15])\n    ],remainder='passthrough')\n    \n    pipe = Pipeline([\n    ('trf1',trf1),\n    ('trf2',trf2),\n    ])\n    \n    pipe.fit_transform(df)\n    \n    return pipe.fit_transform(df)\n\n\ndef data_kemerer():\n    df = pd.read_csv(CFG[5])\n    df_for_training = df.astype(float)\n    #LSTM uses sigmoid and tanh that are sensitive to magnitude so values need to be normalized\n    # normalize the dataset\n    scaler = StandardScaler()\n    scaler = scaler.fit(df_for_training)\n    df_for_training_scaled = scaler.transform(df_for_training)\n    return df\n    \n\n    \ndef data_kitchenham():\n    df = pd.read_csv(CFG[6])\n    df_for_training = df.astype(float)\n    #LSTM uses sigmoid and tanh that are sensitive to magnitude so values need to be normalized\n    # normalize the dataset\n    scaler = MinMaxScaler()\n    scaler = scaler.fit(df_for_training)\n    df_for_training_scaled = scaler.transform(df_for_training)\n    return df\n\n\ndef data_maxwell():\n    df = pd.read_csv(CFG[7])\n    df_for_training = df.astype(float)\n    #LSTM uses sigmoid and tanh that are sensitive to magnitude so values need to be normalized\n    # normalize the dataset\n    scaler = StandardScaler()\n    scaler = scaler.fit(df_for_training)\n    df_for_training_scaled = scaler.transform(df_for_training)\n    return df\n\n\ndef data_miyazaki():\n    df = pd.read_csv(CFG[8])\n    df_for_training = df.astype(float)\n    #LSTM uses sigmoid and tanh that are sensitive to magnitude so values need to be normalized\n    # normalize the dataset\n    scaler = StandardScaler()\n    scaler = scaler.fit(df_for_training)\n    df_for_training_scaled = scaler.transform(df_for_training)\n    return df\n","metadata":{"execution":{"iopub.status.busy":"2024-07-06T04:04:21.404192Z","iopub.execute_input":"2024-07-06T04:04:21.404526Z","iopub.status.idle":"2024-07-06T04:04:21.427448Z","shell.execute_reply.started":"2024-07-06T04:04:21.404501Z","shell.execute_reply":"2024-07-06T04:04:21.426546Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import pywt","metadata":{"execution":{"iopub.status.busy":"2024-07-06T04:04:23.656880Z","iopub.execute_input":"2024-07-06T04:04:23.657544Z","iopub.status.idle":"2024-07-06T04:04:23.748257Z","shell.execute_reply.started":"2024-07-06T04:04:23.657510Z","shell.execute_reply":"2024-07-06T04:04:23.747254Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def morlet_wavelet(t, f0=1.0, sigma=1.0):\n    \"\"\"\n    Morlet wavelet function.\n    t: time variable\n    f0: center frequency\n    sigma: bandwidth parameter\n    \"\"\"\n    return np.exp(-0.5 * (t / sigma) ** 2) * np.cos(2 * np.pi * f0 * t)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T04:04:27.000598Z","iopub.execute_input":"2024-07-06T04:04:27.001143Z","iopub.status.idle":"2024-07-06T04:04:27.006306Z","shell.execute_reply.started":"2024-07-06T04:04:27.001109Z","shell.execute_reply":"2024-07-06T04:04:27.005449Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Layer","metadata":{"execution":{"iopub.status.busy":"2024-07-06T04:04:45.702827Z","iopub.execute_input":"2024-07-06T04:04:45.703227Z","iopub.status.idle":"2024-07-06T04:04:45.707565Z","shell.execute_reply.started":"2024-07-06T04:04:45.703190Z","shell.execute_reply":"2024-07-06T04:04:45.706685Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class MorletWaveletLayer(Layer):\n    def __init__(self, units, w=5.0, **kwargs):\n        super(MorletWaveletLayer, self).__init__(**kwargs)\n        self.units = units\n        self.w = w\n\n    def build(self, input_shape):\n        self.kernel = self.add_weight(name='kernel',\n                                      shape=(input_shape[-1], self.units),\n                                      initializer='glorot_uniform',\n                                      trainable=True)\n\n    def call(self, inputs):\n        # Reshape inputs to 2D for the dense operation\n        input_reshaped = tf.reshape(inputs, [-1, inputs.shape[-1]])\n        z = tf.matmul(input_reshaped, self.kernel)\n\n        # Implement Morlet wavelet using TensorFlow operations\n        wavelet_output = tf.cos(self.w * z) * tf.exp(-z**2 / 2)\n        \n        # Reshape back to 3D\n        final= tf.reshape(wavelet_output, tf.concat([tf.shape(inputs)[:-1], [self.units]], axis=0))\n        print(final.shape)\n        return final","metadata":{"execution":{"iopub.status.busy":"2024-07-06T04:05:29.921367Z","iopub.execute_input":"2024-07-06T04:05:29.922239Z","iopub.status.idle":"2024-07-06T04:05:29.930351Z","shell.execute_reply.started":"2024-07-06T04:05:29.922205Z","shell.execute_reply":"2024-07-06T04:05:29.929350Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class MorletWaveletLayer(Layer):\n    def __init__(self, filters, f0=1.0, sigma=1.0, **kwargs):\n        super(MorletWaveletLayer, self).__init__(**kwargs)\n        self.filters = filters\n        self.f0 = f0\n        self.sigma = sigma\n\n    def build(self, input_shape):\n        self.kernel = self.add_weight(\n            shape=(self.filters, input_shape[-1]),\n            initializer='glorot_uniform',\n            trainable=True,\n        )\n        self.built = True\n\n    def call(self, inputs):\n        time = np.linspace(-1, 1, inputs.shape[1])\n        wavelet = morlet_wavelet(time, self.f0, self.sigma)\n        wavelet = tf.convert_to_tensor(wavelet, dtype=tf.float32)\n        wavelet = tf.reshape(wavelet, (1, -1, 1))\n        \n        wavelet_kernels = wavelet * tf.expand_dims(self.kernel, axis=1)\n        \n        # Ensure the wavelet kernels are properly shaped for convolution\n        wavelet_kernels = tf.transpose(wavelet_kernels, [1, 2, 0])\n        \n        conv = tf.nn.conv1d(inputs, wavelet_kernels, stride=1, padding='SAME')\n        return conv\n","metadata":{"execution":{"iopub.status.busy":"2024-07-06T04:14:58.920878Z","iopub.execute_input":"2024-07-06T04:14:58.921259Z","iopub.status.idle":"2024-07-06T04:14:58.930607Z","shell.execute_reply.started":"2024-07-06T04:14:58.921226Z","shell.execute_reply":"2024-07-06T04:14:58.929544Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"totalX, totalY = data_china()","metadata":{"execution":{"iopub.status.busy":"2024-07-06T04:35:12.902182Z","iopub.execute_input":"2024-07-06T04:35:12.902538Z","iopub.status.idle":"2024-07-06T04:35:12.943383Z","shell.execute_reply.started":"2024-07-06T04:35:12.902509Z","shell.execute_reply":"2024-07-06T04:35:12.942608Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\nclass FireflyAlgorithm:\n    def __init__(self, n_fireflies, n_iterations, alpha=0.5, beta=1.0, gamma=1.0):\n        self.n_fireflies = n_fireflies\n        self.n_iterations = n_iterations\n        self.alpha = alpha\n        self.beta = beta\n        self.gamma = gamma\n\n    def initialize_fireflies(self, bounds):\n        fireflies = np.random.rand(self.n_fireflies, len(bounds))\n        for i in range(len(bounds)):\n            fireflies[:, i] = bounds[i][0] + fireflies[:, i] * (bounds[i][1] - bounds[i][0])\n        return fireflies\n\n    def calculate_light_intensity(self, firefly, objective_function):\n        return objective_function(firefly)\n\n    def move_firefly(self, firefly_i, firefly_j, beta):\n        r = np.linalg.norm(firefly_i - firefly_j)\n        return firefly_i + beta * np.exp(-self.gamma * r ** 2) * (firefly_j - firefly_i) + self.alpha * (np.random.rand(len(firefly_i)) - 0.5)\n\n    def optimize(self, objective_function, bounds):\n        fireflies = self.initialize_fireflies(bounds)\n        light_intensities = np.array([self.calculate_light_intensity(firefly, objective_function) for firefly in fireflies])\n\n        for iteration in range(self.n_iterations):\n            for i in range(self.n_fireflies):\n                for j in range(self.n_fireflies):\n                    if light_intensities[i] > light_intensities[j]:\n                        fireflies[i] = self.move_firefly(fireflies[i], fireflies[j], self.beta)\n                        light_intensities[i] = self.calculate_light_intensity(fireflies[i], objective_function)\n        \n        best_firefly = fireflies[np.argmin(light_intensities)]\n        best_intensity = np.min(light_intensities)\n\n        return best_firefly, best_intensity\n","metadata":{"execution":{"iopub.status.busy":"2024-07-06T04:15:02.862246Z","iopub.execute_input":"2024-07-06T04:15:02.862913Z","iopub.status.idle":"2024-07-06T04:15:02.875209Z","shell.execute_reply.started":"2024-07-06T04:15:02.862878Z","shell.execute_reply":"2024-07-06T04:15:02.874242Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"trainX, testX, trainY, testY = train_test_split(totalX, totalY, test_size=0.25, random_state=42)\n\n# Define the objective function\ndef objective_function(params):\n    f0, sigma = params[0], params[1]\n    \n    # Create the Wavelet Neural Network\n    input_shape = trainX.shape[1:]  # Assuming trainX.shape is (496, 2, 18), input_shape should be (2, 18)\n    wnn = Sequential([\n        BatchNormalization(input_shape=input_shape),\n        Dense(64, activation='relu'),\n        Dropout(0.1),\n        Dense(32, activation='relu'),\n        MorletWaveletLayer(filters=16, f0=1.0, sigma=1.0, input_shape=(None, 1)),  # Replace with appropriate units for your use case\n        Flatten(),\n        Dense(1)  # Adjust output dim to match trainY.shape[1]\n    ])\n\n    # Compile the model\n    wnn.compile(optimizer='adam', loss='mse')\n    wnn.summary()\n    wnn.fit(trainX, trainY, epochs=10, batch_size=32, verbose=0)\n    \n    loss = wnn.evaluate(X_test, y_test, verbose=0)\n    return loss\n","metadata":{"execution":{"iopub.status.busy":"2024-07-06T04:35:19.799101Z","iopub.execute_input":"2024-07-06T04:35:19.799964Z","iopub.status.idle":"2024-07-06T04:35:19.808793Z","shell.execute_reply.started":"2024-07-06T04:35:19.799928Z","shell.execute_reply":"2024-07-06T04:35:19.807811Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# Define the bounds for f0 and sigma\nbounds = [(0.1, 2.0), (0.1, 2.0)]\n\n# Initialize the Firefly Algorithm\nfa = FireflyAlgorithm(n_fireflies=10, n_iterations=2, alpha=0.5, beta=1.0, gamma=1.0)\n\n# Optimize the parameters\nbest_params, best_intensity = fa.optimize(objective_function, bounds)\n\nprint(f'Best Parameters: f0 = {best_params[0]}, sigma = {best_params[1]}')\nprint(f'Best Intensity (MSE): {best_intensity}')\n","metadata":{"execution":{"iopub.status.busy":"2024-07-06T04:35:24.698942Z","iopub.execute_input":"2024-07-06T04:35:24.699678Z","iopub.status.idle":"2024-07-06T04:35:28.779352Z","shell.execute_reply.started":"2024-07-06T04:35:24.699644Z","shell.execute_reply":"2024-07-06T04:35:28.777928Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Model: \"sequential_404\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n batch_normalization (BatchN  (None, 2, 18)            72        \n ormalization)                                                   \n                                                                 \n dense_808 (Dense)           (None, 2, 64)             1216      \n                                                                 \n dropout (Dropout)           (None, 2, 64)             0         \n                                                                 \n dense_809 (Dense)           (None, 2, 32)             2080      \n                                                                 \n morlet_wavelet_layer_405 (M  (None, 2, 16)            512       \n orletWaveletLayer)                                              \n                                                                 \n flatten_404 (Flatten)       (None, 32)                0         \n                                                                 \n dense_810 (Dense)           (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 3,913\nTrainable params: 3,877\nNon-trainable params: 36\n_________________________________________________________________\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[26], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m fa \u001b[38;5;241m=\u001b[39m FireflyAlgorithm(n_fireflies\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, n_iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, beta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Optimize the parameters\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m best_params, best_intensity \u001b[38;5;241m=\u001b[39m \u001b[43mfa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest Parameters: f0 = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_params[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, sigma = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_params[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest Intensity (MSE): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_intensity\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n","Cell \u001b[0;32mIn[20], line 26\u001b[0m, in \u001b[0;36mFireflyAlgorithm.optimize\u001b[0;34m(self, objective_function, bounds)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\u001b[38;5;28mself\u001b[39m, objective_function, bounds):\n\u001b[1;32m     25\u001b[0m     fireflies \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialize_fireflies(bounds)\n\u001b[0;32m---> 26\u001b[0m     light_intensities \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculate_light_intensity(firefly, objective_function) \u001b[38;5;28;01mfor\u001b[39;00m firefly \u001b[38;5;129;01min\u001b[39;00m fireflies])\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m iteration \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iterations):\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_fireflies):\n","Cell \u001b[0;32mIn[20], line 26\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\u001b[38;5;28mself\u001b[39m, objective_function, bounds):\n\u001b[1;32m     25\u001b[0m     fireflies \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialize_fireflies(bounds)\n\u001b[0;32m---> 26\u001b[0m     light_intensities \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_light_intensity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfirefly\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjective_function\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m firefly \u001b[38;5;129;01min\u001b[39;00m fireflies])\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m iteration \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iterations):\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_fireflies):\n","Cell \u001b[0;32mIn[20], line 18\u001b[0m, in \u001b[0;36mFireflyAlgorithm.calculate_light_intensity\u001b[0;34m(self, firefly, objective_function)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_light_intensity\u001b[39m(\u001b[38;5;28mself\u001b[39m, firefly, objective_function):\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobjective_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfirefly\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[25], line 24\u001b[0m, in \u001b[0;36mobjective_function\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     21\u001b[0m wnn\u001b[38;5;241m.\u001b[39msummary()\n\u001b[1;32m     22\u001b[0m wnn\u001b[38;5;241m.\u001b[39mfit(trainX, trainY, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mwnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m/tmp/__autograph_generated_filedvsp3naz.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__test_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n","\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1852, in test_function  *\n        return step_function(self, iterator)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1836, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1824, in run_step  **\n        outputs = model.test_step(data)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1788, in test_step\n        y_pred = self(x, training=False)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_404\" is incompatible with the layer: expected shape=(None, 2, 18), found shape=(None, 100, 1)\n"],"ename":"ValueError","evalue":"in user code:\n\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1852, in test_function  *\n        return step_function(self, iterator)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1836, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1824, in run_step  **\n        outputs = model.test_step(data)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1788, in test_step\n        y_pred = self(x, training=False)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_404\" is incompatible with the layer: expected shape=(None, 2, 18), found shape=(None, 100, 1)\n","output_type":"error"}]},{"cell_type":"code","source":"def objective_function(weights, model, X_train, y_train):\n    model.set_weights(weights)\n    predictions = model.predict(X_train)\n    return mean_squared_error(y_train, predictions)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from fireflyalgorithm import FireflyAlgorithm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FA = FireflyAlgorithm()\nbest = FA.run(function=lambda weights: objective_function(weights, wnn, trainX, trainY), dim=10, lb=-5, ub=5, max_evals=10000)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"    firefly_optimizer = FireflyAlgorithm(\n        func=lambda weights: objective_function(weights, wnn, trainX, trainY),\n        lb=-1.0,\n        ub=1.0,\n        dim=wnn.count_params(),\n        n_fireflies=20,\n        max_iter=100\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"    # Create the Wavelet Neural Network\n    input_shape = trainX.shape[1:]  # Assuming trainX.shape is (496, 2, 18), input_shape should be (2, 18)\n    wnn = Sequential([\n        BatchNormalization(input_shape=input_shape),\n        Dense(64, activation='relu'),\n        Dropout(0.1),\n        Dense(32, activation='relu'),\n        MorletWaveletLayer(10),  # Replace with appropriate units for your use case\n        Dense(trainY.shape[1])  # Adjust output dim to match trainY.shape[1]\n    ])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testY.shape[1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"    # Optimize the model weights using Firefly Algorithm\n    best_weights, best_fitness = firefly_optimizer.optimize()\n    print(best_weights)\n    wnn.set_weights(best_weights)\n\n    # Evaluate the optimized model\n    predictions = wnn.predict(X_test)\n    mse = mean_squared_error(y_test, predictions)\n    print(f'Test Mean Squared Error: {mse}')\n\n    # Plot the results\n    import matplotlib.pyplot as plt\n\n    plt.figure(figsize=(12, 6))\n    plt.plot(y_test.values, label='Actual Effort')\n    plt.plot(predictions, label='Predicted Effort')\n    plt.legend()\n    plt.title('Wavelet Neural Network Predictions with Firefly Algorithm')\n    plt.xlabel('Sample')\n    plt.ylabel('Effort')\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2024-07-06T03:02:51.920788Z","iopub.execute_input":"2024-07-06T03:02:51.921573Z","iopub.status.idle":"2024-07-06T03:02:51.926047Z","shell.execute_reply.started":"2024-07-06T03:02:51.921534Z","shell.execute_reply":"2024-07-06T03:02:51.924903Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"totalX.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    trainX, testX, trainY, testY = train_test_split(totalX, totalY, test_size=0.25, random_state=42)\n    # Print the shape of trainX and trainY\n    print('trainX shape == {}.'.format(trainX.shape))\n    print('trainY shape == {}.'.format(trainY.shape))\n    \n    print('testX shape == {}.'.format(testX.shape))\n    print('testY shape == {}.'.format(testY.shape))\n\n    # Create the Wavelet Neural Network\n    input_shape = trainX.shape[1:]  # Assuming trainX.shape is (496, 2, 18), input_shape should be (2, 18)\n    wnn = Sequential([\n        BatchNormalization(input_shape=input_shape),\n        Dense(64, activation='relu'),\n        Dropout(0.1),\n        Dense(32, activation='relu'),\n        MorletWaveletLayer(filters=16, f0=1.0, sigma=1.0, input_shape=(None, 1)),  # Replace with appropriate units for your use case\n        Flatten(),\n        Dense(1)  # Adjust output dim to match trainY.shape[1]\n    ])\n\n    # Compile the model\n    wnn.compile(optimizer='adam', loss='mse')\n    wnn.summary()\n\n    # Train the model\n    #history = wnn.fit(trainX, trainY, epochs=100, batch_size=32, validation_split=0.25, verbose=1)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-06T03:05:09.640759Z","iopub.execute_input":"2024-07-06T03:05:09.641414Z","iopub.status.idle":"2024-07-06T03:05:09.781855Z","shell.execute_reply.started":"2024-07-06T03:05:09.641380Z","shell.execute_reply":"2024-07-06T03:05:09.780717Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"trainX shape == (372, 2, 18).\ntrainY shape == (372, 1).\ntestX shape == (124, 2, 18).\ntestY shape == (124, 1).\nModel: \"sequential_16\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n batch_normalization_17 (Bat  (None, 2, 18)            72        \n chNormalization)                                                \n                                                                 \n dense_55 (Dense)            (None, 2, 64)             1216      \n                                                                 \n dropout_17 (Dropout)        (None, 2, 64)             0         \n                                                                 \n dense_56 (Dense)            (None, 2, 32)             2080      \n                                                                 \n morlet_wavelet_layer_16 (Mo  (None, 2, 16)            512       \n rletWaveletLayer)                                               \n                                                                 \n flatten_2 (Flatten)         (None, 32)                0         \n                                                                 \n dense_57 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 3,913\nTrainable params: 3,877\nNon-trainable params: 36\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install fireflyalgorithm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"    history = wnn.fit(trainX, trainY, epochs=100, batch_size=32, verbose=0)\n    \n    # Evaluate model\n    predictions = wnn.predict(testX)\n    ","metadata":{"execution":{"iopub.status.busy":"2024-07-06T03:05:22.549106Z","iopub.execute_input":"2024-07-06T03:05:22.549474Z","iopub.status.idle":"2024-07-06T03:05:31.002780Z","shell.execute_reply.started":"2024-07-06T03:05:22.549446Z","shell.execute_reply":"2024-07-06T03:05:31.001914Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"4/4 [==============================] - 0s 12ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"testY.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if predictions.ndim == 2:\n        predictions = np.squeeze(predictions)  # Remove the last dimension","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mse = mean_squared_error(testY, predictions)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T03:05:37.421881Z","iopub.execute_input":"2024-07-06T03:05:37.422920Z","iopub.status.idle":"2024-07-06T03:05:37.430512Z","shell.execute_reply.started":"2024-07-06T03:05:37.422872Z","shell.execute_reply":"2024-07-06T03:05:37.429419Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"mse","metadata":{"execution":{"iopub.status.busy":"2024-07-06T03:05:44.108487Z","iopub.execute_input":"2024-07-06T03:05:44.109171Z","iopub.status.idle":"2024-07-06T03:05:44.115215Z","shell.execute_reply.started":"2024-07-06T03:05:44.109138Z","shell.execute_reply":"2024-07-06T03:05:44.114286Z"},"trusted":true},"execution_count":57,"outputs":[{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"0.021260198595782553"},"metadata":{}}]},{"cell_type":"code","source":"    # Plot training history\n    plt.plot(history.history['loss'], label='Training loss')\n    plt.plot(history.history['val_loss'], label='Validation loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score","metadata":{"execution":{"iopub.status.busy":"2024-07-06T02:38:48.878752Z","iopub.execute_input":"2024-07-06T02:38:48.879522Z","iopub.status.idle":"2024-07-06T02:38:48.883849Z","shell.execute_reply.started":"2024-07-06T02:38:48.879490Z","shell.execute_reply":"2024-07-06T02:38:48.882912Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Flatten","metadata":{"execution":{"iopub.status.busy":"2024-07-06T04:07:15.575874Z","iopub.execute_input":"2024-07-06T04:07:15.576876Z","iopub.status.idle":"2024-07-06T04:07:15.581197Z","shell.execute_reply.started":"2024-07-06T04:07:15.576837Z","shell.execute_reply":"2024-07-06T04:07:15.580230Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def mean_magnitude_relative_error(y_true, y_pred):\n    # Calculate relative error\n    relative_error = np.abs((y_true - y_pred) / y_true)\n    \n    # Calculate mean magnitude of relative error (MMRE)\n    mmre = np.mean(relative_error)\n    \n    return mmre\n\nif __name__ == \"__main__\":\n    # Load or generate your data\n    X, y = data_china()\n\n    # Assuming trainX and trainY are defined appropriately\n    trainX = X[:400]  # Example: using first 400 samples for training\n    trainY = y[:400]  # Example: using first 400 targets for training\n    testX = X[400:]   # Example: using remaining samples for testing\n    testY = y[400:]   # Example: using remaining targets for testing\n\n    # Create the Wavelet Neural Network\n    input_shape = trainX.shape[1:]  # Assuming trainX.shape is (496, 2, 18), input_shape should be (2, 18)\n    wnn = Sequential([\n        BatchNormalization(input_shape=input_shape),\n        Dense(64, activation='relu'),\n        Dropout(0.1),\n        Dense(32, activation='relu'),\n        MorletWaveletLayer(filters=16, f0=1.0, sigma=1.0, input_shape=(None, 1)),  # Replace with appropriate units for your use case\n        Dense(10),\n        Flatten(),\n        Dense(1)  # Adjust output dim to match trainY.shape[1]\n    ])\n\n    # Compile the model\n    wnn.compile(optimizer='adam', loss='mse')\n    wnn.summary()\n\n    # Train the model\n    history = wnn.fit(trainX, trainY, epochs=50, batch_size=32, validation_split=0.0, verbose=1)\n\n    # Evaluate the model on test data\n    predictions = wnn.predict(testX)\n\n    # Calculate evaluation metrics\n    mae = mean_absolute_error(testY, predictions)\n    mse = mean_squared_error(testY, predictions)\n    r2 = r2_score(testY, predictions)\n    mmre = mean_magnitude_relative_error(testY, predictions)\n\n    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n    print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n    print(f\"R-squared (R2) Score: {r2:.4f}\")\n    print(f\"Mean Magnitude of Relative Error (MMRE): {mmre:.4f}\")\n\n    # Plot training history\n    plt.plot(history.history['loss'], label='Training loss')\n    #plt.plot(history.history['val_loss'], label='Validation loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-06T03:06:50.678166Z","iopub.execute_input":"2024-07-06T03:06:50.679151Z","iopub.status.idle":"2024-07-06T03:06:56.645444Z","shell.execute_reply.started":"2024-07-06T03:06:50.679102Z","shell.execute_reply":"2024-07-06T03:06:56.644570Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stdout","text":"Model: \"sequential_18\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n batch_normalization_19 (Bat  (None, 2, 18)            72        \n chNormalization)                                                \n                                                                 \n dense_62 (Dense)            (None, 2, 64)             1216      \n                                                                 \n dropout_19 (Dropout)        (None, 2, 64)             0         \n                                                                 \n dense_63 (Dense)            (None, 2, 32)             2080      \n                                                                 \n morlet_wavelet_layer_18 (Mo  (None, 2, 16)            512       \n rletWaveletLayer)                                               \n                                                                 \n dense_64 (Dense)            (None, 2, 10)             170       \n                                                                 \n flatten_4 (Flatten)         (None, 20)                0         \n                                                                 \n dense_65 (Dense)            (None, 1)                 21        \n                                                                 \n=================================================================\nTotal params: 4,071\nTrainable params: 4,035\nNon-trainable params: 36\n_________________________________________________________________\nEpoch 1/50\n13/13 [==============================] - 2s 5ms/step - loss: 0.1584\nEpoch 2/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0500\nEpoch 3/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0319\nEpoch 4/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0230\nEpoch 5/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0214\nEpoch 6/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0196\nEpoch 7/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0190\nEpoch 8/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0179\nEpoch 9/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0176\nEpoch 10/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0171\nEpoch 11/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0182\nEpoch 12/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0166\nEpoch 13/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0159\nEpoch 14/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0160\nEpoch 15/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0142\nEpoch 16/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0158\nEpoch 17/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0140\nEpoch 18/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0151\nEpoch 19/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0147\nEpoch 20/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0140\nEpoch 21/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0136\nEpoch 22/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0130\nEpoch 23/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0136\nEpoch 24/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0132\nEpoch 25/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0132\nEpoch 26/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0127\nEpoch 27/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0131\nEpoch 28/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0140\nEpoch 29/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0126\nEpoch 30/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0128\nEpoch 31/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0131\nEpoch 32/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0125\nEpoch 33/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0128\nEpoch 34/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0119\nEpoch 35/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0130\nEpoch 36/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0129\nEpoch 37/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0131\nEpoch 38/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0131\nEpoch 39/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0124\nEpoch 40/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0125\nEpoch 41/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0118\nEpoch 42/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0125\nEpoch 43/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0120\nEpoch 44/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0119\nEpoch 45/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0122\nEpoch 46/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0124\nEpoch 47/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0117\nEpoch 48/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0115\nEpoch 49/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0116\nEpoch 50/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0118\n3/3 [==============================] - 0s 2ms/step\nMean Absolute Error (MAE): 0.0737\nMean Squared Error (MSE): 0.0183\nR-squared (R2) Score: -0.0466\nMean Magnitude of Relative Error (MMRE): 4.3562\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"pre=wnn.predict(testX)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T02:56:41.432249Z","iopub.execute_input":"2024-07-06T02:56:41.432645Z","iopub.status.idle":"2024-07-06T02:56:41.497177Z","shell.execute_reply.started":"2024-07-06T02:56:41.432613Z","shell.execute_reply":"2024-07-06T02:56:41.496383Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"3/3 [==============================] - 0s 2ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"wnn=None","metadata":{"execution":{"iopub.status.busy":"2024-07-06T02:54:26.622263Z","iopub.execute_input":"2024-07-06T02:54:26.622677Z","iopub.status.idle":"2024-07-06T02:54:26.627160Z","shell.execute_reply.started":"2024-07-06T02:54:26.622647Z","shell.execute_reply":"2024-07-06T02:54:26.626180Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"pre.shape","metadata":{"execution":{"iopub.status.busy":"2024-07-06T02:56:44.186598Z","iopub.execute_input":"2024-07-06T02:56:44.187211Z","iopub.status.idle":"2024-07-06T02:56:44.192972Z","shell.execute_reply.started":"2024-07-06T02:56:44.187178Z","shell.execute_reply":"2024-07-06T02:56:44.192043Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"(96, 1)"},"metadata":{}}]},{"cell_type":"code","source":"trainY.shape","metadata":{"execution":{"iopub.status.busy":"2024-07-06T02:56:46.971513Z","iopub.execute_input":"2024-07-06T02:56:46.971864Z","iopub.status.idle":"2024-07-06T02:56:46.978398Z","shell.execute_reply.started":"2024-07-06T02:56:46.971838Z","shell.execute_reply":"2024-07-06T02:56:46.977103Z"},"trusted":true},"execution_count":39,"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"(400, 1)"},"metadata":{}}]},{"cell_type":"code","source":"mse = mean_squared_error(testY, pre)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T02:56:49.426702Z","iopub.execute_input":"2024-07-06T02:56:49.427704Z","iopub.status.idle":"2024-07-06T02:56:49.434061Z","shell.execute_reply.started":"2024-07-06T02:56:49.427658Z","shell.execute_reply":"2024-07-06T02:56:49.433056Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"mse","metadata":{"execution":{"iopub.status.busy":"2024-07-06T02:57:19.327359Z","iopub.execute_input":"2024-07-06T02:57:19.327724Z","iopub.status.idle":"2024-07-06T02:57:19.334050Z","shell.execute_reply.started":"2024-07-06T02:57:19.327696Z","shell.execute_reply":"2024-07-06T02:57:19.332916Z"},"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"0.021960801772131947"},"metadata":{}}]},{"cell_type":"code","source":"# Assuming testX and testY are defined appropriately\ntestX = X[400:]   # Example: using remaining samples for testing\ntestY = y[400:]   # Example: using remaining targets for testing\n\n# Evaluate the model on test data\npredictions = wnn.predict(testX)\n\n# Reshape predictions and testY if necessary\nif predictions.ndim > 2:\n    predictions = np.squeeze(predictions)\nif testY.ndim > 2:\n    testY = np.squeeze(testY)\n\n# Calculate evaluation metrics\nmae = np.mean(np.abs(predictions - testY))\nmse = np.mean((predictions - testY)**2)\nr2 = r2_score(testY, predictions)\nmmre = mean_magnitude_relative_error(testY, predictions)\n\nprint(f\"Mean Absolute Error (MAE): {mae:.4f}\")\nprint(f\"Mean Squared Error (MSE): {mse:.4f}\")\nprint(f\"R-squared (R2) Score: {r2:.4f}\")\nprint(f\"Mean Magnitude of Relative Error (MMRE): {mmre:.4f}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LSTM Model","metadata":{}},{"cell_type":"code","source":"trainX, trainY = data_china()\n\n\nprint('trainX shape == {}.'.format(trainX.shape))\nprint('trainY shape == {}.'.format(trainY.shape))\n\n# define the Autoencoder model\n\nmodel = Sequential()\nmodel.add(BatchNormalization(input_shape=(trainX.shape[1], trainX.shape[2])))\nmodel.add(Bidirectional(LSTM(64, activation='relu', return_sequences=True)))\nmodel.add(LSTM(32, activation='relu',))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(trainY.shape[1]))\n\nmodel.compile(optimizer='sgd', loss='mse')\nmodel.summary()\n\n\n# fit the model\nhistory = model.fit(trainX, trainY, epochs=50, batch_size=32, validation_split=0.1, verbose=1)\n\nplt.plot(history.history['loss'], label='Training loss')\nplt.plot(history.history['val_loss'], label='Validation loss')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def root_mean_squared_error(y_true, y_pred):\n    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# trainY.shape[1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainX, trainY = data_china()\n# def create_and_train_autoencoder(trainX, trainY, epochs, batch_size, validation_split, \n#                                  learning_rate, dropout_rate, activation, \n#                                  optimizer, loss):\n    \n#     model = Sequential()\n#     model.add(BatchNormalization(input_shape=(trainX.shape[1], trainX.shape[2])))\n#     model.add(Bidirectional(LSTM(64, activation=activation, return_sequences=True)))\n#     model.add(LSTM(32, activation=activation, return_sequences=False))\n#     model.add(Dropout(dropout_rate))\n#     model.add(Dense(trainY.shape[1]))\n\n#     model.compile(optimizer=optimizer, loss=loss)\n#     model.summary()\n\n\n#     # fit the model\n#     history = model.fit(trainX, trainY, epochs=epochs, batch_size=batch_size, validation_split=validation_split, verbose=1)\n\n#     plt.plot(history.history['loss'], label='Training loss')\n#     plt.plot(history.history['val_loss'], label='Validation loss')\n\n#     return model, history\n\ndef create_and_train_autoencoder(trainX, trainY, epochs, batch_size, validation_split, \n                                 learning_rate, dropout_rate, lstm_units, activation, \n                                 optimizer, loss):\n    \n    model = Sequential()\n    model.add(BatchNormalization(input_shape=(trainX.shape[1], trainX.shape[2])))\n    model.add(Bidirectional(LSTM(lstm_units, activation=activation, return_sequences=True)))\n    model.add(LSTM(lstm_units // 2, activation=activation, return_sequences=True))\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(trainY.shape[1]))\n\n\n\n    \n    model.compile(optimizer=optimizer, loss=loss)\n    model.summary()\n\n    # Define early stopping\n    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n\n    # fit the model with early stopping\n    history = model.fit(trainX, trainY, epochs=epochs, batch_size=batch_size, \n                        validation_split=validation_split, verbose=1, callbacks=[early_stopping])\n   \n    plt.plot(history.history['loss'], label='Training loss')\n    plt.plot(history.history['val_loss'], label='Validation loss')\n\n    return model, history\n\n\n# Example usage:\n# Adjust hyperparameters as needed\n# autoencoder_model, training_history = create_and_train_autoencoder(trainX, trainY, \n#                                                                     epochs=500, batch_size=32, \n#                                                                     validation_split=0.1, \n#                                                                     learning_rate=best_hyperparameters[0], \n#                                                                     dropout_rate=best_hyperparameters[1], \n#                                                                     lstm_units=int(best_hyperparameters[2]),\n#                                                                     activation='relu', \n#                                                                     optimizer='adam', \n#                                                                     loss='mse')\n\nautoencoder_model, training_history = create_and_train_autoencoder(trainX, trainY, \n                                                                    epochs=100, batch_size=16, \n                                                                    validation_split=0.1, \n                                                                    learning_rate=0.01, \n                                                                    dropout_rate=0.1,\n                                                                    lstm_units= 64,\n                                                                    activation='sigmoid', \n                                                                    optimizer='adam', \n                                                                    loss= 'mae',\n                                                                    \n                                                                  )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\ndef harmonic_search(objective_function, search_space, num_iterations):\n    # Initialize parameters\n    num_harmonics = 10\n    pitch_adjustment_rate = 0.01\n\n    # Initialize random solutions within the search space\n    solutions = np.random.uniform(low=search_space[:, 0], high=search_space[:, 1], size=(num_harmonics, len(search_space)))\n\n    for iteration in range(num_iterations):\n        # Evaluate the performance of each solution\n        scores = [objective_function(solution) for solution in solutions]\n\n        # Select the top-performing solutions as parents\n        parents = solutions[np.argsort(scores)[:2]]\n\n        # Generate new candidate solutions by combining and modifying parents\n        new_solutions = parents[0] + np.random.uniform(low=-pitch_adjustment_rate, high=pitch_adjustment_rate, size=parents.shape)\n\n        # Clip new solutions to the search space\n        new_solutions = np.clip(new_solutions, search_space[:, 0], search_space[:, 1])\n\n        # Replace the worst solutions with the new ones\n        worst_index = np.argmax(scores)\n        solutions[worst_index] = new_solutions[0]  # Take the first parent as the new solution\n\n    # Return the best solution found\n    best_solution = solutions[np.argmin(scores)]\n    return best_solution\n\n# Example usage:\n# Define the search space for hyperparameters\nsearch_space = np.array([\n    [0.001, 0.1],  # Learning Rate\n    [0.1, 0.9],    # Dropout Rate\n    [16, 64],     # Number of LSTM units\n])\n\n# Define your objective function (replace with your actual training and evaluation logic)\ndef objective_function(hyperparameters):\n    learning_rate, dropout_rate, lstm_units = hyperparameters\n    \n    try:\n        # Create and train LSTM model with the given hyperparameters\n        # Return the performance metric to be minimized (e.g., validation loss)\n        autoencoder_model, training_history = create_and_train_autoencoder(trainX, trainY, \n                                                                            epochs=100, batch_size=16, \n                                                                            validation_split=0.1, \n                                                                            learning_rate=learning_rate, \n                                                                            dropout_rate=dropout_rate, \n                                                                            lstm_units=int(lstm_units),\n                                                                            activation='sigmoid', \n                                                                            optimizer='adam')\n        \n        # Retrieve the performance metric (e.g., validation loss) from the training history\n        metric = min(training_history.history['val_loss'])  # Assuming 'val_loss' is the relevant metric\n\n        return metric\n    except Exception as e:\n        # Return a large value in case of an error\n        return float('inf')\n\n# Run harmonic search\nbest_hyperparameters = harmonic_search(objective_function, search_space, num_iterations=50)\n\n# Print or log the best hyperparameters\nprint(\"Best Hyperparameters:\")\nprint(\"Learning Rate:\", best_hyperparameters[0])\nprint(\"Dropout Rate:\", best_hyperparameters[1])\nprint(\"LSTM Units:\", int(best_hyperparameters[2]))\n\n# Update your LSTM model with the best hyperparameters\nautoencoder_model, training_history = create_and_train_autoencoder(trainX, trainY, \n                                                                    epochs=100, batch_size=16, \n                                                                    validation_split=0.1, \n                                                                    learning_rate=best_hyperparameters[0], \n                                                                    dropout_rate=best_hyperparameters[1], \n                                                                    lstm_units=int(best_hyperparameters[2]),\n                                                                    activation='sigmoid', \n                                                                    optimizer='adam', \n                                                                    loss='mae',\n                                                                    \n                                                                  )\n\n# Optionally, you can also print or log other relevant information, such as the best performance metric\nbest_metric = objective_function(best_hyperparameters)\nprint(\"Best Performance Metric:\", best_metric)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#     from sklearn.preprocessing import MinMaxScaler\n#     df = pd.read_csv(CFG[1])\n# #     x = df.drop(columns=['id','ID'])\n#     # Assuming your dataset is stored in a DataFrame named 'df'\n#     # If not, replace 'df' with the actual name of your DataFrame\n\n#     # Extract the target variable\n#     target_variable = 'Effort'\n#     y = df[target_variable]\n\n#     # Extract features (excluding target variable)\n#     X = df.drop(columns=['id','ID', 'Effort'])\n\n#     # Normalize the dataset\n#     scaler = MinMaxScaler()\n#     X_normalized = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n\n#     # Split the dataset into training and testing sets\n#     X_train, X_test, y_train, y_test = train_test_split(X_normalized, y, test_size=0.2, random_state=42)\n\n#     # Check the shapes of the resulting sets\n#     print(\"X_train shape:\", X_train.shape)\n#     print(\"X_test shape:\", X_test.shape)\n#     print(\"y_train shape:\", y_train.shape)\n#     print(\"y_test shape:\", y_test.shape)\n# df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn import datasets\n# from sklearn.model_selection import train_test_split\n# from sklearn.svm import SVR\n# from sklearn.metrics import  mean_absolute_error\n\n\n\n# # Create an SVM regressor\n# svm_regressor = SVR(kernel='linear', C=1)\n\n# svm_regressor.fit(X_train, y_train)\n\n# # Make predictions on the test set\n# y_pred = svm_regressor.predict(X_test)\n\n# # Evaluate mean squared error\n# mae = mean_absolute_error(y_test, y_pred)\n# print(f\"Mean Squared Error: {mae}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# trainX.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Attention, Activation\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport matplotlib.pyplot as plt\ndf = pd.read_csv(CFG[1])\n# Load your dataset (assuming it's in a DataFrame named 'df')\n# Replace 'Effort' with the actual column name you want to predict\ntarget_column = 'Effort'\n\n# Separate features (X) and target variable (y)\nX = df.drop(target_column, axis=1)\ny = df[target_column]\n\n# Data Preprocessing\nscaler_X = MinMaxScaler(feature_range=(0, 1))\nscaler_y = MinMaxScaler(feature_range=(0, 1))\n\nX_normalized = scaler_X.fit_transform(X.values)\ny_normalized = scaler_y.fit_transform(y.values.reshape(-1, 1))\n\n# Reshape X for LSTM input (assuming you have a sequence length)\nsequence_length = 1  # Adjust based on your dataset\nX_reshaped = X_normalized.reshape((X_normalized.shape[0], sequence_length, X_normalized.shape[1]))\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_reshaped, y_normalized, test_size=0.2, random_state=42)\n\n# Model Construction and Training\ninp = Input(shape=(X_reshaped.shape[1], X_reshaped.shape[2]))\ne = Sequential()(inp)\ng = LSTM(units=64, return_sequences=True)(e)\nh = LSTM(units=32, return_sequences=True)(g)\ns = Attention(use_scale=True)([h, h])\nf = Activation('relu')(s)\nout = Dense(1)(f)  # Assuming a regression task\n\nmodel = Model(inputs=inp, outputs=out)\nmodel.compile(optimizer='adam', loss='mean_squared_error')  # You can use other loss functions for regression\n\n# Implement Early Stopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n\n# Train the model and monitor the training history\nhistory = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.1, callbacks=[early_stopping])\n\n# Plot the training and validation loss\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\nplt.show()\n\n\n# Prediction\ny_pred = model.predict(X_test)\n\n# Rescale predictions to the original scale\ny_pred_rescaled = scaler_y.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n\n# Rescale true labels to the original scale\ny_test_rescaled = scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten()\n\n# Evaluate mean absolute error\nmae = np.mean(np.abs(y_test_rescaled - y_pred_rescaled))\nprint(f\"Mean Absolute Error: {mae}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(CFG[1])\n# df = df.drop(columns=['id','ID'])\ndf.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Attention, Activation\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.metrics import mean_absolute_error\nimport optuna\n\ndf = pd.read_csv(CFG[7])\n# df = df.drop(columns=['id','ID'])\n# Load your dataset (assuming it's in a DataFrame named 'df')\n# Replace 'Effort' with the actual column name you want to predict\ntarget_column = 'Effort'\n\n# Separate features (X) and target variable (y)\nX = df.drop(target_column, axis=1)\ny = df[target_column]\n\n# Data Preprocessing\nscaler_X = MinMaxScaler(feature_range=(0, 1))\nscaler_y = MinMaxScaler(feature_range=(0, 1))\n\nX_normalized = scaler_X.fit_transform(X.values)\ny_normalized = scaler_y.fit_transform(y.values.reshape(-1, 1))\n\n# Reshape X for LSTM input (assuming you have a sequence length)\nsequence_length = 1  # Adjust based on your dataset\nX_reshaped = X_normalized.reshape((X_normalized.shape[0], sequence_length, X_normalized.shape[1]))\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_reshaped, y_normalized, test_size=0.2, random_state=42)\n\n# Model Construction and Training\ndef build_model(units1, units2, learning_rate):\n    inp = Input(shape=(X_reshaped.shape[1], X_reshaped.shape[2]))\n    e = Sequential()(inp)\n    g = LSTM(units=units1, return_sequences=True)(e)\n    h = LSTM(units=units2, return_sequences=True)(g)\n    s = Attention(use_scale=True)([h, h])\n    f = Activation('relu')(s)\n    out = Dense(1)(f)  # Assuming a regression task\n\n    model = Model(inputs=inp, outputs=out)\n    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n    model.compile(optimizer=optimizer, loss='mean_squared_error')  # You can use other loss functions for regression\n    \n    return model\n\n# Implement Early Stopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n\n# Objective function for Optuna\ndef objective(trial):\n    units1 = trial.suggest_int('units1', 32, 128)\n    units2 = trial.suggest_int('units2', 16, 64)\n    learning_rate = trial.suggest_loguniform('learning_rate', 0.001, 0.1)\n\n    model = build_model(units1=units1, units2=units2, learning_rate=learning_rate)\n    \n    history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.1, callbacks=[early_stopping], verbose=0)\n    \n    return mean_absolute_error(scaler_y.inverse_transform(y_test.reshape(-1, 1)), scaler_y.inverse_transform(model.predict(X_test).reshape(-1, 1)))\n\n# Perform hyperparameter tuning with Optuna\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=10)\n\n# Get the best hyperparameters\nbest_params = study.best_params\n\n# Train the final model with the best hyperparameters\nbest_model = build_model(**best_params)\nbest_model.fit(X_train, y_train, epochs=200, batch_size=32, validation_split=0.1, callbacks=[early_stopping])\n\n# Plot the training and validation loss\nplt.plot(best_model.history.history['loss'], label='Train Loss')\nplt.plot(best_model.history.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\nplt.show()\n\n# Prediction\ny_pred = best_model.predict(X_test)\n\n# Rescale predictions to the original scale\ny_pred_rescaled = scaler_y.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n\n# Rescale true labels to the original scale\ny_test_rescaled = scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten()\n\n# Evaluate mean absolute error\nmae = mean_absolute_error(y_test_rescaled, y_pred_rescaled)\nprint(f\"Mean Absolute Error: {mae}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Attention, Activation\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\n\n# Load your dataset (assuming it's in a DataFrame named 'df')\n# Replace 'Effort' with the actual column name you want to predict\ntarget_column = 'Effort'\n\n# Separate features (X) and target variable (y)\nX = df.drop(target_column, axis=1)\ny = df[target_column]\n\n# Data Preprocessing\nscaler_X = MinMaxScaler(feature_range=(0, 1))\nscaler_y = MinMaxScaler(feature_range=(0, 1))\n\nX_normalized = scaler_X.fit_transform(X.values)\ny_normalized = scaler_y.fit_transform(y.values.reshape(-1, 1))\n\n# Reshape X for LSTM input (assuming you have a sequence length)\nsequence_length = 1  # Adjust based on your dataset\nX_reshaped = X_normalized.reshape((X_normalized.shape[0], sequence_length, X_normalized.shape[1]))\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_reshaped, y_normalized, test_size=0.2, random_state=42)\n\n# Model Construction and Training\ninp = Input(shape=(X_reshaped.shape[1], X_reshaped.shape[2]))\ne = Sequential()(inp)\ng = LSTM(units=64, return_sequences=True)(e)\nh = LSTM(units=32, return_sequences=True)(g)\ns = Attention(use_scale=True)([h, h])\nf = Activation('relu')(s)\nout = Dense(1)(f)  # Assuming a regression task\n\nmodel = Model(inputs=inp, outputs=out)\nmodel.compile(optimizer='adam', loss='mean_squared_error')  # You can use other loss functions for regression\n\n# Implement Early Stopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n\n# Train the model and monitor the training history\nhistory = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.1, callbacks=[early_stopping])\n\n# Plot the training and validation loss\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\nplt.show()\n\n# Prediction\ny_pred = model.predict(X_test)\n\n# Rescale predictions to the original scale\ny_pred_rescaled = scaler_y.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n\n# Rescale true labels to the original scale\ny_test_rescaled = scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten()\n\n# Evaluate root mean squared error (RMSE)\nrmse = np.sqrt(mean_squared_error(y_test_rescaled, y_pred_rescaled))\nprint(f\"Root Mean Squared Error (RMSE): {rmse}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Attention, Activation\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.metrics import mean_squared_error\nimport optuna\n\n# Load your dataset (assuming it's in a DataFrame named 'df')\n# Replace 'Effort' with the actual column name you want to predict\ntarget_column = 'Effort'\n\n# Separate features (X) and target variable (y)\nX = df.drop(target_column, axis=1)\ny = df[target_column]\n\n# Data Preprocessing\nscaler_X = MinMaxScaler(feature_range=(0, 1))\nscaler_y = MinMaxScaler(feature_range=(0, 1))\n\nX_normalized = scaler_X.fit_transform(X.values)\ny_normalized = scaler_y.fit_transform(y.values.reshape(-1, 1))\n\n# Reshape X for LSTM input (assuming you have a sequence length)\nsequence_length = 1  # Adjust based on your dataset\nX_reshaped = X_normalized.reshape((X_normalized.shape[0], sequence_length, X_normalized.shape[1]))\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_reshaped, y_normalized, test_size=0.2, random_state=42)\n\n# Model Construction and Training\ndef build_model(units1, units2, learning_rate):\n    inp = Input(shape=(X_reshaped.shape[1], X_reshaped.shape[2]))\n    e = Sequential()(inp)\n    g = LSTM(units=units1, return_sequences=True)(e)\n    h = LSTM(units=units2, return_sequences=True)(g)\n    s = Attention(use_scale=True)([h, h])\n    f = Activation('relu')(s)\n    out = Dense(1)(f)  # Assuming a regression task\n\n    model = Model(inputs=inp, outputs=out)\n    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n    model.compile(optimizer=optimizer, loss='mean_squared_error')  # You can use other loss functions for regression\n    \n    return model\n\n# Implement Early Stopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n\n# Objective function for Optuna\ndef objective(trial):\n    units1 = trial.suggest_int('units1', 32, 128)\n    units2 = trial.suggest_int('units2', 16, 64)\n    learning_rate = trial.suggest_loguniform('learning_rate', 0.001, 0.1)\n\n    model = build_model(units1=units1, units2=units2, learning_rate=learning_rate)\n    \n    history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.1, callbacks=[early_stopping], verbose=0)\n    \n    return mean_absolute_error(scaler_y.inverse_transform(y_test.reshape(-1, 1)), scaler_y.inverse_transform(model.predict(X_test).reshape(-1, 1)))\n\n# Perform hyperparameter tuning with Optuna\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=10)\n\n# Get the best hyperparameters\nbest_params = study.best_params\n\n# Train the final model with the best hyperparameters\nbest_model = build_model(**best_params)\nbest_model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.1, callbacks=[early_stopping])\n\n# Plot the training and validation loss\nplt.plot(best_model.history.history['loss'], label='Train Loss')\nplt.plot(best_model.history.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\nplt.show()\n\n# Prediction\ny_pred = best_model.predict(X_test)\n\n# Rescale predictions to the original scale\ny_pred_rescaled = scaler_y.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n\n# Rescale true labels to the original scale\ny_test_rescaled = scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten()\n\n# Evaluate root mean squared error (RMSE)\nrmse = np.sqrt(mean_squared_error(y_test_rescaled, y_pred_rescaled))\nprint(f\"Root Mean Squared Error (RMSE): {rmse}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Attention, Activation\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport optuna\n\n# Load your dataset (assuming it's in a DataFrame named 'df')\n# Replace 'Effort' with the actual column name you want to predict\ntarget_column = 'Effort'\n\n# Separate features (X) and target variable (y)\nX = df.drop(target_column, axis=1)\ny = df[target_column]\n\n# Data Preprocessing\nscaler_X = MinMaxScaler(feature_range=(0, 1))\nscaler_y = MinMaxScaler(feature_range=(0, 1))\n\nX_normalized = scaler_X.fit_transform(X.values)\ny_normalized = scaler_y.fit_transform(y.values.reshape(-1, 1))\n\n# Reshape X for LSTM input (assuming you have a sequence length)\nsequence_length = 1  # Adjust based on your dataset\nX_reshaped = X_normalized.reshape((X_normalized.shape[0], sequence_length, X_normalized.shape[1]))\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_reshaped, y_normalized, test_size=0.2, random_state=42)\n\n# Model Construction and Training\ndef build_model(units1, units2, learning_rate):\n    inp = Input(shape=(X_reshaped.shape[1], X_reshaped.shape[2]))\n    e = Sequential()(inp)\n    g = LSTM(units=units1, return_sequences=True)(e)\n    h = LSTM(units=units2, return_sequences=True)(g)\n    s = Attention(use_scale=True)([h, h])\n    f = Activation('relu')(s)\n    out = Dense(1)(f)  # Assuming a regression task\n\n    model = Model(inputs=inp, outputs=out)\n    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n    model.compile(optimizer=optimizer, loss='mean_squared_error')  # You can use other loss functions for regression\n    \n    return model\n\n# Implement Early Stopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n\n# Objective function for Optuna\ndef objective(trial):\n    units1 = trial.suggest_int('units1', 32, 128)\n    units2 = trial.suggest_int('units2', 16, 64)\n    learning_rate = trial.suggest_loguniform('learning_rate', 0.001, 0.1)\n\n    model = build_model(units1=units1, units2=units2, learning_rate=learning_rate)\n    \n    history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.1, callbacks=[early_stopping], verbose=0)\n    \n    return mean_absolute_error(scaler_y.inverse_transform(y_test.reshape(-1, 1)), scaler_y.inverse_transform(model.predict(X_test).reshape(-1, 1)))\n\n# Perform hyperparameter tuning with Optuna\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=10)\n\n# Get the best hyperparameters\nbest_params = study.best_params\n\n# Train the final model with the best hyperparameters\nbest_model = build_model(**best_params)\nbest_model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.1, callbacks=[early_stopping])\n\n# Plot the training and validation loss\nplt.plot(best_model.history.history['loss'], label='Train Loss')\nplt.plot(best_model.history.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\nplt.show()\n\n# Prediction\ny_pred = best_model.predict(X_test)\n\n# Rescale predictions to the original scale\ny_pred_rescaled = scaler_y.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n\n# Rescale true labels to the original scale\ny_test_rescaled = scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten()\n\n# Evaluate R-squared\nr_squared = r2_score(y_test_rescaled, y_pred_rescaled)\nprint(f\"R-squared: {r_squared}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
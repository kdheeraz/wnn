{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6801955,"sourceType":"datasetVersion","datasetId":3909889}],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline,make_pipeline\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Bidirectional\nfrom matplotlib import pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras import backend as K","metadata":{"execution":{"iopub.status.busy":"2024-07-05T15:09:39.204516Z","iopub.execute_input":"2024-07-05T15:09:39.204965Z","iopub.status.idle":"2024-07-05T15:09:43.430539Z","shell.execute_reply.started":"2024-07-05T15:09:39.204932Z","shell.execute_reply":"2024-07-05T15:09:43.429707Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"CFG = [\n    '/kaggle/input/software-effort-estimation-datasets/albrecht.csv',\n    '/kaggle/input/software-effort-estimation-datasets/china.csv',\n    '/kaggle/input/software-effort-estimation-datasets/desharnais.csv',\n    '/kaggle/input/software-effort-estimation-datasets/finnish.csv',\n    '/kaggle/input/software-effort-estimation-datasets/isbsg10.csv',\n    '/kaggle/input/software-effort-estimation-datasets/kemerer.csv',\n    '/kaggle/input/software-effort-estimation-datasets/kitchenham.csv',\n    '/kaggle/input/software-effort-estimation-datasets/maxwell.csv',\n    '/kaggle/input/software-effort-estimation-datasets/miyazaki94.csv'\n]","metadata":{"execution":{"iopub.status.busy":"2024-07-05T15:10:11.978033Z","iopub.execute_input":"2024-07-05T15:10:11.978747Z","iopub.status.idle":"2024-07-05T15:10:11.984355Z","shell.execute_reply.started":"2024-07-05T15:10:11.978700Z","shell.execute_reply":"2024-07-05T15:10:11.983336Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"code","source":"def data_albrecht():\n    df = pd.read_csv(CFG[0])\n    df_for_training = df.astype(float)\n    #LSTM uses sigmoid and tanh that are sensitive to magnitude so values need to be normalized\n    # normalize the dataset\n    scaler = MinMaxScaler()\n    scaler = scaler.fit(df_for_training)\n    df_for_training_scaled = scaler.transform(df_for_training)\n    return df\n\n\ndef data_china():\n    \n    df = pd.read_csv(CFG[1])\n    df = df.drop(columns=['id','ID'])\n    df_for_training = df.astype(float)\n#     print(df.columns)\n    #LSTM uses sigmoid and tanh that are sensitive to magnitude so values need to be normalized\n    # normalize the dataset\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    scaler = scaler.fit(df_for_training)\n    df_for_training_scaled = scaler.transform(df_for_training)\n    \n    trainX = []\n    trainY = []\n    n_future = 2   # Number of days we want to look into the future based on the past days.\n    n_past = 2 # Number of past days we want to use to predict the future.\n\n    for i in range(n_past, len(df_for_training_scaled) - n_future +1):\n        trainX.append(df_for_training_scaled[i - n_past:i, 0:df_for_training.shape[1]])\n        trainY.append(df_for_training_scaled[i + n_future - 1:i + n_future, df_for_training.shape[1] - 1])\n\n    trainX, trainY = np.array(trainX), np.array(trainY)\n\n    return trainX, trainY\n\n\ndef data_desharnais():\n    df = pd.read_csv(CFG[2])\n    df = df.drop(columns=['Project','YearEnd', 'Language'])\n    df_for_training = df.astype(float)\n    #LSTM uses sigmoid and tanh that are sensitive to magnitude so values need to be normalized\n    # normalize the dataset\n    scaler = StandardScaler()\n    scaler = scaler.fit(df_for_training)\n    df_for_training_scaled = scaler.transform(df_for_training)\n    return df\n\n\ndef data_finnish():\n    df = pd.read_csv(CFG[3])\n    df_for_training = df.astype(float)\n    #LSTM uses sigmoid and tanh that are sensitive to magnitude so values need to be normalized\n    # normalize the dataset\n    scaler = MinMaxScaler()\n    scaler = scaler.fit(df_for_training)\n    df_for_training_scaled = scaler.transform(df_for_training)\n    return df\n\n\ndef data_isbsg10():\n\n    df = pd.read_csv(CFG[4])\n    df = df.drop(columns = ['id', 'ID', 'Data_Quality', 'Year','AG', 'N_effort_level1',\n           'N_PDR1', 'SDR', 'PET', 'PIT', 'I_Date', 'PAS',\n           'Recording_Method', 'Resource_Level', 'MTS', 'ATS', 'R_PWE_NPA',\n           'P_UWE', 'CASE_Tool', 'UM', 'HMA', 'Hardware1',\n           'IDE', 'DT1', 'DBS1', 'CS1', 'WS1', 'MS1', 'OP1', 'RTA',\n           'SP_CMMI', 'SP_ISO', 'SP_TICKIT', 'MIN_Defects', 'MAJ_Defects',\n           'X_Defects', 'TOT_Defects', 'UB_BU', 'UB_L', 'UB_DU', 'UB_CU',\n           'IMarket', 'T_Platform', 'D_Embedded', 'SE', 'SEA', 'SEM', 'E_Estimate',\n           'E_Estimate_Method', 'DDE', 'DDEM', 'C_Estimate', 'CEC', 'CEM',\n           'E_Tool', 'E_Comments', 'EC_Date', 'SR?', 'SR', 'R_FPC', 'R_FPA',\n           'P_Defects', 'D_Defects', 'MIN_B_Defects', 'MAJ_B_Defects',\n           'X_B_Defects', 'TOT_B_Defects', 'MIN_T_Defects', 'MAJ_T_Defects',\n           'X_T_Defects', 'TOT_T_Defects','S_Defects', 'MIN_I_Defects', 'MAJ_I_Defects',\n           'X_I_Defects', 'TOT_I_Defects'], axis = 1)\n    # Replace \"?\" with numpy.nan\n    df.replace('?', np.nan, inplace=True)\n#     print(df.columns)\n    \n    # imputation transformer\n    trf1 = ColumnTransformer([\n        ('impute',SimpleImputer(strategy='constant', fill_value='Missing'),[14,15,16,17,18])\n    ],remainder='passthrough')\n    \n    trf2 = ColumnTransformer([\n    ('ohe',OneHotEncoder(sparse_output=False,drop= 'first'),[0,1,2,3,4,5,6,7,8,9,10,11,12,13,15])\n    ],remainder='passthrough')\n    \n    pipe = Pipeline([\n    ('trf1',trf1),\n    ('trf2',trf2),\n    ])\n    \n    pipe.fit_transform(df)\n    \n    return pipe.fit_transform(df)\n\n\ndef data_kemerer():\n    df = pd.read_csv(CFG[5])\n    df_for_training = df.astype(float)\n    #LSTM uses sigmoid and tanh that are sensitive to magnitude so values need to be normalized\n    # normalize the dataset\n    scaler = StandardScaler()\n    scaler = scaler.fit(df_for_training)\n    df_for_training_scaled = scaler.transform(df_for_training)\n    return df\n    \n\n    \ndef data_kitchenham():\n    df = pd.read_csv(CFG[6])\n    df_for_training = df.astype(float)\n    #LSTM uses sigmoid and tanh that are sensitive to magnitude so values need to be normalized\n    # normalize the dataset\n    scaler = MinMaxScaler()\n    scaler = scaler.fit(df_for_training)\n    df_for_training_scaled = scaler.transform(df_for_training)\n    return df\n\n\ndef data_maxwell():\n    df = pd.read_csv(CFG[7])\n    df_for_training = df.astype(float)\n    #LSTM uses sigmoid and tanh that are sensitive to magnitude so values need to be normalized\n    # normalize the dataset\n    scaler = StandardScaler()\n    scaler = scaler.fit(df_for_training)\n    df_for_training_scaled = scaler.transform(df_for_training)\n    return df\n\n\ndef data_miyazaki():\n    df = pd.read_csv(CFG[8])\n    df_for_training = df.astype(float)\n    #LSTM uses sigmoid and tanh that are sensitive to magnitude so values need to be normalized\n    # normalize the dataset\n    scaler = StandardScaler()\n    scaler = scaler.fit(df_for_training)\n    df_for_training_scaled = scaler.transform(df_for_training)\n    return df\n","metadata":{"execution":{"iopub.status.busy":"2024-07-05T15:10:25.285049Z","iopub.execute_input":"2024-07-05T15:10:25.286081Z","iopub.status.idle":"2024-07-05T15:10:25.310171Z","shell.execute_reply.started":"2024-07-05T15:10:25.286026Z","shell.execute_reply":"2024-07-05T15:10:25.309058Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import pywt","metadata":{"execution":{"iopub.status.busy":"2024-07-05T15:10:27.561127Z","iopub.execute_input":"2024-07-05T15:10:27.561889Z","iopub.status.idle":"2024-07-05T15:10:27.584104Z","shell.execute_reply.started":"2024-07-05T15:10:27.561852Z","shell.execute_reply":"2024-07-05T15:10:27.583279Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Define the Morlet wavelet activation function\ndef morlet_wavelet(x, w=5.0):\n    return np.cos(w * x) * np.exp(-x**2 / 2)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-05T15:10:29.466363Z","iopub.execute_input":"2024-07-05T15:10:29.466739Z","iopub.status.idle":"2024-07-05T15:10:29.471830Z","shell.execute_reply.started":"2024-07-05T15:10:29.466708Z","shell.execute_reply":"2024-07-05T15:10:29.470774Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Layer","metadata":{"execution":{"iopub.status.busy":"2024-07-05T15:10:31.855145Z","iopub.execute_input":"2024-07-05T15:10:31.856065Z","iopub.status.idle":"2024-07-05T15:10:31.860254Z","shell.execute_reply.started":"2024-07-05T15:10:31.856028Z","shell.execute_reply":"2024-07-05T15:10:31.859282Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class MorletWaveletLayer(Layer):\n    def __init__(self, units, w=5.0, **kwargs):\n        super(MorletWaveletLayer, self).__init__(**kwargs)\n        self.units = units\n        self.w = w\n\n    def build(self, input_shape):\n        self.kernel = self.add_weight(name='kernel',\n                                      shape=(input_shape[-1], self.units),\n                                      initializer='glorot_uniform',\n                                      trainable=True)\n\n    def call(self, inputs):\n        # Reshape inputs to 2D for the dense operation\n        input_reshaped = tf.reshape(inputs, [-1, inputs.shape[-1]])\n        z = tf.matmul(input_reshaped, self.kernel)\n\n        # Implement Morlet wavelet using TensorFlow operations\n        wavelet_output = tf.cos(self.w * z) * tf.exp(-z**2 / 2)\n\n        # Reshape back to 3D\n        return tf.reshape(wavelet_output, tf.concat([tf.shape(inputs)[:-1], [self.units]], axis=0))","metadata":{"execution":{"iopub.status.busy":"2024-07-05T15:57:08.037490Z","iopub.execute_input":"2024-07-05T15:57:08.037880Z","iopub.status.idle":"2024-07-05T15:57:08.046835Z","shell.execute_reply.started":"2024-07-05T15:57:08.037846Z","shell.execute_reply":"2024-07-05T15:57:08.045647Z"},"trusted":true},"execution_count":110,"outputs":[]},{"cell_type":"code","source":"class MorletWaveletLayer(Layer):\n    def __init__(self, units, w=5.0, **kwargs):\n        super(MorletWaveletLayer, self).__init__(**kwargs)\n        self.units = units\n        self.w = w\n\n    def build(self, input_shape):\n        self.kernel = self.add_weight(name='kernel',\n                              shape=(input_shape[-1], self.units),\n                              initializer='glorot_uniform',\n                              trainable=True)\n\n    def call(self, inputs):\n        # Reshape inputs to 2D for the dense operation\n        input_reshaped = tf.reshape(inputs, [-1, inputs.shape[-1]])\n        z = tf.matmul(input_reshaped, self.kernel)\n\n        wavelet_output = tf.cos(self.w * z) * tf.exp(-z**2 / 2)\n\n        # Reshape back to 3D with the appropriate output shape\n        return wavelet_output\n        #return tf.reshape(wavelet_output, [-1, tf.shape(inputs)[1], self.units])","metadata":{"execution":{"iopub.status.busy":"2024-07-05T15:52:32.838587Z","iopub.execute_input":"2024-07-05T15:52:32.838956Z","iopub.status.idle":"2024-07-05T15:52:32.847647Z","shell.execute_reply.started":"2024-07-05T15:52:32.838928Z","shell.execute_reply":"2024-07-05T15:52:32.846240Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"code","source":"totalX, totalY = data_china()","metadata":{"execution":{"iopub.status.busy":"2024-07-05T15:10:53.669706Z","iopub.execute_input":"2024-07-05T15:10:53.670059Z","iopub.status.idle":"2024-07-05T15:10:53.687626Z","shell.execute_reply.started":"2024-07-05T15:10:53.670032Z","shell.execute_reply":"2024-07-05T15:10:53.686813Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"trainX.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Firefly Algorithm for optimization\nclass FireflyAlgorithmx:\n    def __init__(self, func, lb, ub, dim, n_fireflies=20, max_iter=100, alpha=0.5, beta=0.2, gamma=1.0):\n        self.func = func\n        self.lb = lb\n        self.ub = ub\n        self.dim = dim\n        self.n_fireflies = n_fireflies\n        self.max_iter = max_iter\n        self.alpha = alpha\n        self.beta = beta\n        self.gamma = gamma\n\n    def optimize(self):\n        fireflies = np.random.uniform(self.lb, self.ub, (self.n_fireflies, self.dim))\n        fitness = np.apply_along_axis(self.func, 1, fireflies)\n        \n        best_firefly = fireflies[np.argmin(fitness)]\n        best_fitness = np.min(fitness)\n\n        for t in range(self.max_iter):\n            for i in range(self.n_fireflies):\n                for j in range(self.n_fireflies):\n                    if fitness[j] < fitness[i]:\n                        r = np.linalg.norm(fireflies[i] - fireflies[j])\n                        beta = self.beta * np.exp(-self.gamma * r ** 2)\n                        fireflies[i] += beta * (fireflies[j] - fireflies[i]) + self.alpha * (np.random.rand(self.dim) - 0.5)\n                        fireflies[i] = np.clip(fireflies[i], self.lb, self.ub)\n                        fitness[i] = self.func(fireflies[i])\n                        \n                        if fitness[i] < best_fitness:\n                            best_firefly = fireflies[i]\n                            best_fitness = fitness[i]\n\n            self.alpha *= 0.97  # Decrease alpha over time\n        \n        return best_firefly, best_fitness","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def objective_function(weights, model, X_train, y_train):\n    model.set_weights(weights)\n    predictions = model.predict(X_train)\n    return mean_squared_error(y_train, predictions)","metadata":{"execution":{"iopub.status.busy":"2024-07-05T15:11:00.003841Z","iopub.execute_input":"2024-07-05T15:11:00.004293Z","iopub.status.idle":"2024-07-05T15:11:00.009402Z","shell.execute_reply.started":"2024-07-05T15:11:00.004256Z","shell.execute_reply":"2024-07-05T15:11:00.008421Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from fireflyalgorithm import FireflyAlgorithm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FA = FireflyAlgorithm()\nbest = FA.run(function=lambda weights: objective_function(weights, wnn, trainX, trainY), dim=10, lb=-5, ub=5, max_evals=10000)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"    firefly_optimizer = FireflyAlgorithm(\n        func=lambda weights: objective_function(weights, wnn, trainX, trainY),\n        lb=-1.0,\n        ub=1.0,\n        dim=wnn.count_params(),\n        n_fireflies=20,\n        max_iter=100\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"    # Create the Wavelet Neural Network\n    input_shape = trainX.shape[1:]  # Assuming trainX.shape is (496, 2, 18), input_shape should be (2, 18)\n    wnn = Sequential([\n        BatchNormalization(input_shape=input_shape),\n        Dense(64, activation='relu'),\n        Dropout(0.1),\n        Dense(32, activation='relu'),\n        MorletWaveletLayer(10),  # Replace with appropriate units for your use case\n        Dense(trainY.shape[1])  # Adjust output dim to match trainY.shape[1]\n    ])","metadata":{"execution":{"iopub.status.busy":"2024-07-05T15:54:24.773960Z","iopub.execute_input":"2024-07-05T15:54:24.774371Z","iopub.status.idle":"2024-07-05T15:54:24.877106Z","shell.execute_reply.started":"2024-07-05T15:54:24.774335Z","shell.execute_reply":"2024-07-05T15:54:24.875933Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"code","source":"testY.shape[1]","metadata":{"execution":{"iopub.status.busy":"2024-07-05T15:54:37.361610Z","iopub.execute_input":"2024-07-05T15:54:37.362002Z","iopub.status.idle":"2024-07-05T15:54:37.368281Z","shell.execute_reply.started":"2024-07-05T15:54:37.361970Z","shell.execute_reply":"2024-07-05T15:54:37.367180Z"},"trusted":true},"execution_count":105,"outputs":[{"execution_count":105,"output_type":"execute_result","data":{"text/plain":"1"},"metadata":{}}]},{"cell_type":"code","source":"    # Optimize the model weights using Firefly Algorithm\n    best_weights, best_fitness = firefly_optimizer.optimize()\n    print(best_weights)\n    wnn.set_weights(best_weights)\n\n    # Evaluate the optimized model\n    predictions = wnn.predict(X_test)\n    mse = mean_squared_error(y_test, predictions)\n    print(f'Test Mean Squared Error: {mse}')\n\n    # Plot the results\n    import matplotlib.pyplot as plt\n\n    plt.figure(figsize=(12, 6))\n    plt.plot(y_test.values, label='Actual Effort')\n    plt.plot(predictions, label='Predicted Effort')\n    plt.legend()\n    plt.title('Wavelet Neural Network Predictions with Firefly Algorithm')\n    plt.xlabel('Sample')\n    plt.ylabel('Effort')\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2024-07-05T15:24:56.870786Z","iopub.execute_input":"2024-07-05T15:24:56.871836Z","iopub.status.idle":"2024-07-05T15:24:56.877525Z","shell.execute_reply.started":"2024-07-05T15:24:56.871788Z","shell.execute_reply":"2024-07-05T15:24:56.876310Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"totalX.shape","metadata":{"execution":{"iopub.status.busy":"2024-07-05T15:42:45.822951Z","iopub.execute_input":"2024-07-05T15:42:45.823816Z","iopub.status.idle":"2024-07-05T15:42:45.830320Z","shell.execute_reply.started":"2024-07-05T15:42:45.823781Z","shell.execute_reply":"2024-07-05T15:42:45.829235Z"},"trusted":true},"execution_count":83,"outputs":[{"execution_count":83,"output_type":"execute_result","data":{"text/plain":"(496, 2, 18)"},"metadata":{}}]},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    trainX, testX, trainY, testY = train_test_split(totalX, totalY, test_size=0.25, random_state=42)\n    # Print the shape of trainX and trainY\n    print('trainX shape == {}.'.format(trainX.shape))\n    print('trainY shape == {}.'.format(trainY.shape))\n    \n    print('testX shape == {}.'.format(testX.shape))\n    print('testY shape == {}.'.format(testY.shape))\n\n    # Create the Wavelet Neural Network\n    input_shape = trainX.shape[1:]  # Assuming trainX.shape is (496, 2, 18), input_shape should be (2, 18)\n    wnn = Sequential([\n        BatchNormalization(input_shape=input_shape),\n        Dense(64, activation='relu'),\n        Dropout(0.1),\n        Dense(32, activation='relu'),\n        MorletWaveletLayer(10),  # Replace with appropriate units for your use case\n        Dense(units=1, activation='linear')  # Adjust output dim to match trainY.shape[1]\n    ])\n\n    # Compile the model\n    wnn.compile(optimizer='adam', loss='mse')\n    wnn.summary()\n\n    # Train the model\n    #history = wnn.fit(trainX, trainY, epochs=100, batch_size=32, validation_split=0.25, verbose=1)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-05T15:54:53.425473Z","iopub.execute_input":"2024-07-05T15:54:53.426319Z","iopub.status.idle":"2024-07-05T15:54:53.562153Z","shell.execute_reply.started":"2024-07-05T15:54:53.426287Z","shell.execute_reply":"2024-07-05T15:54:53.561099Z"},"trusted":true},"execution_count":106,"outputs":[{"name":"stdout","text":"trainX shape == (372, 2, 18).\ntrainY shape == (372, 1).\ntestX shape == (124, 2, 18).\ntestY shape == (124, 1).\nModel: \"sequential_20\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n batch_normalization_22 (Bat  (None, 2, 18)            72        \n chNormalization)                                                \n                                                                 \n dense_58 (Dense)            (None, 2, 64)             1216      \n                                                                 \n dropout_22 (Dropout)        (None, 2, 64)             0         \n                                                                 \n dense_59 (Dense)            (None, 2, 32)             2080      \n                                                                 \n morlet_wavelet_layer_19 (Mo  (None, 10)               320       \n rletWaveletLayer)                                               \n                                                                 \n dense_60 (Dense)            (None, 1)                 11        \n                                                                 \n=================================================================\nTotal params: 3,699\nTrainable params: 3,663\nNon-trainable params: 36\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install fireflyalgorithm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"    history = wnn.fit(trainX, trainY, epochs=100, batch_size=32, verbose=0)\n    \n    # Evaluate model\n    predictions = wnn.predict(testX)\n    ","metadata":{"execution":{"iopub.status.busy":"2024-07-05T15:55:02.614011Z","iopub.execute_input":"2024-07-05T15:55:02.614902Z","iopub.status.idle":"2024-07-05T15:55:03.814013Z","shell.execute_reply.started":"2024-07-05T15:55:02.614866Z","shell.execute_reply":"2024-07-05T15:55:03.812270Z"},"trusted":true},"execution_count":107,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)","Cell \u001b[0;32mIn[107], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mwnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Evaluate model\u001b[39;00m\n\u001b[1;32m      4\u001b[0m predictions \u001b[38;5;241m=\u001b[39m wnn\u001b[38;5;241m.\u001b[39mpredict(testX)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'mean_squared_error/SquaredDifference' defined at (most recent call last):\n    File \"/opt/conda/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/opt/conda/lib/python3.10/runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"/opt/conda/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/opt/conda/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 728, in start\n      self.io_loop.start()\n    File \"/opt/conda/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"/opt/conda/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 513, in dispatch_queue\n      await self.process_one()\n    File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 502, in process_one\n      await dispatch(*args)\n    File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 409, in dispatch_shell\n      await result\n    File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"/opt/conda/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"/opt/conda/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_5953/2142505836.py\", line 1, in <module>\n      history = wnn.fit(trainX, trainY, epochs=100, batch_size=32, verbose=0)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1051, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1109, in compute_loss\n      return self.compiled_loss(\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/losses.py\", line 142, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/losses.py\", line 268, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/losses.py\", line 1470, in mean_squared_error\n      return backend.mean(tf.math.squared_difference(y_pred, y_true), axis=-1)\nNode: 'mean_squared_error/SquaredDifference'\nrequired broadcastable shapes\n\t [[{{node mean_squared_error/SquaredDifference}}]] [Op:__inference_train_function_90801]"],"ename":"InvalidArgumentError","evalue":"Graph execution error:\n\nDetected at node 'mean_squared_error/SquaredDifference' defined at (most recent call last):\n    File \"/opt/conda/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/opt/conda/lib/python3.10/runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"/opt/conda/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/opt/conda/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 728, in start\n      self.io_loop.start()\n    File \"/opt/conda/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"/opt/conda/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 513, in dispatch_queue\n      await self.process_one()\n    File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 502, in process_one\n      await dispatch(*args)\n    File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 409, in dispatch_shell\n      await result\n    File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"/opt/conda/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"/opt/conda/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_5953/2142505836.py\", line 1, in <module>\n      history = wnn.fit(trainX, trainY, epochs=100, batch_size=32, verbose=0)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1051, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1109, in compute_loss\n      return self.compiled_loss(\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/losses.py\", line 142, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/losses.py\", line 268, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/losses.py\", line 1470, in mean_squared_error\n      return backend.mean(tf.math.squared_difference(y_pred, y_true), axis=-1)\nNode: 'mean_squared_error/SquaredDifference'\nrequired broadcastable shapes\n\t [[{{node mean_squared_error/SquaredDifference}}]] [Op:__inference_train_function_90801]","output_type":"error"}]},{"cell_type":"code","source":"testY.shape","metadata":{"execution":{"iopub.status.busy":"2024-07-05T15:49:40.462587Z","iopub.execute_input":"2024-07-05T15:49:40.463426Z","iopub.status.idle":"2024-07-05T15:49:40.469440Z","shell.execute_reply.started":"2024-07-05T15:49:40.463389Z","shell.execute_reply":"2024-07-05T15:49:40.468484Z"},"trusted":true},"execution_count":91,"outputs":[{"execution_count":91,"output_type":"execute_result","data":{"text/plain":"(124, 1)"},"metadata":{}}]},{"cell_type":"code","source":"predictions[0]","metadata":{"execution":{"iopub.status.busy":"2024-07-05T15:39:54.104188Z","iopub.execute_input":"2024-07-05T15:39:54.104577Z","iopub.status.idle":"2024-07-05T15:39:54.111478Z","shell.execute_reply.started":"2024-07-05T15:39:54.104545Z","shell.execute_reply":"2024-07-05T15:39:54.110508Z"},"trusted":true},"execution_count":82,"outputs":[{"execution_count":82,"output_type":"execute_result","data":{"text/plain":"array([[0.07274467],\n       [0.06709348]], dtype=float32)"},"metadata":{}}]},{"cell_type":"code","source":"if predictions.ndim == 3:\n        predictions = np.squeeze(predictions)  # Remove the last dimension","metadata":{"execution":{"iopub.status.busy":"2024-07-05T15:38:21.581324Z","iopub.execute_input":"2024-07-05T15:38:21.581758Z","iopub.status.idle":"2024-07-05T15:38:21.586933Z","shell.execute_reply.started":"2024-07-05T15:38:21.581724Z","shell.execute_reply":"2024-07-05T15:38:21.585725Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"predictions.shape","metadata":{"execution":{"iopub.status.busy":"2024-07-05T15:38:23.336757Z","iopub.execute_input":"2024-07-05T15:38:23.337150Z","iopub.status.idle":"2024-07-05T15:38:23.343855Z","shell.execute_reply.started":"2024-07-05T15:38:23.337118Z","shell.execute_reply":"2024-07-05T15:38:23.342782Z"},"trusted":true},"execution_count":77,"outputs":[{"execution_count":77,"output_type":"execute_result","data":{"text/plain":"(124, 2)"},"metadata":{}}]},{"cell_type":"code","source":"mse = mean_squared_error(testY, predictions)","metadata":{"execution":{"iopub.status.busy":"2024-07-05T15:38:25.712874Z","iopub.execute_input":"2024-07-05T15:38:25.713532Z","iopub.status.idle":"2024-07-05T15:38:25.784212Z","shell.execute_reply.started":"2024-07-05T15:38:25.713484Z","shell.execute_reply":"2024-07-05T15:38:25.782622Z"},"trusted":true},"execution_count":78,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[78], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mse \u001b[38;5;241m=\u001b[39m \u001b[43mmean_squared_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtestY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_regression.py:442\u001b[0m, in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmean_squared_error\u001b[39m(\n\u001b[1;32m    383\u001b[0m     y_true, y_pred, \u001b[38;5;241m*\u001b[39m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, multioutput\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muniform_average\u001b[39m\u001b[38;5;124m\"\u001b[39m, squared\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    384\u001b[0m ):\n\u001b[1;32m    385\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Mean squared error regression loss.\u001b[39;00m\n\u001b[1;32m    386\u001b[0m \n\u001b[1;32m    387\u001b[0m \u001b[38;5;124;03m    Read more in the :ref:`User Guide <mean_squared_error>`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;124;03m    0.825...\u001b[39;00m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 442\u001b[0m     y_type, y_true, y_pred, multioutput \u001b[38;5;241m=\u001b[39m \u001b[43m_check_reg_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m     check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    446\u001b[0m     output_errors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39maverage((y_true \u001b[38;5;241m-\u001b[39m y_pred) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, weights\u001b[38;5;241m=\u001b[39msample_weight)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_regression.py:111\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[1;32m    108\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m y_pred\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_true\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m y_pred\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m--> 111\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    112\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true and y_pred have different number of output (\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m!=\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    113\u001b[0m             y_true\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], y_pred\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    114\u001b[0m         )\n\u001b[1;32m    115\u001b[0m     )\n\u001b[1;32m    117\u001b[0m n_outputs \u001b[38;5;241m=\u001b[39m y_true\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    118\u001b[0m allowed_multioutput_str \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_values\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muniform_average\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvariance_weighted\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mValueError\u001b[0m: y_true and y_pred have different number of output (1!=2)"],"ename":"ValueError","evalue":"y_true and y_pred have different number of output (1!=2)","output_type":"error"}]},{"cell_type":"code","source":"    # Plot training history\n    plt.plot(history.history['loss'], label='Training loss')\n    plt.plot(history.history['val_loss'], label='Validation loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score","metadata":{"execution":{"iopub.status.busy":"2024-07-05T15:14:52.516955Z","iopub.execute_input":"2024-07-05T15:14:52.517360Z","iopub.status.idle":"2024-07-05T15:14:52.522127Z","shell.execute_reply.started":"2024-07-05T15:14:52.517327Z","shell.execute_reply":"2024-07-05T15:14:52.521010Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def mean_magnitude_relative_error(y_true, y_pred):\n    # Calculate relative error\n    relative_error = np.abs((y_true - y_pred) / y_true)\n    \n    # Calculate mean magnitude of relative error (MMRE)\n    mmre = np.mean(relative_error)\n    \n    return mmre\n\nif __name__ == \"__main__\":\n    # Load or generate your data\n    X, y = data_china()\n\n    # Assuming trainX and trainY are defined appropriately\n    trainX = X[:400]  # Example: using first 400 samples for training\n    trainY = y[:400]  # Example: using first 400 targets for training\n    testX = X[400:]   # Example: using remaining samples for testing\n    testY = y[400:]   # Example: using remaining targets for testing\n\n    # Create the Wavelet Neural Network\n    input_shape = trainX.shape[1:]  # Assuming trainX.shape is (496, 2, 18), input_shape should be (2, 18)\n    wnn = Sequential([\n        BatchNormalization(input_shape=input_shape),\n        Dense(64, activation='relu'),\n        Dropout(0.1),\n        Dense(32, activation='relu'),\n        MorletWaveletLayer(10),  # Replace with appropriate units for your use case\n        Dense(trainY.shape[1])  # Adjust output dim to match trainY.shape[1]\n    ])\n\n    # Compile the model\n    wnn.compile(optimizer='adam', loss='mse')\n    wnn.summary()\n\n    # Train the model\n    history = wnn.fit(trainX, trainY, epochs=50, batch_size=32, validation_split=0.0, verbose=1)\n\n    # Evaluate the model on test data\n    predictions = wnn.predict(testX)\n\n    # Calculate evaluation metrics\n    mae = mean_absolute_error(testY, predictions)\n    mse = mean_squared_error(testY, predictions)\n    r2 = r2_score(testY, predictions)\n    mmre = mean_magnitude_relative_error(testY, predictions)\n\n    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n    print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n    print(f\"R-squared (R2) Score: {r2:.4f}\")\n    print(f\"Mean Magnitude of Relative Error (MMRE): {mmre:.4f}\")\n\n    # Plot training history\n    plt.plot(history.history['loss'], label='Training loss')\n    plt.plot(history.history['val_loss'], label='Validation loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-05T15:59:03.467234Z","iopub.execute_input":"2024-07-05T15:59:03.467655Z","iopub.status.idle":"2024-07-05T15:59:10.144696Z","shell.execute_reply.started":"2024-07-05T15:59:03.467621Z","shell.execute_reply":"2024-07-05T15:59:10.143271Z"},"trusted":true},"execution_count":113,"outputs":[{"name":"stdout","text":"Model: \"sequential_24\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n batch_normalization_26 (Bat  (None, 2, 18)            72        \n chNormalization)                                                \n                                                                 \n dense_70 (Dense)            (None, 2, 64)             1216      \n                                                                 \n dropout_26 (Dropout)        (None, 2, 64)             0         \n                                                                 \n dense_71 (Dense)            (None, 2, 32)             2080      \n                                                                 \n morlet_wavelet_layer_23 (Mo  (None, 2, 10)            320       \n rletWaveletLayer)                                               \n                                                                 \n dense_72 (Dense)            (None, 2, 1)              11        \n                                                                 \n=================================================================\nTotal params: 3,699\nTrainable params: 3,663\nNon-trainable params: 36\n_________________________________________________________________\nEpoch 1/50\n13/13 [==============================] - 2s 5ms/step - loss: 0.4090\nEpoch 2/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.2578\nEpoch 3/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.2016\nEpoch 4/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.1563\nEpoch 5/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.1374\nEpoch 6/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.1355\nEpoch 7/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.1190\nEpoch 8/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.1100\nEpoch 9/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.0935\nEpoch 10/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0805\nEpoch 11/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.0999\nEpoch 12/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.0810\nEpoch 13/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0787\nEpoch 14/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0733\nEpoch 15/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0662\nEpoch 16/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.0594\nEpoch 17/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0571\nEpoch 18/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0569\nEpoch 19/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0577\nEpoch 20/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0533\nEpoch 21/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.0554\nEpoch 22/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0493\nEpoch 23/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.0501\nEpoch 24/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0486\nEpoch 25/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0432\nEpoch 26/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0441\nEpoch 27/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0388\nEpoch 28/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0347\nEpoch 29/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0365\nEpoch 30/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0310\nEpoch 31/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0372\nEpoch 32/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.0363\nEpoch 33/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0317\nEpoch 34/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.0329\nEpoch 35/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.0302\nEpoch 36/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.0272\nEpoch 37/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.0259\nEpoch 38/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.0295\nEpoch 39/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.0275\nEpoch 40/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0276\nEpoch 41/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.0237\nEpoch 42/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.0238\nEpoch 43/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0239\nEpoch 44/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0214\nEpoch 45/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0243\nEpoch 46/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0215\nEpoch 47/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.0234\nEpoch 48/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.0227\nEpoch 49/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.0228\nEpoch 50/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.0220\n3/3 [==============================] - 0s 2ms/step\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[113], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m predictions \u001b[38;5;241m=\u001b[39m wnn\u001b[38;5;241m.\u001b[39mpredict(testX)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Calculate evaluation metrics\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m mae \u001b[38;5;241m=\u001b[39m \u001b[43mmean_absolute_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtestY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m mse \u001b[38;5;241m=\u001b[39m mean_squared_error(testY, predictions)\n\u001b[1;32m     44\u001b[0m r2 \u001b[38;5;241m=\u001b[39m r2_score(testY, predictions)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_regression.py:196\u001b[0m, in \u001b[0;36mmean_absolute_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmean_absolute_error\u001b[39m(\n\u001b[1;32m    142\u001b[0m     y_true, y_pred, \u001b[38;5;241m*\u001b[39m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, multioutput\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muniform_average\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    143\u001b[0m ):\n\u001b[1;32m    144\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Mean absolute error regression loss.\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \n\u001b[1;32m    146\u001b[0m \u001b[38;5;124;03m    Read more in the :ref:`User Guide <mean_absolute_error>`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;03m    0.85...\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     y_type, y_true, y_pred, multioutput \u001b[38;5;241m=\u001b[39m \u001b[43m_check_reg_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m     check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    200\u001b[0m     output_errors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39maverage(np\u001b[38;5;241m.\u001b[39mabs(y_pred \u001b[38;5;241m-\u001b[39m y_true), weights\u001b[38;5;241m=\u001b[39msample_weight, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_regression.py:102\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[1;32m    101\u001b[0m y_true \u001b[38;5;241m=\u001b[39m check_array(y_true, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m--> 102\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_true\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    105\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m y_true\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:915\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    912\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    913\u001b[0m     )\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nd \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m--> 915\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    916\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    917\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    918\u001b[0m     )\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m    921\u001b[0m     _assert_all_finite(\n\u001b[1;32m    922\u001b[0m         array,\n\u001b[1;32m    923\u001b[0m         input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[1;32m    924\u001b[0m         estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[1;32m    925\u001b[0m         allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    926\u001b[0m     )\n","\u001b[0;31mValueError\u001b[0m: Found array with dim 3. None expected <= 2."],"ename":"ValueError","evalue":"Found array with dim 3. None expected <= 2.","output_type":"error"}]},{"cell_type":"code","source":"pre=model.predict(testX)","metadata":{"execution":{"iopub.status.busy":"2024-07-05T15:59:38.068162Z","iopub.execute_input":"2024-07-05T15:59:38.068554Z","iopub.status.idle":"2024-07-05T15:59:38.140686Z","shell.execute_reply.started":"2024-07-05T15:59:38.068522Z","shell.execute_reply":"2024-07-05T15:59:38.139747Z"},"trusted":true},"execution_count":115,"outputs":[{"name":"stdout","text":"3/3 [==============================] - 0s 4ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"mse = mean_squared_error(testY, pre)","metadata":{"execution":{"iopub.status.busy":"2024-07-05T16:00:05.330820Z","iopub.execute_input":"2024-07-05T16:00:05.331166Z","iopub.status.idle":"2024-07-05T16:00:05.336368Z","shell.execute_reply.started":"2024-07-05T16:00:05.331137Z","shell.execute_reply":"2024-07-05T16:00:05.335258Z"},"trusted":true},"execution_count":116,"outputs":[]},{"cell_type":"code","source":"mse","metadata":{"execution":{"iopub.status.busy":"2024-07-05T16:00:10.399646Z","iopub.execute_input":"2024-07-05T16:00:10.400567Z","iopub.status.idle":"2024-07-05T16:00:10.407399Z","shell.execute_reply.started":"2024-07-05T16:00:10.400531Z","shell.execute_reply":"2024-07-05T16:00:10.405711Z"},"trusted":true},"execution_count":117,"outputs":[{"execution_count":117,"output_type":"execute_result","data":{"text/plain":"0.01868482579681453"},"metadata":{}}]},{"cell_type":"code","source":"# Assuming testX and testY are defined appropriately\ntestX = X[400:]   # Example: using remaining samples for testing\ntestY = y[400:]   # Example: using remaining targets for testing\n\n# Evaluate the model on test data\npredictions = wnn.predict(testX)\n\n# Reshape predictions and testY if necessary\nif predictions.ndim > 2:\n    predictions = np.squeeze(predictions)\nif testY.ndim > 2:\n    testY = np.squeeze(testY)\n\n# Calculate evaluation metrics\nmae = np.mean(np.abs(predictions - testY))\nmse = np.mean((predictions - testY)**2)\nr2 = r2_score(testY, predictions)\nmmre = mean_magnitude_relative_error(testY, predictions)\n\nprint(f\"Mean Absolute Error (MAE): {mae:.4f}\")\nprint(f\"Mean Squared Error (MSE): {mse:.4f}\")\nprint(f\"R-squared (R2) Score: {r2:.4f}\")\nprint(f\"Mean Magnitude of Relative Error (MMRE): {mmre:.4f}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LSTM Model","metadata":{}},{"cell_type":"code","source":"trainX, trainY = data_china()\n\n\nprint('trainX shape == {}.'.format(trainX.shape))\nprint('trainY shape == {}.'.format(trainY.shape))\n\n# define the Autoencoder model\n\nmodel = Sequential()\nmodel.add(BatchNormalization(input_shape=(trainX.shape[1], trainX.shape[2])))\nmodel.add(Bidirectional(LSTM(64, activation='relu', return_sequences=True)))\nmodel.add(LSTM(32, activation='relu',))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(trainY.shape[1]))\n\nmodel.compile(optimizer='sgd', loss='mse')\nmodel.summary()\n\n\n# fit the model\nhistory = model.fit(trainX, trainY, epochs=50, batch_size=32, validation_split=0.1, verbose=1)\n\nplt.plot(history.history['loss'], label='Training loss')\nplt.plot(history.history['val_loss'], label='Validation loss')\n","metadata":{"execution":{"iopub.status.busy":"2024-07-05T15:46:11.078367Z","iopub.execute_input":"2024-07-05T15:46:11.078739Z","iopub.status.idle":"2024-07-05T15:46:30.877921Z","shell.execute_reply.started":"2024-07-05T15:46:11.078694Z","shell.execute_reply":"2024-07-05T15:46:30.876943Z"},"trusted":true},"execution_count":86,"outputs":[{"name":"stdout","text":"trainX shape == (496, 2, 18).\ntrainY shape == (496, 1).\nModel: \"sequential_12\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n batch_normalization_13 (Bat  (None, 2, 18)            72        \n chNormalization)                                                \n                                                                 \n bidirectional_2 (Bidirectio  (None, 2, 128)           42496     \n nal)                                                            \n                                                                 \n lstm_5 (LSTM)               (None, 32)                20608     \n                                                                 \n dropout_13 (Dropout)        (None, 32)                0         \n                                                                 \n dense_34 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 63,209\nTrainable params: 63,173\nNon-trainable params: 36\n_________________________________________________________________\nEpoch 1/50\n14/14 [==============================] - 5s 58ms/step - loss: 0.0163 - val_loss: 0.0146\nEpoch 2/50\n14/14 [==============================] - 0s 21ms/step - loss: 0.0159 - val_loss: 0.0138\nEpoch 3/50\n14/14 [==============================] - 0s 21ms/step - loss: 0.0155 - val_loss: 0.0134\nEpoch 4/50\n14/14 [==============================] - 0s 21ms/step - loss: 0.0156 - val_loss: 0.0130\nEpoch 5/50\n14/14 [==============================] - 0s 21ms/step - loss: 0.0154 - val_loss: 0.0128\nEpoch 6/50\n14/14 [==============================] - 0s 21ms/step - loss: 0.0152 - val_loss: 0.0126\nEpoch 7/50\n14/14 [==============================] - 0s 21ms/step - loss: 0.0154 - val_loss: 0.0125\nEpoch 8/50\n14/14 [==============================] - 0s 20ms/step - loss: 0.0154 - val_loss: 0.0124\nEpoch 9/50\n14/14 [==============================] - 0s 20ms/step - loss: 0.0152 - val_loss: 0.0123\nEpoch 10/50\n14/14 [==============================] - 0s 20ms/step - loss: 0.0154 - val_loss: 0.0122\nEpoch 11/50\n14/14 [==============================] - 0s 20ms/step - loss: 0.0154 - val_loss: 0.0122\nEpoch 12/50\n14/14 [==============================] - 0s 20ms/step - loss: 0.0155 - val_loss: 0.0121\nEpoch 13/50\n14/14 [==============================] - 0s 21ms/step - loss: 0.0152 - val_loss: 0.0121\nEpoch 14/50\n14/14 [==============================] - 0s 21ms/step - loss: 0.0151 - val_loss: 0.0120\nEpoch 15/50\n14/14 [==============================] - 0s 20ms/step - loss: 0.0151 - val_loss: 0.0120\nEpoch 16/50\n14/14 [==============================] - 0s 23ms/step - loss: 0.0152 - val_loss: 0.0120\nEpoch 17/50\n14/14 [==============================] - 0s 20ms/step - loss: 0.0151 - val_loss: 0.0119\nEpoch 18/50\n14/14 [==============================] - 0s 20ms/step - loss: 0.0152 - val_loss: 0.0119\nEpoch 19/50\n14/14 [==============================] - 0s 21ms/step - loss: 0.0152 - val_loss: 0.0119\nEpoch 20/50\n14/14 [==============================] - 0s 21ms/step - loss: 0.0149 - val_loss: 0.0118\nEpoch 21/50\n14/14 [==============================] - 0s 21ms/step - loss: 0.0153 - val_loss: 0.0118\nEpoch 22/50\n14/14 [==============================] - 0s 20ms/step - loss: 0.0151 - val_loss: 0.0118\nEpoch 23/50\n14/14 [==============================] - 0s 21ms/step - loss: 0.0152 - val_loss: 0.0118\nEpoch 24/50\n14/14 [==============================] - 0s 20ms/step - loss: 0.0151 - val_loss: 0.0118\nEpoch 25/50\n14/14 [==============================] - 0s 21ms/step - loss: 0.0152 - val_loss: 0.0117\nEpoch 26/50\n14/14 [==============================] - 0s 20ms/step - loss: 0.0152 - val_loss: 0.0117\nEpoch 27/50\n14/14 [==============================] - 0s 20ms/step - loss: 0.0150 - val_loss: 0.0117\nEpoch 28/50\n14/14 [==============================] - 0s 20ms/step - loss: 0.0151 - val_loss: 0.0117\nEpoch 29/50\n14/14 [==============================] - 0s 20ms/step - loss: 0.0152 - val_loss: 0.0117\nEpoch 30/50\n14/14 [==============================] - 0s 21ms/step - loss: 0.0152 - val_loss: 0.0117\nEpoch 31/50\n14/14 [==============================] - 0s 21ms/step - loss: 0.0150 - val_loss: 0.0117\nEpoch 32/50\n14/14 [==============================] - 0s 20ms/step - loss: 0.0151 - val_loss: 0.0117\nEpoch 33/50\n14/14 [==============================] - 0s 21ms/step - loss: 0.0149 - val_loss: 0.0117\nEpoch 34/50\n14/14 [==============================] - 0s 21ms/step - loss: 0.0152 - val_loss: 0.0117\nEpoch 35/50\n14/14 [==============================] - 0s 22ms/step - loss: 0.0151 - val_loss: 0.0117\nEpoch 36/50\n14/14 [==============================] - 0s 20ms/step - loss: 0.0152 - val_loss: 0.0117\nEpoch 37/50\n14/14 [==============================] - 0s 21ms/step - loss: 0.0150 - val_loss: 0.0117\nEpoch 38/50\n14/14 [==============================] - 0s 21ms/step - loss: 0.0150 - val_loss: 0.0117\nEpoch 39/50\n14/14 [==============================] - 0s 20ms/step - loss: 0.0152 - val_loss: 0.0117\nEpoch 40/50\n14/14 [==============================] - 0s 20ms/step - loss: 0.0151 - val_loss: 0.0117\nEpoch 41/50\n14/14 [==============================] - 0s 21ms/step - loss: 0.0154 - val_loss: 0.0117\nEpoch 42/50\n14/14 [==============================] - 0s 21ms/step - loss: 0.0151 - val_loss: 0.0117\nEpoch 43/50\n14/14 [==============================] - 0s 21ms/step - loss: 0.0150 - val_loss: 0.0118\nEpoch 44/50\n14/14 [==============================] - 0s 21ms/step - loss: 0.0149 - val_loss: 0.0118\nEpoch 45/50\n14/14 [==============================] - 0s 20ms/step - loss: 0.0148 - val_loss: 0.0118\nEpoch 46/50\n14/14 [==============================] - 0s 20ms/step - loss: 0.0151 - val_loss: 0.0118\nEpoch 47/50\n14/14 [==============================] - 0s 21ms/step - loss: 0.0150 - val_loss: 0.0118\nEpoch 48/50\n14/14 [==============================] - 0s 20ms/step - loss: 0.0150 - val_loss: 0.0118\nEpoch 49/50\n14/14 [==============================] - 0s 21ms/step - loss: 0.0151 - val_loss: 0.0118\nEpoch 50/50\n14/14 [==============================] - 0s 26ms/step - loss: 0.0149 - val_loss: 0.0118\n","output_type":"stream"},{"execution_count":86,"output_type":"execute_result","data":{"text/plain":"[<matplotlib.lines.Line2D at 0x7a5e7c6e0340>]"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"def root_mean_squared_error(y_true, y_pred):\n    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# trainY.shape[1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainX, trainY = data_china()\n# def create_and_train_autoencoder(trainX, trainY, epochs, batch_size, validation_split, \n#                                  learning_rate, dropout_rate, activation, \n#                                  optimizer, loss):\n    \n#     model = Sequential()\n#     model.add(BatchNormalization(input_shape=(trainX.shape[1], trainX.shape[2])))\n#     model.add(Bidirectional(LSTM(64, activation=activation, return_sequences=True)))\n#     model.add(LSTM(32, activation=activation, return_sequences=False))\n#     model.add(Dropout(dropout_rate))\n#     model.add(Dense(trainY.shape[1]))\n\n#     model.compile(optimizer=optimizer, loss=loss)\n#     model.summary()\n\n\n#     # fit the model\n#     history = model.fit(trainX, trainY, epochs=epochs, batch_size=batch_size, validation_split=validation_split, verbose=1)\n\n#     plt.plot(history.history['loss'], label='Training loss')\n#     plt.plot(history.history['val_loss'], label='Validation loss')\n\n#     return model, history\n\ndef create_and_train_autoencoder(trainX, trainY, epochs, batch_size, validation_split, \n                                 learning_rate, dropout_rate, lstm_units, activation, \n                                 optimizer, loss):\n    \n    model = Sequential()\n    model.add(BatchNormalization(input_shape=(trainX.shape[1], trainX.shape[2])))\n    model.add(Bidirectional(LSTM(lstm_units, activation=activation, return_sequences=True)))\n    model.add(LSTM(lstm_units // 2, activation=activation, return_sequences=True))\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(trainY.shape[1]))\n\n\n\n    \n    model.compile(optimizer=optimizer, loss=loss)\n    model.summary()\n\n    # Define early stopping\n    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n\n    # fit the model with early stopping\n    history = model.fit(trainX, trainY, epochs=epochs, batch_size=batch_size, \n                        validation_split=validation_split, verbose=1, callbacks=[early_stopping])\n   \n    plt.plot(history.history['loss'], label='Training loss')\n    plt.plot(history.history['val_loss'], label='Validation loss')\n\n    return model, history\n\n\n# Example usage:\n# Adjust hyperparameters as needed\n# autoencoder_model, training_history = create_and_train_autoencoder(trainX, trainY, \n#                                                                     epochs=500, batch_size=32, \n#                                                                     validation_split=0.1, \n#                                                                     learning_rate=best_hyperparameters[0], \n#                                                                     dropout_rate=best_hyperparameters[1], \n#                                                                     lstm_units=int(best_hyperparameters[2]),\n#                                                                     activation='relu', \n#                                                                     optimizer='adam', \n#                                                                     loss='mse')\n\nautoencoder_model, training_history = create_and_train_autoencoder(trainX, trainY, \n                                                                    epochs=100, batch_size=16, \n                                                                    validation_split=0.1, \n                                                                    learning_rate=0.01, \n                                                                    dropout_rate=0.1,\n                                                                    lstm_units= 64,\n                                                                    activation='sigmoid', \n                                                                    optimizer='adam', \n                                                                    loss= 'mae',\n                                                                    \n                                                                  )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\ndef harmonic_search(objective_function, search_space, num_iterations):\n    # Initialize parameters\n    num_harmonics = 10\n    pitch_adjustment_rate = 0.01\n\n    # Initialize random solutions within the search space\n    solutions = np.random.uniform(low=search_space[:, 0], high=search_space[:, 1], size=(num_harmonics, len(search_space)))\n\n    for iteration in range(num_iterations):\n        # Evaluate the performance of each solution\n        scores = [objective_function(solution) for solution in solutions]\n\n        # Select the top-performing solutions as parents\n        parents = solutions[np.argsort(scores)[:2]]\n\n        # Generate new candidate solutions by combining and modifying parents\n        new_solutions = parents[0] + np.random.uniform(low=-pitch_adjustment_rate, high=pitch_adjustment_rate, size=parents.shape)\n\n        # Clip new solutions to the search space\n        new_solutions = np.clip(new_solutions, search_space[:, 0], search_space[:, 1])\n\n        # Replace the worst solutions with the new ones\n        worst_index = np.argmax(scores)\n        solutions[worst_index] = new_solutions[0]  # Take the first parent as the new solution\n\n    # Return the best solution found\n    best_solution = solutions[np.argmin(scores)]\n    return best_solution\n\n# Example usage:\n# Define the search space for hyperparameters\nsearch_space = np.array([\n    [0.001, 0.1],  # Learning Rate\n    [0.1, 0.9],    # Dropout Rate\n    [16, 64],     # Number of LSTM units\n])\n\n# Define your objective function (replace with your actual training and evaluation logic)\ndef objective_function(hyperparameters):\n    learning_rate, dropout_rate, lstm_units = hyperparameters\n    \n    try:\n        # Create and train LSTM model with the given hyperparameters\n        # Return the performance metric to be minimized (e.g., validation loss)\n        autoencoder_model, training_history = create_and_train_autoencoder(trainX, trainY, \n                                                                            epochs=100, batch_size=16, \n                                                                            validation_split=0.1, \n                                                                            learning_rate=learning_rate, \n                                                                            dropout_rate=dropout_rate, \n                                                                            lstm_units=int(lstm_units),\n                                                                            activation='sigmoid', \n                                                                            optimizer='adam')\n        \n        # Retrieve the performance metric (e.g., validation loss) from the training history\n        metric = min(training_history.history['val_loss'])  # Assuming 'val_loss' is the relevant metric\n\n        return metric\n    except Exception as e:\n        # Return a large value in case of an error\n        return float('inf')\n\n# Run harmonic search\nbest_hyperparameters = harmonic_search(objective_function, search_space, num_iterations=50)\n\n# Print or log the best hyperparameters\nprint(\"Best Hyperparameters:\")\nprint(\"Learning Rate:\", best_hyperparameters[0])\nprint(\"Dropout Rate:\", best_hyperparameters[1])\nprint(\"LSTM Units:\", int(best_hyperparameters[2]))\n\n# Update your LSTM model with the best hyperparameters\nautoencoder_model, training_history = create_and_train_autoencoder(trainX, trainY, \n                                                                    epochs=100, batch_size=16, \n                                                                    validation_split=0.1, \n                                                                    learning_rate=best_hyperparameters[0], \n                                                                    dropout_rate=best_hyperparameters[1], \n                                                                    lstm_units=int(best_hyperparameters[2]),\n                                                                    activation='sigmoid', \n                                                                    optimizer='adam', \n                                                                    loss='mae',\n                                                                    \n                                                                  )\n\n# Optionally, you can also print or log other relevant information, such as the best performance metric\nbest_metric = objective_function(best_hyperparameters)\nprint(\"Best Performance Metric:\", best_metric)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#     from sklearn.preprocessing import MinMaxScaler\n#     df = pd.read_csv(CFG[1])\n# #     x = df.drop(columns=['id','ID'])\n#     # Assuming your dataset is stored in a DataFrame named 'df'\n#     # If not, replace 'df' with the actual name of your DataFrame\n\n#     # Extract the target variable\n#     target_variable = 'Effort'\n#     y = df[target_variable]\n\n#     # Extract features (excluding target variable)\n#     X = df.drop(columns=['id','ID', 'Effort'])\n\n#     # Normalize the dataset\n#     scaler = MinMaxScaler()\n#     X_normalized = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n\n#     # Split the dataset into training and testing sets\n#     X_train, X_test, y_train, y_test = train_test_split(X_normalized, y, test_size=0.2, random_state=42)\n\n#     # Check the shapes of the resulting sets\n#     print(\"X_train shape:\", X_train.shape)\n#     print(\"X_test shape:\", X_test.shape)\n#     print(\"y_train shape:\", y_train.shape)\n#     print(\"y_test shape:\", y_test.shape)\n# df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn import datasets\n# from sklearn.model_selection import train_test_split\n# from sklearn.svm import SVR\n# from sklearn.metrics import  mean_absolute_error\n\n\n\n# # Create an SVM regressor\n# svm_regressor = SVR(kernel='linear', C=1)\n\n# svm_regressor.fit(X_train, y_train)\n\n# # Make predictions on the test set\n# y_pred = svm_regressor.predict(X_test)\n\n# # Evaluate mean squared error\n# mae = mean_absolute_error(y_test, y_pred)\n# print(f\"Mean Squared Error: {mae}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# trainX.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Attention, Activation\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport matplotlib.pyplot as plt\ndf = pd.read_csv(CFG[1])\n# Load your dataset (assuming it's in a DataFrame named 'df')\n# Replace 'Effort' with the actual column name you want to predict\ntarget_column = 'Effort'\n\n# Separate features (X) and target variable (y)\nX = df.drop(target_column, axis=1)\ny = df[target_column]\n\n# Data Preprocessing\nscaler_X = MinMaxScaler(feature_range=(0, 1))\nscaler_y = MinMaxScaler(feature_range=(0, 1))\n\nX_normalized = scaler_X.fit_transform(X.values)\ny_normalized = scaler_y.fit_transform(y.values.reshape(-1, 1))\n\n# Reshape X for LSTM input (assuming you have a sequence length)\nsequence_length = 1  # Adjust based on your dataset\nX_reshaped = X_normalized.reshape((X_normalized.shape[0], sequence_length, X_normalized.shape[1]))\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_reshaped, y_normalized, test_size=0.2, random_state=42)\n\n# Model Construction and Training\ninp = Input(shape=(X_reshaped.shape[1], X_reshaped.shape[2]))\ne = Sequential()(inp)\ng = LSTM(units=64, return_sequences=True)(e)\nh = LSTM(units=32, return_sequences=True)(g)\ns = Attention(use_scale=True)([h, h])\nf = Activation('relu')(s)\nout = Dense(1)(f)  # Assuming a regression task\n\nmodel = Model(inputs=inp, outputs=out)\nmodel.compile(optimizer='adam', loss='mean_squared_error')  # You can use other loss functions for regression\n\n# Implement Early Stopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n\n# Train the model and monitor the training history\nhistory = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.1, callbacks=[early_stopping])\n\n# Plot the training and validation loss\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\nplt.show()\n\n\n# Prediction\ny_pred = model.predict(X_test)\n\n# Rescale predictions to the original scale\ny_pred_rescaled = scaler_y.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n\n# Rescale true labels to the original scale\ny_test_rescaled = scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten()\n\n# Evaluate mean absolute error\nmae = np.mean(np.abs(y_test_rescaled - y_pred_rescaled))\nprint(f\"Mean Absolute Error: {mae}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(CFG[1])\n# df = df.drop(columns=['id','ID'])\ndf.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Attention, Activation\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.metrics import mean_absolute_error\nimport optuna\n\ndf = pd.read_csv(CFG[7])\n# df = df.drop(columns=['id','ID'])\n# Load your dataset (assuming it's in a DataFrame named 'df')\n# Replace 'Effort' with the actual column name you want to predict\ntarget_column = 'Effort'\n\n# Separate features (X) and target variable (y)\nX = df.drop(target_column, axis=1)\ny = df[target_column]\n\n# Data Preprocessing\nscaler_X = MinMaxScaler(feature_range=(0, 1))\nscaler_y = MinMaxScaler(feature_range=(0, 1))\n\nX_normalized = scaler_X.fit_transform(X.values)\ny_normalized = scaler_y.fit_transform(y.values.reshape(-1, 1))\n\n# Reshape X for LSTM input (assuming you have a sequence length)\nsequence_length = 1  # Adjust based on your dataset\nX_reshaped = X_normalized.reshape((X_normalized.shape[0], sequence_length, X_normalized.shape[1]))\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_reshaped, y_normalized, test_size=0.2, random_state=42)\n\n# Model Construction and Training\ndef build_model(units1, units2, learning_rate):\n    inp = Input(shape=(X_reshaped.shape[1], X_reshaped.shape[2]))\n    e = Sequential()(inp)\n    g = LSTM(units=units1, return_sequences=True)(e)\n    h = LSTM(units=units2, return_sequences=True)(g)\n    s = Attention(use_scale=True)([h, h])\n    f = Activation('relu')(s)\n    out = Dense(1)(f)  # Assuming a regression task\n\n    model = Model(inputs=inp, outputs=out)\n    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n    model.compile(optimizer=optimizer, loss='mean_squared_error')  # You can use other loss functions for regression\n    \n    return model\n\n# Implement Early Stopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n\n# Objective function for Optuna\ndef objective(trial):\n    units1 = trial.suggest_int('units1', 32, 128)\n    units2 = trial.suggest_int('units2', 16, 64)\n    learning_rate = trial.suggest_loguniform('learning_rate', 0.001, 0.1)\n\n    model = build_model(units1=units1, units2=units2, learning_rate=learning_rate)\n    \n    history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.1, callbacks=[early_stopping], verbose=0)\n    \n    return mean_absolute_error(scaler_y.inverse_transform(y_test.reshape(-1, 1)), scaler_y.inverse_transform(model.predict(X_test).reshape(-1, 1)))\n\n# Perform hyperparameter tuning with Optuna\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=10)\n\n# Get the best hyperparameters\nbest_params = study.best_params\n\n# Train the final model with the best hyperparameters\nbest_model = build_model(**best_params)\nbest_model.fit(X_train, y_train, epochs=200, batch_size=32, validation_split=0.1, callbacks=[early_stopping])\n\n# Plot the training and validation loss\nplt.plot(best_model.history.history['loss'], label='Train Loss')\nplt.plot(best_model.history.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\nplt.show()\n\n# Prediction\ny_pred = best_model.predict(X_test)\n\n# Rescale predictions to the original scale\ny_pred_rescaled = scaler_y.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n\n# Rescale true labels to the original scale\ny_test_rescaled = scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten()\n\n# Evaluate mean absolute error\nmae = mean_absolute_error(y_test_rescaled, y_pred_rescaled)\nprint(f\"Mean Absolute Error: {mae}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Attention, Activation\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\n\n# Load your dataset (assuming it's in a DataFrame named 'df')\n# Replace 'Effort' with the actual column name you want to predict\ntarget_column = 'Effort'\n\n# Separate features (X) and target variable (y)\nX = df.drop(target_column, axis=1)\ny = df[target_column]\n\n# Data Preprocessing\nscaler_X = MinMaxScaler(feature_range=(0, 1))\nscaler_y = MinMaxScaler(feature_range=(0, 1))\n\nX_normalized = scaler_X.fit_transform(X.values)\ny_normalized = scaler_y.fit_transform(y.values.reshape(-1, 1))\n\n# Reshape X for LSTM input (assuming you have a sequence length)\nsequence_length = 1  # Adjust based on your dataset\nX_reshaped = X_normalized.reshape((X_normalized.shape[0], sequence_length, X_normalized.shape[1]))\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_reshaped, y_normalized, test_size=0.2, random_state=42)\n\n# Model Construction and Training\ninp = Input(shape=(X_reshaped.shape[1], X_reshaped.shape[2]))\ne = Sequential()(inp)\ng = LSTM(units=64, return_sequences=True)(e)\nh = LSTM(units=32, return_sequences=True)(g)\ns = Attention(use_scale=True)([h, h])\nf = Activation('relu')(s)\nout = Dense(1)(f)  # Assuming a regression task\n\nmodel = Model(inputs=inp, outputs=out)\nmodel.compile(optimizer='adam', loss='mean_squared_error')  # You can use other loss functions for regression\n\n# Implement Early Stopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n\n# Train the model and monitor the training history\nhistory = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.1, callbacks=[early_stopping])\n\n# Plot the training and validation loss\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\nplt.show()\n\n# Prediction\ny_pred = model.predict(X_test)\n\n# Rescale predictions to the original scale\ny_pred_rescaled = scaler_y.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n\n# Rescale true labels to the original scale\ny_test_rescaled = scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten()\n\n# Evaluate root mean squared error (RMSE)\nrmse = np.sqrt(mean_squared_error(y_test_rescaled, y_pred_rescaled))\nprint(f\"Root Mean Squared Error (RMSE): {rmse}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Attention, Activation\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.metrics import mean_squared_error\nimport optuna\n\n# Load your dataset (assuming it's in a DataFrame named 'df')\n# Replace 'Effort' with the actual column name you want to predict\ntarget_column = 'Effort'\n\n# Separate features (X) and target variable (y)\nX = df.drop(target_column, axis=1)\ny = df[target_column]\n\n# Data Preprocessing\nscaler_X = MinMaxScaler(feature_range=(0, 1))\nscaler_y = MinMaxScaler(feature_range=(0, 1))\n\nX_normalized = scaler_X.fit_transform(X.values)\ny_normalized = scaler_y.fit_transform(y.values.reshape(-1, 1))\n\n# Reshape X for LSTM input (assuming you have a sequence length)\nsequence_length = 1  # Adjust based on your dataset\nX_reshaped = X_normalized.reshape((X_normalized.shape[0], sequence_length, X_normalized.shape[1]))\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_reshaped, y_normalized, test_size=0.2, random_state=42)\n\n# Model Construction and Training\ndef build_model(units1, units2, learning_rate):\n    inp = Input(shape=(X_reshaped.shape[1], X_reshaped.shape[2]))\n    e = Sequential()(inp)\n    g = LSTM(units=units1, return_sequences=True)(e)\n    h = LSTM(units=units2, return_sequences=True)(g)\n    s = Attention(use_scale=True)([h, h])\n    f = Activation('relu')(s)\n    out = Dense(1)(f)  # Assuming a regression task\n\n    model = Model(inputs=inp, outputs=out)\n    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n    model.compile(optimizer=optimizer, loss='mean_squared_error')  # You can use other loss functions for regression\n    \n    return model\n\n# Implement Early Stopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n\n# Objective function for Optuna\ndef objective(trial):\n    units1 = trial.suggest_int('units1', 32, 128)\n    units2 = trial.suggest_int('units2', 16, 64)\n    learning_rate = trial.suggest_loguniform('learning_rate', 0.001, 0.1)\n\n    model = build_model(units1=units1, units2=units2, learning_rate=learning_rate)\n    \n    history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.1, callbacks=[early_stopping], verbose=0)\n    \n    return mean_absolute_error(scaler_y.inverse_transform(y_test.reshape(-1, 1)), scaler_y.inverse_transform(model.predict(X_test).reshape(-1, 1)))\n\n# Perform hyperparameter tuning with Optuna\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=10)\n\n# Get the best hyperparameters\nbest_params = study.best_params\n\n# Train the final model with the best hyperparameters\nbest_model = build_model(**best_params)\nbest_model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.1, callbacks=[early_stopping])\n\n# Plot the training and validation loss\nplt.plot(best_model.history.history['loss'], label='Train Loss')\nplt.plot(best_model.history.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\nplt.show()\n\n# Prediction\ny_pred = best_model.predict(X_test)\n\n# Rescale predictions to the original scale\ny_pred_rescaled = scaler_y.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n\n# Rescale true labels to the original scale\ny_test_rescaled = scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten()\n\n# Evaluate root mean squared error (RMSE)\nrmse = np.sqrt(mean_squared_error(y_test_rescaled, y_pred_rescaled))\nprint(f\"Root Mean Squared Error (RMSE): {rmse}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Attention, Activation\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport optuna\n\n# Load your dataset (assuming it's in a DataFrame named 'df')\n# Replace 'Effort' with the actual column name you want to predict\ntarget_column = 'Effort'\n\n# Separate features (X) and target variable (y)\nX = df.drop(target_column, axis=1)\ny = df[target_column]\n\n# Data Preprocessing\nscaler_X = MinMaxScaler(feature_range=(0, 1))\nscaler_y = MinMaxScaler(feature_range=(0, 1))\n\nX_normalized = scaler_X.fit_transform(X.values)\ny_normalized = scaler_y.fit_transform(y.values.reshape(-1, 1))\n\n# Reshape X for LSTM input (assuming you have a sequence length)\nsequence_length = 1  # Adjust based on your dataset\nX_reshaped = X_normalized.reshape((X_normalized.shape[0], sequence_length, X_normalized.shape[1]))\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_reshaped, y_normalized, test_size=0.2, random_state=42)\n\n# Model Construction and Training\ndef build_model(units1, units2, learning_rate):\n    inp = Input(shape=(X_reshaped.shape[1], X_reshaped.shape[2]))\n    e = Sequential()(inp)\n    g = LSTM(units=units1, return_sequences=True)(e)\n    h = LSTM(units=units2, return_sequences=True)(g)\n    s = Attention(use_scale=True)([h, h])\n    f = Activation('relu')(s)\n    out = Dense(1)(f)  # Assuming a regression task\n\n    model = Model(inputs=inp, outputs=out)\n    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n    model.compile(optimizer=optimizer, loss='mean_squared_error')  # You can use other loss functions for regression\n    \n    return model\n\n# Implement Early Stopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n\n# Objective function for Optuna\ndef objective(trial):\n    units1 = trial.suggest_int('units1', 32, 128)\n    units2 = trial.suggest_int('units2', 16, 64)\n    learning_rate = trial.suggest_loguniform('learning_rate', 0.001, 0.1)\n\n    model = build_model(units1=units1, units2=units2, learning_rate=learning_rate)\n    \n    history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.1, callbacks=[early_stopping], verbose=0)\n    \n    return mean_absolute_error(scaler_y.inverse_transform(y_test.reshape(-1, 1)), scaler_y.inverse_transform(model.predict(X_test).reshape(-1, 1)))\n\n# Perform hyperparameter tuning with Optuna\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=10)\n\n# Get the best hyperparameters\nbest_params = study.best_params\n\n# Train the final model with the best hyperparameters\nbest_model = build_model(**best_params)\nbest_model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.1, callbacks=[early_stopping])\n\n# Plot the training and validation loss\nplt.plot(best_model.history.history['loss'], label='Train Loss')\nplt.plot(best_model.history.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\nplt.show()\n\n# Prediction\ny_pred = best_model.predict(X_test)\n\n# Rescale predictions to the original scale\ny_pred_rescaled = scaler_y.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n\n# Rescale true labels to the original scale\ny_test_rescaled = scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten()\n\n# Evaluate R-squared\nr_squared = r2_score(y_test_rescaled, y_pred_rescaled)\nprint(f\"R-squared: {r_squared}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
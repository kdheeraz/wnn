{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6801955,"sourceType":"datasetVersion","datasetId":3909889},{"sourceId":8882856,"sourceType":"datasetVersion","datasetId":5345376}],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline,make_pipeline\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Bidirectional\nfrom matplotlib import pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras import backend as K","metadata":{"execution":{"iopub.status.busy":"2024-07-06T16:27:45.052692Z","iopub.execute_input":"2024-07-06T16:27:45.053471Z","iopub.status.idle":"2024-07-06T16:27:49.153701Z","shell.execute_reply.started":"2024-07-06T16:27:45.053434Z","shell.execute_reply":"2024-07-06T16:27:49.152882Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom tensorflow.keras.layers import Flatten\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2024-07-06T16:28:11.218323Z","iopub.execute_input":"2024-07-06T16:28:11.219006Z","iopub.status.idle":"2024-07-06T16:28:11.223900Z","shell.execute_reply.started":"2024-07-06T16:28:11.218972Z","shell.execute_reply":"2024-07-06T16:28:11.222817Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"CFG = [\n    '/kaggle/input/software-effort-estimation-datasets/albrecht.csv',\n    '/kaggle/input/software-effort-estimation-datasets/china.csv',\n    '/kaggle/input/software-effort-estimation-datasets/desharnais.csv',\n    '/kaggle/input/software-effort-estimation-datasets/finnish.csv',\n    '/kaggle/input/software-effort-estimation-datasets/isbsg10.csv',\n    '/kaggle/input/software-effort-estimation-datasets/kemerer.csv',\n    '/kaggle/input/software-effort-estimation-datasets/kitchenham.csv',\n    '/kaggle/input/software-effort-estimation-datasets/maxwell.csv',\n    '/kaggle/input/software-effort-estimation-datasets/miyazaki94.csv'\n]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cocomo_path= '/kaggle/input/cocomo/5.cocomo81.csv'","metadata":{"execution":{"iopub.status.busy":"2024-07-06T16:28:15.269948Z","iopub.execute_input":"2024-07-06T16:28:15.270975Z","iopub.status.idle":"2024-07-06T16:28:15.274833Z","shell.execute_reply.started":"2024-07-06T16:28:15.270935Z","shell.execute_reply":"2024-07-06T16:28:15.273924Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def load_cocomo_data():\n    data = pd.read_csv(cocomo_path)\n    X = data.iloc[:, :-1].values  # Features\n    y = data.iloc[:, -1].values  # Effort\n    return X, y\n\nX, y = load_cocomo_data()","metadata":{"execution":{"iopub.status.busy":"2024-07-06T16:28:17.919175Z","iopub.execute_input":"2024-07-06T16:28:17.919854Z","iopub.status.idle":"2024-07-06T16:28:17.938030Z","shell.execute_reply.started":"2024-07-06T16:28:17.919818Z","shell.execute_reply":"2024-07-06T16:28:17.937327Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"code","source":"def data_albrecht():\n    df = pd.read_csv(CFG[0])\n    df_for_training = df.astype(float)\n    #LSTM uses sigmoid and tanh that are sensitive to magnitude so values need to be normalized\n    # normalize the dataset\n    scaler = MinMaxScaler()\n    scaler = scaler.fit(df_for_training)\n    df_for_training_scaled = scaler.transform(df_for_training)\n    return df\n\n\ndef data_china():\n    \n    df = pd.read_csv(CFG[1])\n    df = df.drop(columns=['id','ID'])\n    df_for_training = df.astype(float)\n#     print(df.columns)\n    #LSTM uses sigmoid and tanh that are sensitive to magnitude so values need to be normalized\n    # normalize the dataset\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    scaler = scaler.fit(df_for_training)\n    df_for_training_scaled = scaler.transform(df_for_training)\n    \n    trainX = []\n    trainY = []\n    n_future = 2   # Number of days we want to look into the future based on the past days.\n    n_past = 2 # Number of past days we want to use to predict the future.\n\n    for i in range(n_past, len(df_for_training_scaled) - n_future +1):\n        trainX.append(df_for_training_scaled[i - n_past:i, 0:df_for_training.shape[1]])\n        trainY.append(df_for_training_scaled[i + n_future - 1:i + n_future, df_for_training.shape[1] - 1])\n\n    trainX, trainY = np.array(trainX), np.array(trainY)\n\n    return trainX, trainY\n\n\ndef data_desharnais():\n    df = pd.read_csv(CFG[2])\n    df = df.drop(columns=['Project','YearEnd', 'Language'])\n    df_for_training = df.astype(float)\n    #LSTM uses sigmoid and tanh that are sensitive to magnitude so values need to be normalized\n    # normalize the dataset\n    scaler = StandardScaler()\n    scaler = scaler.fit(df_for_training)\n    df_for_training_scaled = scaler.transform(df_for_training)\n    return df\n\n\ndef data_finnish():\n    df = pd.read_csv(CFG[3])\n    df_for_training = df.astype(float)\n    #LSTM uses sigmoid and tanh that are sensitive to magnitude so values need to be normalized\n    # normalize the dataset\n    scaler = MinMaxScaler()\n    scaler = scaler.fit(df_for_training)\n    df_for_training_scaled = scaler.transform(df_for_training)\n    return df\n\n\ndef data_isbsg10():\n\n    df = pd.read_csv(CFG[4])\n    df = df.drop(columns = ['id', 'ID', 'Data_Quality', 'Year','AG', 'N_effort_level1',\n           'N_PDR1', 'SDR', 'PET', 'PIT', 'I_Date', 'PAS',\n           'Recording_Method', 'Resource_Level', 'MTS', 'ATS', 'R_PWE_NPA',\n           'P_UWE', 'CASE_Tool', 'UM', 'HMA', 'Hardware1',\n           'IDE', 'DT1', 'DBS1', 'CS1', 'WS1', 'MS1', 'OP1', 'RTA',\n           'SP_CMMI', 'SP_ISO', 'SP_TICKIT', 'MIN_Defects', 'MAJ_Defects',\n           'X_Defects', 'TOT_Defects', 'UB_BU', 'UB_L', 'UB_DU', 'UB_CU',\n           'IMarket', 'T_Platform', 'D_Embedded', 'SE', 'SEA', 'SEM', 'E_Estimate',\n           'E_Estimate_Method', 'DDE', 'DDEM', 'C_Estimate', 'CEC', 'CEM',\n           'E_Tool', 'E_Comments', 'EC_Date', 'SR?', 'SR', 'R_FPC', 'R_FPA',\n           'P_Defects', 'D_Defects', 'MIN_B_Defects', 'MAJ_B_Defects',\n           'X_B_Defects', 'TOT_B_Defects', 'MIN_T_Defects', 'MAJ_T_Defects',\n           'X_T_Defects', 'TOT_T_Defects','S_Defects', 'MIN_I_Defects', 'MAJ_I_Defects',\n           'X_I_Defects', 'TOT_I_Defects'], axis = 1)\n    # Replace \"?\" with numpy.nan\n    df.replace('?', np.nan, inplace=True)\n#     print(df.columns)\n    \n    # imputation transformer\n    trf1 = ColumnTransformer([\n        ('impute',SimpleImputer(strategy='constant', fill_value='Missing'),[14,15,16,17,18])\n    ],remainder='passthrough')\n    \n    trf2 = ColumnTransformer([\n    ('ohe',OneHotEncoder(sparse_output=False,drop= 'first'),[0,1,2,3,4,5,6,7,8,9,10,11,12,13,15])\n    ],remainder='passthrough')\n    \n    pipe = Pipeline([\n    ('trf1',trf1),\n    ('trf2',trf2),\n    ])\n    \n    pipe.fit_transform(df)\n    \n    return pipe.fit_transform(df)\n\n\ndef data_kemerer():\n    df = pd.read_csv(CFG[5])\n    df_for_training = df.astype(float)\n    #LSTM uses sigmoid and tanh that are sensitive to magnitude so values need to be normalized\n    # normalize the dataset\n    scaler = StandardScaler()\n    scaler = scaler.fit(df_for_training)\n    df_for_training_scaled = scaler.transform(df_for_training)\n    return df\n    \n\n    \ndef data_kitchenham():\n    df = pd.read_csv(CFG[6])\n    df_for_training = df.astype(float)\n    #LSTM uses sigmoid and tanh that are sensitive to magnitude so values need to be normalized\n    # normalize the dataset\n    scaler = MinMaxScaler()\n    scaler = scaler.fit(df_for_training)\n    df_for_training_scaled = scaler.transform(df_for_training)\n    return df\n\n\ndef data_maxwell():\n    df = pd.read_csv(CFG[7])\n    df_for_training = df.astype(float)\n    #LSTM uses sigmoid and tanh that are sensitive to magnitude so values need to be normalized\n    # normalize the dataset\n    scaler = StandardScaler()\n    scaler = scaler.fit(df_for_training)\n    df_for_training_scaled = scaler.transform(df_for_training)\n    return df\n\n\ndef data_miyazaki():\n    df = pd.read_csv(CFG[8])\n    df_for_training = df.astype(float)\n    #LSTM uses sigmoid and tanh that are sensitive to magnitude so values need to be normalized\n    # normalize the dataset\n    scaler = StandardScaler()\n    scaler = scaler.fit(df_for_training)\n    df_for_training_scaled = scaler.transform(df_for_training)\n    return df\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pywt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def morlet_wavelet(t, f0=1.0, sigma=1.0):\n    \"\"\"\n    Morlet wavelet function.\n    t: time variable\n    f0: center frequency\n    sigma: bandwidth parameter\n    \"\"\"\n    return np.exp(-0.5 * (t / sigma) ** 2) * np.cos(2 * np.pi * f0 * t)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T16:28:27.816477Z","iopub.execute_input":"2024-07-06T16:28:27.816868Z","iopub.status.idle":"2024-07-06T16:28:27.822575Z","shell.execute_reply.started":"2024-07-06T16:28:27.816836Z","shell.execute_reply":"2024-07-06T16:28:27.821609Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Layer","metadata":{"execution":{"iopub.status.busy":"2024-07-06T16:28:38.284094Z","iopub.execute_input":"2024-07-06T16:28:38.284505Z","iopub.status.idle":"2024-07-06T16:28:38.289091Z","shell.execute_reply.started":"2024-07-06T16:28:38.284469Z","shell.execute_reply":"2024-07-06T16:28:38.288072Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class MorletWaveletLayer(Layer):\n    def __init__(self, units, w=5.0, **kwargs):\n        super(MorletWaveletLayer, self).__init__(**kwargs)\n        self.units = units\n        self.w = w\n\n    def build(self, input_shape):\n        self.kernel = self.add_weight(name='kernel',\n                                      shape=(input_shape[-1], self.units),\n                                      initializer='glorot_uniform',\n                                      trainable=True)\n\n    def call(self, inputs):\n        # Reshape inputs to 2D for the dense operation\n        input_reshaped = tf.reshape(inputs, [-1, inputs.shape[-1]])\n        z = tf.matmul(input_reshaped, self.kernel)\n\n        # Implement Morlet wavelet using TensorFlow operations\n        wavelet_output = tf.cos(self.w * z) * tf.exp(-z**2 / 2)\n        \n        # Reshape back to 3D\n        final= tf.reshape(wavelet_output, tf.concat([tf.shape(inputs)[:-1], [self.units]], axis=0))\n        print(final.shape)\n        return final","metadata":{"execution":{"iopub.status.busy":"2024-07-06T16:28:40.587224Z","iopub.execute_input":"2024-07-06T16:28:40.588140Z","iopub.status.idle":"2024-07-06T16:28:40.596537Z","shell.execute_reply.started":"2024-07-06T16:28:40.588097Z","shell.execute_reply":"2024-07-06T16:28:40.595374Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class MorletWaveletLayer(Layer):\n    def __init__(self, filters, f0=1.0, sigma=1.0, **kwargs):\n        super(MorletWaveletLayer, self).__init__(**kwargs)\n        self.filters = filters\n        self.f0 = f0\n        self.sigma = sigma\n\n    def build(self, input_shape):\n        self.kernel = self.add_weight(\n            shape=(self.filters, input_shape[-1]),\n            initializer='glorot_uniform',\n            trainable=True,\n        )\n        self.built = True\n\n    def call(self, inputs):\n        time = np.linspace(-1, 1, inputs.shape[1])\n        wavelet = morlet_wavelet(time, self.f0, self.sigma)\n        wavelet = tf.convert_to_tensor(wavelet, dtype=tf.float32)\n        wavelet = tf.reshape(wavelet, (1, -1, 1))\n        \n        wavelet_kernels = wavelet * tf.expand_dims(self.kernel, axis=1)\n        \n        # Ensure the wavelet kernels are properly shaped for convolution\n        wavelet_kernels = tf.transpose(wavelet_kernels, [1, 2, 0])\n        \n        conv = tf.nn.conv1d(inputs, wavelet_kernels, stride=1, padding='SAME')\n        return conv\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"totalX, totalY = load_cocomo_data()","metadata":{"execution":{"iopub.status.busy":"2024-07-06T16:28:45.475346Z","iopub.execute_input":"2024-07-06T16:28:45.475752Z","iopub.status.idle":"2024-07-06T16:28:45.485730Z","shell.execute_reply.started":"2024-07-06T16:28:45.475716Z","shell.execute_reply":"2024-07-06T16:28:45.484709Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n############# NOT USING FOR NOW#######\nclass FireflyAlgorithm:\n    def __init__(self, n_fireflies, n_iterations, alpha=0.5, beta=1.0, gamma=1.0):\n        self.n_fireflies = n_fireflies\n        self.n_iterations = n_iterations\n        self.alpha = alpha\n        self.beta = beta\n        self.gamma = gamma\n\n    def initialize_fireflies(self, bounds):\n        fireflies = np.random.rand(self.n_fireflies, len(bounds))\n        for i in range(len(bounds)):\n            fireflies[:, i] = bounds[i][0] + fireflies[:, i] * (bounds[i][1] - bounds[i][0])\n        return fireflies\n\n    def calculate_light_intensity(self, firefly, objective_function):\n        return objective_function(firefly)\n\n    def move_firefly(self, firefly_i, firefly_j, beta):\n        r = np.linalg.norm(firefly_i - firefly_j)\n        return firefly_i + beta * np.exp(-self.gamma * r ** 2) * (firefly_j - firefly_i) + self.alpha * (np.random.rand(len(firefly_i)) - 0.5)\n\n    def optimize(self, objective_function, bounds):\n        fireflies = self.initialize_fireflies(bounds)\n        light_intensities = np.array([self.calculate_light_intensity(firefly, objective_function) for firefly in fireflies])\n\n        for iteration in range(self.n_iterations):\n            for i in range(self.n_fireflies):\n                for j in range(self.n_fireflies):\n                    if light_intensities[i] > light_intensities[j]:\n                        fireflies[i] = self.move_firefly(fireflies[i], fireflies[j], self.beta)\n                        light_intensities[i] = self.calculate_light_intensity(fireflies[i], objective_function)\n        \n        best_firefly = fireflies[np.argmin(light_intensities)]\n        best_intensity = np.min(light_intensities)\n\n        return best_firefly, best_intensity\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainX, testX, trainY, testY = train_test_split(totalX, totalY, test_size=0.25, random_state=42)\n\n# Define the objective function\ndef objective_function(params):\n    f0, sigma = params[0], params[1]\n    \n    # Create the Wavelet Neural Network\n    input_shape = trainX.shape[1:]  # Assuming trainX.shape is (496, 2, 18), input_shape should be (2, 18)\n    wnn = Sequential([\n        BatchNormalization(input_shape=input_shape),\n        Dense(64, activation='relu'),\n        Dropout(0.1),\n        Dense(32, activation='relu'),\n        MorletWaveletLayer(filters=16, f0=1.0, sigma=1.0, input_shape=(None, 1)),  # Replace with appropriate units for your use case\n        Flatten(),\n        Dense(1)  # Adjust output dim to match trainY.shape[1]\n    ])\n\n    # Compile the model\n    wnn.compile(optimizer='adam', loss='mse')\n    #wnn.summary()\n    wnn.fit(trainX, trainY, epochs=10, batch_size=32, verbose=0)\n    \n    loss = wnn.evaluate(testX, testY, verbose=0)\n    return loss\n","metadata":{"execution":{"iopub.status.busy":"2024-07-06T16:28:55.148569Z","iopub.execute_input":"2024-07-06T16:28:55.148948Z","iopub.status.idle":"2024-07-06T16:28:55.158313Z","shell.execute_reply.started":"2024-07-06T16:28:55.148917Z","shell.execute_reply":"2024-07-06T16:28:55.157072Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"    # Create the Wavelet Neural Network\n    input_shape = trainX.shape[1]  # Assuming trainX.shape is (496, 2, 18), input_shape should be (2, 18)\n    wnn = Sequential([\n        BatchNormalization(input_shape=(input_shape,)),\n        Dense(64, activation='relu'),\n        Dropout(0.1),\n        Dense(32, activation='relu'),\n        MorletWaveletLayer(10),  # Replace with appropriate units for your use case\n        Dense(1)  # Adjust output dim to match trainY.shape[1]\n    ])","metadata":{"execution":{"iopub.status.busy":"2024-07-06T16:28:59.747316Z","iopub.execute_input":"2024-07-06T16:28:59.747733Z","iopub.status.idle":"2024-07-06T16:29:00.766357Z","shell.execute_reply.started":"2024-07-06T16:28:59.747701Z","shell.execute_reply":"2024-07-06T16:29:00.765421Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"(None, 10)\n","output_type":"stream"}]},{"cell_type":"code","source":"wnn.compile(optimizer='adam', loss='mse')","metadata":{"execution":{"iopub.status.busy":"2024-07-06T16:29:06.690055Z","iopub.execute_input":"2024-07-06T16:29:06.690467Z","iopub.status.idle":"2024-07-06T16:29:06.711901Z","shell.execute_reply.started":"2024-07-06T16:29:06.690433Z","shell.execute_reply":"2024-07-06T16:29:06.710718Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"wnn.summary()","metadata":{"execution":{"iopub.status.busy":"2024-07-06T16:29:10.625583Z","iopub.execute_input":"2024-07-06T16:29:10.625966Z","iopub.status.idle":"2024-07-06T16:29:10.651750Z","shell.execute_reply.started":"2024-07-06T16:29:10.625934Z","shell.execute_reply":"2024-07-06T16:29:10.650870Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Model: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n batch_normalization (BatchN  (None, 16)               64        \n ormalization)                                                   \n                                                                 \n dense (Dense)               (None, 64)                1088      \n                                                                 \n dropout (Dropout)           (None, 64)                0         \n                                                                 \n dense_1 (Dense)             (None, 32)                2080      \n                                                                 \n morlet_wavelet_layer (Morle  (None, 10)               320       \n tWaveletLayer)                                                  \n                                                                 \n dense_2 (Dense)             (None, 1)                 11        \n                                                                 \n=================================================================\nTotal params: 3,563\nTrainable params: 3,531\nNon-trainable params: 32\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"history = wnn.fit(trainX, trainY, epochs=100, batch_size=32, verbose=0)\n# Evaluate model\npredictions = wnn.predict(testX)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T16:29:15.258095Z","iopub.execute_input":"2024-07-06T16:29:15.258801Z","iopub.status.idle":"2024-07-06T16:29:18.647609Z","shell.execute_reply.started":"2024-07-06T16:29:15.258765Z","shell.execute_reply":"2024-07-06T16:29:18.646657Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"(None, 10)\n(None, 10)\n(None, 10)\n1/1 [==============================] - 0s 120ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"mse = mean_squared_error(testY, predictions)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T16:29:23.755312Z","iopub.execute_input":"2024-07-06T16:29:23.756221Z","iopub.status.idle":"2024-07-06T16:29:23.761056Z","shell.execute_reply.started":"2024-07-06T16:29:23.756185Z","shell.execute_reply":"2024-07-06T16:29:23.760102Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"mse","metadata":{"execution":{"iopub.status.busy":"2024-07-06T16:29:26.174937Z","iopub.execute_input":"2024-07-06T16:29:26.175697Z","iopub.status.idle":"2024-07-06T16:29:26.182543Z","shell.execute_reply.started":"2024-07-06T16:29:26.175661Z","shell.execute_reply":"2024-07-06T16:29:26.181436Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"338008.20830537344"},"metadata":{}}]},{"cell_type":"code","source":"def mean_magnitude_relative_error(y_true, y_pred):\n    # Calculate relative error\n    relative_error = np.abs((y_true - y_pred) / y_true)\n    # Calculate mean magnitude of relative error (MMRE)\n    mmre = np.mean(relative_error)\n    \n    return mmre","metadata":{"execution":{"iopub.status.busy":"2024-07-06T16:29:30.025856Z","iopub.execute_input":"2024-07-06T16:29:30.026487Z","iopub.status.idle":"2024-07-06T16:29:30.031346Z","shell.execute_reply.started":"2024-07-06T16:29:30.026449Z","shell.execute_reply":"2024-07-06T16:29:30.030427Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"mean_magnitude_relative_error(testY,predictions)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T16:29:32.704947Z","iopub.execute_input":"2024-07-06T16:29:32.706005Z","iopub.status.idle":"2024-07-06T16:29:32.712575Z","shell.execute_reply.started":"2024-07-06T16:29:32.705965Z","shell.execute_reply":"2024-07-06T16:29:32.711548Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"0.9909676698965766"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
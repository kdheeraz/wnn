{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6801955,"sourceType":"datasetVersion","datasetId":3909889}],"dockerImageVersionId":30558,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline,make_pipeline\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Bidirectional\nfrom matplotlib import pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras import backend as K","metadata":{"execution":{"iopub.status.busy":"2024-07-16T06:26:36.613686Z","iopub.execute_input":"2024-07-16T06:26:36.614088Z","iopub.status.idle":"2024-07-16T06:26:47.870940Z","shell.execute_reply.started":"2024-07-16T06:26:36.614055Z","shell.execute_reply":"2024-07-16T06:26:47.869010Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"CFG = [\n    '/kaggle/input/software-effort-estimation-datasets/albrecht.csv',\n    '/kaggle/input/software-effort-estimation-datasets/china.csv',\n    '/kaggle/input/software-effort-estimation-datasets/desharnais.csv',\n    '/kaggle/input/software-effort-estimation-datasets/finnish.csv',\n    '/kaggle/input/software-effort-estimation-datasets/isbsg10.csv',\n    '/kaggle/input/software-effort-estimation-datasets/kemerer.csv',\n    '/kaggle/input/software-effort-estimation-datasets/kitchenham.csv',\n    '/kaggle/input/software-effort-estimation-datasets/maxwell.csv',\n    '/kaggle/input/software-effort-estimation-datasets/miyazaki94.csv'\n]","metadata":{"execution":{"iopub.status.busy":"2024-07-16T06:28:14.764646Z","iopub.execute_input":"2024-07-16T06:28:14.765457Z","iopub.status.idle":"2024-07-16T06:28:14.772419Z","shell.execute_reply.started":"2024-07-16T06:28:14.765420Z","shell.execute_reply":"2024-07-16T06:28:14.770596Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"code","source":"def data_albrecht():\n    df = pd.read_csv(CFG[0])\n    df_for_training = df.astype(float)\n    #LSTM uses sigmoid and tanh that are sensitive to magnitude so values need to be normalized\n    # normalize the dataset\n    scaler = MinMaxScaler()\n    scaler = scaler.fit(df_for_training)\n    df_for_training_scaled = scaler.transform(df_for_training)\n    return df\n\n\ndef data_china():\n    \n    df = pd.read_csv(CFG[1])\n    df = df.drop(columns=['id','ID'])\n    df_for_training = df.astype(float)\n#     print(df.columns)\n    #LSTM uses sigmoid and tanh that are sensitive to magnitude so values need to be normalized\n    # normalize the dataset\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    scaler = scaler.fit(df_for_training)\n    df_for_training_scaled = scaler.transform(df_for_training)\n    \n    trainX = []\n    trainY = []\n    n_future = 2   # Number of days we want to look into the future based on the past days.\n    n_past = 2 # Number of past days we want to use to predict the future.\n\n    for i in range(n_past, len(df_for_training_scaled) - n_future +1):\n        trainX.append(df_for_training_scaled[i - n_past:i, 0:df_for_training.shape[1]])\n        trainY.append(df_for_training_scaled[i + n_future - 1:i + n_future, df_for_training.shape[1] - 1])\n\n    trainX, trainY = np.array(trainX), np.array(trainY)\n\n    return trainX, trainY\n\n\ndef data_desharnais():\n    df = pd.read_csv(CFG[2])\n    df = df.drop(columns=['Project','YearEnd', 'Language'])\n    df_for_training = df.astype(float)\n    #LSTM uses sigmoid and tanh that are sensitive to magnitude so values need to be normalized\n    # normalize the dataset\n    scaler = StandardScaler()\n    scaler = scaler.fit(df_for_training)\n    df_for_training_scaled = scaler.transform(df_for_training)\n    return df\n\n\ndef data_finnish():\n    df = pd.read_csv(CFG[3])\n    df_for_training = df.astype(float)\n    #LSTM uses sigmoid and tanh that are sensitive to magnitude so values need to be normalized\n    # normalize the dataset\n    scaler = MinMaxScaler()\n    scaler = scaler.fit(df_for_training)\n    df_for_training_scaled = scaler.transform(df_for_training)\n    return df\n\n\ndef data_isbsg10():\n\n    df = pd.read_csv(CFG[4])\n    df = df.drop(columns = ['id', 'ID', 'Data_Quality', 'Year','AG', 'N_effort_level1',\n           'N_PDR1', 'SDR', 'PET', 'PIT', 'I_Date', 'PAS',\n           'Recording_Method', 'Resource_Level', 'MTS', 'ATS', 'R_PWE_NPA',\n           'P_UWE', 'CASE_Tool', 'UM', 'HMA', 'Hardware1',\n           'IDE', 'DT1', 'DBS1', 'CS1', 'WS1', 'MS1', 'OP1', 'RTA',\n           'SP_CMMI', 'SP_ISO', 'SP_TICKIT', 'MIN_Defects', 'MAJ_Defects',\n           'X_Defects', 'TOT_Defects', 'UB_BU', 'UB_L', 'UB_DU', 'UB_CU',\n           'IMarket', 'T_Platform', 'D_Embedded', 'SE', 'SEA', 'SEM', 'E_Estimate',\n           'E_Estimate_Method', 'DDE', 'DDEM', 'C_Estimate', 'CEC', 'CEM',\n           'E_Tool', 'E_Comments', 'EC_Date', 'SR?', 'SR', 'R_FPC', 'R_FPA',\n           'P_Defects', 'D_Defects', 'MIN_B_Defects', 'MAJ_B_Defects',\n           'X_B_Defects', 'TOT_B_Defects', 'MIN_T_Defects', 'MAJ_T_Defects',\n           'X_T_Defects', 'TOT_T_Defects','S_Defects', 'MIN_I_Defects', 'MAJ_I_Defects',\n           'X_I_Defects', 'TOT_I_Defects'], axis = 1)\n    # Replace \"?\" with numpy.nan\n    df.replace('?', np.nan, inplace=True)\n#     print(df.columns)\n    \n    # imputation transformer\n    trf1 = ColumnTransformer([\n        ('impute',SimpleImputer(strategy='constant', fill_value='Missing'),[14,15,16,17,18])\n    ],remainder='passthrough')\n    \n    trf2 = ColumnTransformer([\n    ('ohe',OneHotEncoder(sparse_output=False,drop= 'first'),[0,1,2,3,4,5,6,7,8,9,10,11,12,13,15])\n    ],remainder='passthrough')\n    \n    pipe = Pipeline([\n    ('trf1',trf1),\n    ('trf2',trf2),\n    ])\n    \n    pipe.fit_transform(df)\n    \n    return pipe.fit_transform(df)\n\n\ndef data_kemerer():\n    df = pd.read_csv(CFG[5])\n    df_for_training = df.astype(float)\n    #LSTM uses sigmoid and tanh that are sensitive to magnitude so values need to be normalized\n    # normalize the dataset\n    scaler = StandardScaler()\n    scaler = scaler.fit(df_for_training)\n    df_for_training_scaled = scaler.transform(df_for_training)\n    return df\n    \n\n    \ndef data_kitchenham():\n    df = pd.read_csv(CFG[6])\n    df_for_training = df.astype(float)\n    #LSTM uses sigmoid and tanh that are sensitive to magnitude so values need to be normalized\n    # normalize the dataset\n    scaler = MinMaxScaler()\n    scaler = scaler.fit(df_for_training)\n    df_for_training_scaled = scaler.transform(df_for_training)\n    return df\n\n\ndef data_maxwell():\n    df = pd.read_csv(CFG[7])\n    df_for_training = df.astype(float)\n    #LSTM uses sigmoid and tanh that are sensitive to magnitude so values need to be normalized\n    # normalize the dataset\n    scaler = StandardScaler()\n    scaler = scaler.fit(df_for_training)\n    df_for_training_scaled = scaler.transform(df_for_training)\n    return df\n\n\ndef data_miyazaki():\n    df = pd.read_csv(CFG[8])\n    df_for_training = df.astype(float)\n    #LSTM uses sigmoid and tanh that are sensitive to magnitude so values need to be normalized\n    # normalize the dataset\n    scaler = StandardScaler()\n    scaler = scaler.fit(df_for_training)\n    df_for_training_scaled = scaler.transform(df_for_training)\n    return df\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T06:28:18.628145Z","iopub.execute_input":"2024-07-16T06:28:18.628655Z","iopub.status.idle":"2024-07-16T06:28:18.661588Z","shell.execute_reply.started":"2024-07-16T06:28:18.628613Z","shell.execute_reply":"2024-07-16T06:28:18.660198Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"x_train, y_train = data_china()\nx_train[0]","metadata":{"execution":{"iopub.status.busy":"2024-07-16T06:30:07.697011Z","iopub.execute_input":"2024-07-16T06:30:07.698047Z","iopub.status.idle":"2024-07-16T06:30:07.722941Z","shell.execute_reply.started":"2024-07-16T06:30:07.698009Z","shell.execute_reply":"2024-07-16T06:30:07.721683Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"array([[9.01250785e-02, 8.23054020e-02, 1.05906314e-01, 3.57142857e-01,\n        4.33164129e-02, 0.00000000e+00, 1.10603829e-01, 0.00000000e+00,\n        0.00000000e+00, 5.26946108e-02, 4.88058152e-02, 4.27435388e-02,\n        4.26320667e-02, 1.00000000e+00, 0.00000000e+00, 3.61445783e-02,\n        1.36639250e-01, 1.36718321e-01],\n       [1.43354846e-02, 9.57039558e-04, 1.62932790e-03, 3.15126050e-03,\n        6.53130288e-02, 2.60814249e-02, 3.75552283e-03, 2.65742345e-02,\n        2.29582236e-02, 1.88023952e-01, 1.69262721e-01, 1.55069583e-01,\n        1.50139018e-01, 3.33333333e-01, 0.00000000e+00, 1.92771084e-01,\n        7.54547619e-02, 7.55394366e-02]])"},"metadata":{}}]},{"cell_type":"code","source":"import numpy as np\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.layers import Layer\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.optimizers import Adam","metadata":{"execution":{"iopub.status.busy":"2024-07-16T06:31:02.117724Z","iopub.execute_input":"2024-07-16T06:31:02.118168Z","iopub.status.idle":"2024-07-16T06:31:02.124329Z","shell.execute_reply.started":"2024-07-16T06:31:02.118102Z","shell.execute_reply":"2024-07-16T06:31:02.123053Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Define the Morlet wavelet function as a custom layer\nclass MorletWavelet(Layer):\n    def __init__(self, **kwargs):\n        super(MorletWavelet, self).__init__(**kwargs)\n\n    def call(self, inputs):\n        sigma = 1.0\n        return K.exp(-0.5 * K.square(inputs / sigma)) * K.cos(5.0 * inputs)\n\n# Build the neural network model\ndef create_morlet_nn(input_dim):\n    model = Sequential()\n    model.add(BatchNormalization(input_shape=input_dim))\n    model.add(Dense(50, input_dim=input_dim))\n    model.add(MorletWavelet())\n\n    model.add(Dense(100))\n    model.add(MorletWavelet())\n\n    model.add(Dense(50))\n    model.add(MorletWavelet())\n\n    model.add(Dense(1, activation='linear'))\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-07-16T06:38:02.373578Z","iopub.execute_input":"2024-07-16T06:38:02.374016Z","iopub.status.idle":"2024-07-16T06:38:02.384294Z","shell.execute_reply.started":"2024-07-16T06:38:02.373984Z","shell.execute_reply":"2024-07-16T06:38:02.382971Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"input_dim = x_train.shape[1:]\nmodel = create_morlet_nn(input_dim)\nmodel.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n\nmodel.summary()\nhistory=model.fit(x_train, y_train, epochs=100, batch_size=32, validation_split=0.2)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T06:40:30.674502Z","iopub.execute_input":"2024-07-16T06:40:30.674963Z","iopub.status.idle":"2024-07-16T06:40:42.820557Z","shell.execute_reply.started":"2024-07-16T06:40:30.674928Z","shell.execute_reply":"2024-07-16T06:40:42.819371Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Model: \"sequential_5\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n batch_normalization_4 (Batc  (None, 2, 18)            72        \n hNormalization)                                                 \n                                                                 \n dense_12 (Dense)            (None, 2, 50)             950       \n                                                                 \n morlet_wavelet_9 (MorletWav  (None, 2, 50)            0         \n elet)                                                           \n                                                                 \n dense_13 (Dense)            (None, 2, 100)            5100      \n                                                                 \n morlet_wavelet_10 (MorletWa  (None, 2, 100)           0         \n velet)                                                          \n                                                                 \n dense_14 (Dense)            (None, 2, 50)             5050      \n                                                                 \n morlet_wavelet_11 (MorletWa  (None, 2, 50)            0         \n velet)                                                          \n                                                                 \n dense_15 (Dense)            (None, 2, 1)              51        \n                                                                 \n=================================================================\nTotal params: 11,223\nTrainable params: 11,187\nNon-trainable params: 36\n_________________________________________________________________\nEpoch 1/100\n13/13 [==============================] - 2s 27ms/step - loss: 0.4939 - val_loss: 0.8046\nEpoch 2/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.4223 - val_loss: 0.3468\nEpoch 3/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.4207 - val_loss: 0.1515\nEpoch 4/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.3552 - val_loss: 0.1400\nEpoch 5/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.3106 - val_loss: 0.1807\nEpoch 6/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.2924 - val_loss: 0.1784\nEpoch 7/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.2763 - val_loss: 0.2340\nEpoch 8/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.2342 - val_loss: 0.1107\nEpoch 9/100\n13/13 [==============================] - 0s 6ms/step - loss: 0.1997 - val_loss: 0.1234\nEpoch 10/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.1881 - val_loss: 0.1377\nEpoch 11/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.1766 - val_loss: 0.1794\nEpoch 12/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.1552 - val_loss: 0.2491\nEpoch 13/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.1245 - val_loss: 0.3030\nEpoch 14/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.1132 - val_loss: 0.2577\nEpoch 15/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.1029 - val_loss: 0.2037\nEpoch 16/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.0982 - val_loss: 0.1753\nEpoch 17/100\n13/13 [==============================] - 0s 6ms/step - loss: 0.0919 - val_loss: 0.1269\nEpoch 18/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.0833 - val_loss: 0.0807\nEpoch 19/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.0706 - val_loss: 0.0626\nEpoch 20/100\n13/13 [==============================] - 0s 6ms/step - loss: 0.0697 - val_loss: 0.0643\nEpoch 21/100\n13/13 [==============================] - 0s 7ms/step - loss: 0.0517 - val_loss: 0.0588\nEpoch 22/100\n13/13 [==============================] - 0s 6ms/step - loss: 0.0505 - val_loss: 0.0535\nEpoch 23/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.0446 - val_loss: 0.0516\nEpoch 24/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.0458 - val_loss: 0.0433\nEpoch 25/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.0390 - val_loss: 0.0352\nEpoch 26/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.0353 - val_loss: 0.0323\nEpoch 27/100\n13/13 [==============================] - 0s 6ms/step - loss: 0.0308 - val_loss: 0.0304\nEpoch 28/100\n13/13 [==============================] - 0s 6ms/step - loss: 0.0291 - val_loss: 0.0291\nEpoch 29/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.0271 - val_loss: 0.0280\nEpoch 30/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.0256 - val_loss: 0.0259\nEpoch 31/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.0221 - val_loss: 0.0272\nEpoch 32/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.0216 - val_loss: 0.0262\nEpoch 33/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.0213 - val_loss: 0.0244\nEpoch 34/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.0194 - val_loss: 0.0225\nEpoch 35/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.0181 - val_loss: 0.0218\nEpoch 36/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.0174 - val_loss: 0.0218\nEpoch 37/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.0171 - val_loss: 0.0212\nEpoch 38/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.0161 - val_loss: 0.0209\nEpoch 39/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.0156 - val_loss: 0.0207\nEpoch 40/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.0158 - val_loss: 0.0202\nEpoch 41/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.0143 - val_loss: 0.0193\nEpoch 42/100\n13/13 [==============================] - 0s 6ms/step - loss: 0.0155 - val_loss: 0.0185\nEpoch 43/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.0147 - val_loss: 0.0180\nEpoch 44/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.0147 - val_loss: 0.0176\nEpoch 45/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.0146 - val_loss: 0.0176\nEpoch 46/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.0139 - val_loss: 0.0174\nEpoch 47/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.0143 - val_loss: 0.0174\nEpoch 48/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.0139 - val_loss: 0.0174\nEpoch 49/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.0143 - val_loss: 0.0172\nEpoch 50/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0171\nEpoch 51/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.0138 - val_loss: 0.0171\nEpoch 52/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.0136 - val_loss: 0.0170\nEpoch 53/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.0136 - val_loss: 0.0170\nEpoch 54/100\n13/13 [==============================] - 0s 6ms/step - loss: 0.0136 - val_loss: 0.0171\nEpoch 55/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.0138 - val_loss: 0.0171\nEpoch 56/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.0136 - val_loss: 0.0170\nEpoch 57/100\n13/13 [==============================] - 0s 6ms/step - loss: 0.0138 - val_loss: 0.0170\nEpoch 58/100\n13/13 [==============================] - 0s 6ms/step - loss: 0.0135 - val_loss: 0.0169\nEpoch 59/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0169\nEpoch 60/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0169\nEpoch 61/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0169\nEpoch 62/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0170\nEpoch 63/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0170\nEpoch 64/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0170\nEpoch 65/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0170\nEpoch 66/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0170\nEpoch 67/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.0134 - val_loss: 0.0170\nEpoch 68/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.0136 - val_loss: 0.0170\nEpoch 69/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.0134 - val_loss: 0.0171\nEpoch 70/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.0136 - val_loss: 0.0172\nEpoch 71/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.0136 - val_loss: 0.0171\nEpoch 72/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0171\nEpoch 73/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0171\nEpoch 74/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0171\nEpoch 75/100\n13/13 [==============================] - 0s 6ms/step - loss: 0.0134 - val_loss: 0.0171\nEpoch 76/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0173\nEpoch 77/100\n13/13 [==============================] - 0s 6ms/step - loss: 0.0134 - val_loss: 0.0173\nEpoch 78/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0172\nEpoch 79/100\n13/13 [==============================] - 0s 6ms/step - loss: 0.0135 - val_loss: 0.0169\nEpoch 80/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0169\nEpoch 81/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0168\nEpoch 82/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0167\nEpoch 83/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.0136 - val_loss: 0.0168\nEpoch 84/100\n13/13 [==============================] - 0s 6ms/step - loss: 0.0137 - val_loss: 0.0169\nEpoch 85/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0170\nEpoch 86/100\n13/13 [==============================] - 0s 6ms/step - loss: 0.0136 - val_loss: 0.0171\nEpoch 87/100\n13/13 [==============================] - 0s 6ms/step - loss: 0.0135 - val_loss: 0.0171\nEpoch 88/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0170\nEpoch 89/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.0136 - val_loss: 0.0170\nEpoch 90/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.0134 - val_loss: 0.0169\nEpoch 91/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0168\nEpoch 92/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.0136 - val_loss: 0.0170\nEpoch 93/100\n13/13 [==============================] - 0s 6ms/step - loss: 0.0134 - val_loss: 0.0171\nEpoch 94/100\n13/13 [==============================] - 0s 7ms/step - loss: 0.0136 - val_loss: 0.0172\nEpoch 95/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0171\nEpoch 96/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0171\nEpoch 97/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.0134 - val_loss: 0.0173\nEpoch 98/100\n13/13 [==============================] - 0s 7ms/step - loss: 0.0136 - val_loss: 0.0173\nEpoch 99/100\n13/13 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0173\nEpoch 100/100\n13/13 [==============================] - 0s 6ms/step - loss: 0.0136 - val_loss: 0.0173\n","output_type":"stream"}]},{"cell_type":"code","source":"# Plot the training graph\nplt.plot(history.history['loss'], label='train_loss')\nplt.plot(history.history['val_loss'], label='val_loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.title('Training and Validation Loss')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-16T06:41:35.753872Z","iopub.execute_input":"2024-07-16T06:41:35.754360Z","iopub.status.idle":"2024-07-16T06:41:36.132550Z","shell.execute_reply.started":"2024-07-16T06:41:35.754306Z","shell.execute_reply":"2024-07-16T06:41:36.131367Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"markdown","source":"# LSTM Model","metadata":{}},{"cell_type":"code","source":"# trainX, trainY = data_china()\n\n\n# print('trainX shape == {}.'.format(trainX.shape))\n# print('trainY shape == {}.'.format(trainY.shape))\n\n# # define the Autoencoder model\n\n# model = Sequential()\n# model.add(BatchNormalization(input_shape=(trainX.shape[1], trainX.shape[2])))\n# model.add(Bidirectional(LSTM(64, activation='relu', return_sequences=True)))\n# model.add(LSTM(32, activation='relu', return_sequences=False))\n# model.add(Dropout(0.1))\n# model.add(Dense(trainY.shape[1]))\n\n# model.compile(optimizer='sgd', loss='mse')\n# model.summary()\n\n\n# # fit the model\n# history = model.fit(trainX, trainY, epochs=50, batch_size=32, validation_split=0.1, verbose=1)\n\n# plt.plot(history.history['loss'], label='Training loss')\n# plt.plot(history.history['val_loss'], label='Validation loss')\n","metadata":{"execution":{"iopub.status.busy":"2023-12-03T18:40:48.693138Z","iopub.execute_input":"2023-12-03T18:40:48.693959Z","iopub.status.idle":"2023-12-03T18:40:48.701912Z","shell.execute_reply.started":"2023-12-03T18:40:48.693915Z","shell.execute_reply":"2023-12-03T18:40:48.701247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def root_mean_squared_error(y_true, y_pred):\n    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n","metadata":{"execution":{"iopub.status.busy":"2023-12-03T18:40:48.855939Z","iopub.execute_input":"2023-12-03T18:40:48.85649Z","iopub.status.idle":"2023-12-03T18:40:48.86059Z","shell.execute_reply.started":"2023-12-03T18:40:48.856458Z","shell.execute_reply":"2023-12-03T18:40:48.859544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# trainY.shape[1]","metadata":{"execution":{"iopub.status.busy":"2023-12-03T18:40:48.862382Z","iopub.execute_input":"2023-12-03T18:40:48.863037Z","iopub.status.idle":"2023-12-03T18:40:48.871846Z","shell.execute_reply.started":"2023-12-03T18:40:48.86301Z","shell.execute_reply":"2023-12-03T18:40:48.870747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainX, trainY = data_china()\n# def create_and_train_autoencoder(trainX, trainY, epochs, batch_size, validation_split, \n#                                  learning_rate, dropout_rate, activation, \n#                                  optimizer, loss):\n    \n#     model = Sequential()\n#     model.add(BatchNormalization(input_shape=(trainX.shape[1], trainX.shape[2])))\n#     model.add(Bidirectional(LSTM(64, activation=activation, return_sequences=True)))\n#     model.add(LSTM(32, activation=activation, return_sequences=False))\n#     model.add(Dropout(dropout_rate))\n#     model.add(Dense(trainY.shape[1]))\n\n#     model.compile(optimizer=optimizer, loss=loss)\n#     model.summary()\n\n\n#     # fit the model\n#     history = model.fit(trainX, trainY, epochs=epochs, batch_size=batch_size, validation_split=validation_split, verbose=1)\n\n#     plt.plot(history.history['loss'], label='Training loss')\n#     plt.plot(history.history['val_loss'], label='Validation loss')\n\n#     return model, history\n\ndef create_and_train_autoencoder(trainX, trainY, epochs, batch_size, validation_split, \n                                 learning_rate, dropout_rate, lstm_units, activation, \n                                 optimizer, loss):\n    \n    model = Sequential()\n    model.add(BatchNormalization(input_shape=(trainX.shape[1], trainX.shape[2])))\n    model.add(Bidirectional(LSTM(lstm_units, activation=activation, return_sequences=True)))\n    model.add(LSTM(lstm_units // 2, activation=activation, return_sequences=True))\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(trainY.shape[1]))\n\n\n\n    \n    model.compile(optimizer=optimizer, loss=loss)\n    model.summary()\n\n    # Define early stopping\n    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n\n    # fit the model with early stopping\n    history = model.fit(trainX, trainY, epochs=epochs, batch_size=batch_size, \n                        validation_split=validation_split, verbose=1, callbacks=[early_stopping])\n   \n    plt.plot(history.history['loss'], label='Training loss')\n    plt.plot(history.history['val_loss'], label='Validation loss')\n\n    return model, history\n\n\n# Example usage:\n# Adjust hyperparameters as needed\n# autoencoder_model, training_history = create_and_train_autoencoder(trainX, trainY, \n#                                                                     epochs=500, batch_size=32, \n#                                                                     validation_split=0.1, \n#                                                                     learning_rate=best_hyperparameters[0], \n#                                                                     dropout_rate=best_hyperparameters[1], \n#                                                                     lstm_units=int(best_hyperparameters[2]),\n#                                                                     activation='relu', \n#                                                                     optimizer='adam', \n#                                                                     loss='mse')\n\nautoencoder_model, training_history = create_and_train_autoencoder(trainX, trainY, \n                                                                    epochs=100, batch_size=16, \n                                                                    validation_split=0.1, \n                                                                    learning_rate=0.01, \n                                                                    dropout_rate=0.1,\n                                                                    lstm_units= 64,\n                                                                    activation='sigmoid', \n                                                                    optimizer='adam', \n                                                                    loss= 'mae',\n                                                                    \n                                                                  )","metadata":{"execution":{"iopub.status.busy":"2023-12-03T18:40:48.873077Z","iopub.execute_input":"2023-12-03T18:40:48.873343Z","iopub.status.idle":"2023-12-03T18:40:56.321274Z","shell.execute_reply.started":"2023-12-03T18:40:48.873322Z","shell.execute_reply":"2023-12-03T18:40:56.320228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\ndef harmonic_search(objective_function, search_space, num_iterations):\n    # Initialize parameters\n    num_harmonics = 10\n    pitch_adjustment_rate = 0.01\n\n    # Initialize random solutions within the search space\n    solutions = np.random.uniform(low=search_space[:, 0], high=search_space[:, 1], size=(num_harmonics, len(search_space)))\n\n    for iteration in range(num_iterations):\n        # Evaluate the performance of each solution\n        scores = [objective_function(solution) for solution in solutions]\n\n        # Select the top-performing solutions as parents\n        parents = solutions[np.argsort(scores)[:2]]\n\n        # Generate new candidate solutions by combining and modifying parents\n        new_solutions = parents[0] + np.random.uniform(low=-pitch_adjustment_rate, high=pitch_adjustment_rate, size=parents.shape)\n\n        # Clip new solutions to the search space\n        new_solutions = np.clip(new_solutions, search_space[:, 0], search_space[:, 1])\n\n        # Replace the worst solutions with the new ones\n        worst_index = np.argmax(scores)\n        solutions[worst_index] = new_solutions[0]  # Take the first parent as the new solution\n\n    # Return the best solution found\n    best_solution = solutions[np.argmin(scores)]\n    return best_solution\n\n# Example usage:\n# Define the search space for hyperparameters\nsearch_space = np.array([\n    [0.001, 0.1],  # Learning Rate\n    [0.1, 0.9],    # Dropout Rate\n    [16, 64],     # Number of LSTM units\n])\n\n# Define your objective function (replace with your actual training and evaluation logic)\ndef objective_function(hyperparameters):\n    learning_rate, dropout_rate, lstm_units = hyperparameters\n    \n    try:\n        # Create and train LSTM model with the given hyperparameters\n        # Return the performance metric to be minimized (e.g., validation loss)\n        autoencoder_model, training_history = create_and_train_autoencoder(trainX, trainY, \n                                                                            epochs=100, batch_size=16, \n                                                                            validation_split=0.1, \n                                                                            learning_rate=learning_rate, \n                                                                            dropout_rate=dropout_rate, \n                                                                            lstm_units=int(lstm_units),\n                                                                            activation='sigmoid', \n                                                                            optimizer='adam')\n        \n        # Retrieve the performance metric (e.g., validation loss) from the training history\n        metric = min(training_history.history['val_loss'])  # Assuming 'val_loss' is the relevant metric\n\n        return metric\n    except Exception as e:\n        # Return a large value in case of an error\n        return float('inf')\n\n# Run harmonic search\nbest_hyperparameters = harmonic_search(objective_function, search_space, num_iterations=50)\n\n# Print or log the best hyperparameters\nprint(\"Best Hyperparameters:\")\nprint(\"Learning Rate:\", best_hyperparameters[0])\nprint(\"Dropout Rate:\", best_hyperparameters[1])\nprint(\"LSTM Units:\", int(best_hyperparameters[2]))\n\n# Update your LSTM model with the best hyperparameters\nautoencoder_model, training_history = create_and_train_autoencoder(trainX, trainY, \n                                                                    epochs=100, batch_size=16, \n                                                                    validation_split=0.1, \n                                                                    learning_rate=best_hyperparameters[0], \n                                                                    dropout_rate=best_hyperparameters[1], \n                                                                    lstm_units=int(best_hyperparameters[2]),\n                                                                    activation='sigmoid', \n                                                                    optimizer='adam', \n                                                                    loss='mae',\n                                                                    \n                                                                  )\n\n# Optionally, you can also print or log other relevant information, such as the best performance metric\nbest_metric = objective_function(best_hyperparameters)\nprint(\"Best Performance Metric:\", best_metric)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-03T18:40:56.324666Z","iopub.execute_input":"2023-12-03T18:40:56.325943Z","iopub.status.idle":"2023-12-03T18:41:08.408575Z","shell.execute_reply.started":"2023-12-03T18:40:56.325879Z","shell.execute_reply":"2023-12-03T18:41:08.407902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#     from sklearn.preprocessing import MinMaxScaler\n#     df = pd.read_csv(CFG[1])\n# #     x = df.drop(columns=['id','ID'])\n#     # Assuming your dataset is stored in a DataFrame named 'df'\n#     # If not, replace 'df' with the actual name of your DataFrame\n\n#     # Extract the target variable\n#     target_variable = 'Effort'\n#     y = df[target_variable]\n\n#     # Extract features (excluding target variable)\n#     X = df.drop(columns=['id','ID', 'Effort'])\n\n#     # Normalize the dataset\n#     scaler = MinMaxScaler()\n#     X_normalized = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n\n#     # Split the dataset into training and testing sets\n#     X_train, X_test, y_train, y_test = train_test_split(X_normalized, y, test_size=0.2, random_state=42)\n\n#     # Check the shapes of the resulting sets\n#     print(\"X_train shape:\", X_train.shape)\n#     print(\"X_test shape:\", X_test.shape)\n#     print(\"y_train shape:\", y_train.shape)\n#     print(\"y_test shape:\", y_test.shape)\n# df","metadata":{"execution":{"iopub.status.busy":"2023-12-03T18:41:08.409879Z","iopub.execute_input":"2023-12-03T18:41:08.410479Z","iopub.status.idle":"2023-12-03T18:41:08.415774Z","shell.execute_reply.started":"2023-12-03T18:41:08.410454Z","shell.execute_reply":"2023-12-03T18:41:08.414586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn import datasets\n# from sklearn.model_selection import train_test_split\n# from sklearn.svm import SVR\n# from sklearn.metrics import  mean_absolute_error\n\n\n\n# # Create an SVM regressor\n# svm_regressor = SVR(kernel='linear', C=1)\n\n# svm_regressor.fit(X_train, y_train)\n\n# # Make predictions on the test set\n# y_pred = svm_regressor.predict(X_test)\n\n# # Evaluate mean squared error\n# mae = mean_absolute_error(y_test, y_pred)\n# print(f\"Mean Squared Error: {mae}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-12-03T18:41:08.41725Z","iopub.execute_input":"2023-12-03T18:41:08.417544Z","iopub.status.idle":"2023-12-03T18:41:08.425814Z","shell.execute_reply.started":"2023-12-03T18:41:08.417521Z","shell.execute_reply":"2023-12-03T18:41:08.424822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# trainX.shape","metadata":{"execution":{"iopub.status.busy":"2023-12-03T18:41:08.427028Z","iopub.execute_input":"2023-12-03T18:41:08.427281Z","iopub.status.idle":"2023-12-03T18:41:08.435802Z","shell.execute_reply.started":"2023-12-03T18:41:08.42726Z","shell.execute_reply":"2023-12-03T18:41:08.434271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Attention, Activation\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport matplotlib.pyplot as plt\ndf = pd.read_csv(CFG[1])\n# Load your dataset (assuming it's in a DataFrame named 'df')\n# Replace 'Effort' with the actual column name you want to predict\ntarget_column = 'Effort'\n\n# Separate features (X) and target variable (y)\nX = df.drop(target_column, axis=1)\ny = df[target_column]\n\n# Data Preprocessing\nscaler_X = MinMaxScaler(feature_range=(0, 1))\nscaler_y = MinMaxScaler(feature_range=(0, 1))\n\nX_normalized = scaler_X.fit_transform(X.values)\ny_normalized = scaler_y.fit_transform(y.values.reshape(-1, 1))\n\n# Reshape X for LSTM input (assuming you have a sequence length)\nsequence_length = 1  # Adjust based on your dataset\nX_reshaped = X_normalized.reshape((X_normalized.shape[0], sequence_length, X_normalized.shape[1]))\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_reshaped, y_normalized, test_size=0.2, random_state=42)\n\n# Model Construction and Training\ninp = Input(shape=(X_reshaped.shape[1], X_reshaped.shape[2]))\ne = Sequential()(inp)\ng = LSTM(units=64, return_sequences=True)(e)\nh = LSTM(units=32, return_sequences=True)(g)\ns = Attention(use_scale=True)([h, h])\nf = Activation('relu')(s)\nout = Dense(1)(f)  # Assuming a regression task\n\nmodel = Model(inputs=inp, outputs=out)\nmodel.compile(optimizer='adam', loss='mean_squared_error')  # You can use other loss functions for regression\n\n# Implement Early Stopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n\n# Train the model and monitor the training history\nhistory = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.1, callbacks=[early_stopping])\n\n# Plot the training and validation loss\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\nplt.show()\n\n\n# Prediction\ny_pred = model.predict(X_test)\n\n# Rescale predictions to the original scale\ny_pred_rescaled = scaler_y.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n\n# Rescale true labels to the original scale\ny_test_rescaled = scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten()\n\n# Evaluate mean absolute error\nmae = np.mean(np.abs(y_test_rescaled - y_pred_rescaled))\nprint(f\"Mean Absolute Error: {mae}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-12-03T18:41:08.437576Z","iopub.execute_input":"2023-12-03T18:41:08.438828Z","iopub.status.idle":"2023-12-03T18:41:22.808409Z","shell.execute_reply.started":"2023-12-03T18:41:08.438755Z","shell.execute_reply":"2023-12-03T18:41:22.807583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(CFG[1])\n# df = df.drop(columns=['id','ID'])\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2023-12-03T18:41:22.810752Z","iopub.execute_input":"2023-12-03T18:41:22.811034Z","iopub.status.idle":"2023-12-03T18:41:22.823386Z","shell.execute_reply.started":"2023-12-03T18:41:22.811013Z","shell.execute_reply":"2023-12-03T18:41:22.822388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Attention, Activation\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.metrics import mean_absolute_error\nimport optuna\n\ndf = pd.read_csv(CFG[7])\n# df = df.drop(columns=['id','ID'])\n# Load your dataset (assuming it's in a DataFrame named 'df')\n# Replace 'Effort' with the actual column name you want to predict\ntarget_column = 'Effort'\n\n# Separate features (X) and target variable (y)\nX = df.drop(target_column, axis=1)\ny = df[target_column]\n\n# Data Preprocessing\nscaler_X = MinMaxScaler(feature_range=(0, 1))\nscaler_y = MinMaxScaler(feature_range=(0, 1))\n\nX_normalized = scaler_X.fit_transform(X.values)\ny_normalized = scaler_y.fit_transform(y.values.reshape(-1, 1))\n\n# Reshape X for LSTM input (assuming you have a sequence length)\nsequence_length = 1  # Adjust based on your dataset\nX_reshaped = X_normalized.reshape((X_normalized.shape[0], sequence_length, X_normalized.shape[1]))\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_reshaped, y_normalized, test_size=0.2, random_state=42)\n\n# Model Construction and Training\ndef build_model(units1, units2, learning_rate):\n    inp = Input(shape=(X_reshaped.shape[1], X_reshaped.shape[2]))\n    e = Sequential()(inp)\n    g = LSTM(units=units1, return_sequences=True)(e)\n    h = LSTM(units=units2, return_sequences=True)(g)\n    s = Attention(use_scale=True)([h, h])\n    f = Activation('relu')(s)\n    out = Dense(1)(f)  # Assuming a regression task\n\n    model = Model(inputs=inp, outputs=out)\n    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n    model.compile(optimizer=optimizer, loss='mean_squared_error')  # You can use other loss functions for regression\n    \n    return model\n\n# Implement Early Stopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n\n# Objective function for Optuna\ndef objective(trial):\n    units1 = trial.suggest_int('units1', 32, 128)\n    units2 = trial.suggest_int('units2', 16, 64)\n    learning_rate = trial.suggest_loguniform('learning_rate', 0.001, 0.1)\n\n    model = build_model(units1=units1, units2=units2, learning_rate=learning_rate)\n    \n    history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.1, callbacks=[early_stopping], verbose=0)\n    \n    return mean_absolute_error(scaler_y.inverse_transform(y_test.reshape(-1, 1)), scaler_y.inverse_transform(model.predict(X_test).reshape(-1, 1)))\n\n# Perform hyperparameter tuning with Optuna\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=10)\n\n# Get the best hyperparameters\nbest_params = study.best_params\n\n# Train the final model with the best hyperparameters\nbest_model = build_model(**best_params)\nbest_model.fit(X_train, y_train, epochs=200, batch_size=32, validation_split=0.1, callbacks=[early_stopping])\n\n# Plot the training and validation loss\nplt.plot(best_model.history.history['loss'], label='Train Loss')\nplt.plot(best_model.history.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\nplt.show()\n\n# Prediction\ny_pred = best_model.predict(X_test)\n\n# Rescale predictions to the original scale\ny_pred_rescaled = scaler_y.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n\n# Rescale true labels to the original scale\ny_test_rescaled = scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten()\n\n# Evaluate mean absolute error\nmae = mean_absolute_error(y_test_rescaled, y_pred_rescaled)\nprint(f\"Mean Absolute Error: {mae}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-12-03T18:41:22.824773Z","iopub.execute_input":"2023-12-03T18:41:22.825062Z","iopub.status.idle":"2023-12-03T18:42:31.7374Z","shell.execute_reply.started":"2023-12-03T18:41:22.82504Z","shell.execute_reply":"2023-12-03T18:42:31.736504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Attention, Activation\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\n\n# Load your dataset (assuming it's in a DataFrame named 'df')\n# Replace 'Effort' with the actual column name you want to predict\ntarget_column = 'Effort'\n\n# Separate features (X) and target variable (y)\nX = df.drop(target_column, axis=1)\ny = df[target_column]\n\n# Data Preprocessing\nscaler_X = MinMaxScaler(feature_range=(0, 1))\nscaler_y = MinMaxScaler(feature_range=(0, 1))\n\nX_normalized = scaler_X.fit_transform(X.values)\ny_normalized = scaler_y.fit_transform(y.values.reshape(-1, 1))\n\n# Reshape X for LSTM input (assuming you have a sequence length)\nsequence_length = 1  # Adjust based on your dataset\nX_reshaped = X_normalized.reshape((X_normalized.shape[0], sequence_length, X_normalized.shape[1]))\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_reshaped, y_normalized, test_size=0.2, random_state=42)\n\n# Model Construction and Training\ninp = Input(shape=(X_reshaped.shape[1], X_reshaped.shape[2]))\ne = Sequential()(inp)\ng = LSTM(units=64, return_sequences=True)(e)\nh = LSTM(units=32, return_sequences=True)(g)\ns = Attention(use_scale=True)([h, h])\nf = Activation('relu')(s)\nout = Dense(1)(f)  # Assuming a regression task\n\nmodel = Model(inputs=inp, outputs=out)\nmodel.compile(optimizer='adam', loss='mean_squared_error')  # You can use other loss functions for regression\n\n# Implement Early Stopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n\n# Train the model and monitor the training history\nhistory = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.1, callbacks=[early_stopping])\n\n# Plot the training and validation loss\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\nplt.show()\n\n# Prediction\ny_pred = model.predict(X_test)\n\n# Rescale predictions to the original scale\ny_pred_rescaled = scaler_y.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n\n# Rescale true labels to the original scale\ny_test_rescaled = scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten()\n\n# Evaluate root mean squared error (RMSE)\nrmse = np.sqrt(mean_squared_error(y_test_rescaled, y_pred_rescaled))\nprint(f\"Root Mean Squared Error (RMSE): {rmse}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-12-03T18:42:31.739012Z","iopub.execute_input":"2023-12-03T18:42:31.739328Z","iopub.status.idle":"2023-12-03T18:42:37.258957Z","shell.execute_reply.started":"2023-12-03T18:42:31.7393Z","shell.execute_reply":"2023-12-03T18:42:37.258028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Attention, Activation\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.metrics import mean_squared_error\nimport optuna\n\n# Load your dataset (assuming it's in a DataFrame named 'df')\n# Replace 'Effort' with the actual column name you want to predict\ntarget_column = 'Effort'\n\n# Separate features (X) and target variable (y)\nX = df.drop(target_column, axis=1)\ny = df[target_column]\n\n# Data Preprocessing\nscaler_X = MinMaxScaler(feature_range=(0, 1))\nscaler_y = MinMaxScaler(feature_range=(0, 1))\n\nX_normalized = scaler_X.fit_transform(X.values)\ny_normalized = scaler_y.fit_transform(y.values.reshape(-1, 1))\n\n# Reshape X for LSTM input (assuming you have a sequence length)\nsequence_length = 1  # Adjust based on your dataset\nX_reshaped = X_normalized.reshape((X_normalized.shape[0], sequence_length, X_normalized.shape[1]))\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_reshaped, y_normalized, test_size=0.2, random_state=42)\n\n# Model Construction and Training\ndef build_model(units1, units2, learning_rate):\n    inp = Input(shape=(X_reshaped.shape[1], X_reshaped.shape[2]))\n    e = Sequential()(inp)\n    g = LSTM(units=units1, return_sequences=True)(e)\n    h = LSTM(units=units2, return_sequences=True)(g)\n    s = Attention(use_scale=True)([h, h])\n    f = Activation('relu')(s)\n    out = Dense(1)(f)  # Assuming a regression task\n\n    model = Model(inputs=inp, outputs=out)\n    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n    model.compile(optimizer=optimizer, loss='mean_squared_error')  # You can use other loss functions for regression\n    \n    return model\n\n# Implement Early Stopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n\n# Objective function for Optuna\ndef objective(trial):\n    units1 = trial.suggest_int('units1', 32, 128)\n    units2 = trial.suggest_int('units2', 16, 64)\n    learning_rate = trial.suggest_loguniform('learning_rate', 0.001, 0.1)\n\n    model = build_model(units1=units1, units2=units2, learning_rate=learning_rate)\n    \n    history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.1, callbacks=[early_stopping], verbose=0)\n    \n    return mean_absolute_error(scaler_y.inverse_transform(y_test.reshape(-1, 1)), scaler_y.inverse_transform(model.predict(X_test).reshape(-1, 1)))\n\n# Perform hyperparameter tuning with Optuna\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=10)\n\n# Get the best hyperparameters\nbest_params = study.best_params\n\n# Train the final model with the best hyperparameters\nbest_model = build_model(**best_params)\nbest_model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.1, callbacks=[early_stopping])\n\n# Plot the training and validation loss\nplt.plot(best_model.history.history['loss'], label='Train Loss')\nplt.plot(best_model.history.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\nplt.show()\n\n# Prediction\ny_pred = best_model.predict(X_test)\n\n# Rescale predictions to the original scale\ny_pred_rescaled = scaler_y.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n\n# Rescale true labels to the original scale\ny_test_rescaled = scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten()\n\n# Evaluate root mean squared error (RMSE)\nrmse = np.sqrt(mean_squared_error(y_test_rescaled, y_pred_rescaled))\nprint(f\"Root Mean Squared Error (RMSE): {rmse}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-12-03T18:42:37.260272Z","iopub.execute_input":"2023-12-03T18:42:37.260566Z","iopub.status.idle":"2023-12-03T18:43:50.480486Z","shell.execute_reply.started":"2023-12-03T18:42:37.260543Z","shell.execute_reply":"2023-12-03T18:43:50.479477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Attention, Activation\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport optuna\n\n# Load your dataset (assuming it's in a DataFrame named 'df')\n# Replace 'Effort' with the actual column name you want to predict\ntarget_column = 'Effort'\n\n# Separate features (X) and target variable (y)\nX = df.drop(target_column, axis=1)\ny = df[target_column]\n\n# Data Preprocessing\nscaler_X = MinMaxScaler(feature_range=(0, 1))\nscaler_y = MinMaxScaler(feature_range=(0, 1))\n\nX_normalized = scaler_X.fit_transform(X.values)\ny_normalized = scaler_y.fit_transform(y.values.reshape(-1, 1))\n\n# Reshape X for LSTM input (assuming you have a sequence length)\nsequence_length = 1  # Adjust based on your dataset\nX_reshaped = X_normalized.reshape((X_normalized.shape[0], sequence_length, X_normalized.shape[1]))\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_reshaped, y_normalized, test_size=0.2, random_state=42)\n\n# Model Construction and Training\ndef build_model(units1, units2, learning_rate):\n    inp = Input(shape=(X_reshaped.shape[1], X_reshaped.shape[2]))\n    e = Sequential()(inp)\n    g = LSTM(units=units1, return_sequences=True)(e)\n    h = LSTM(units=units2, return_sequences=True)(g)\n    s = Attention(use_scale=True)([h, h])\n    f = Activation('relu')(s)\n    out = Dense(1)(f)  # Assuming a regression task\n\n    model = Model(inputs=inp, outputs=out)\n    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n    model.compile(optimizer=optimizer, loss='mean_squared_error')  # You can use other loss functions for regression\n    \n    return model\n\n# Implement Early Stopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n\n# Objective function for Optuna\ndef objective(trial):\n    units1 = trial.suggest_int('units1', 32, 128)\n    units2 = trial.suggest_int('units2', 16, 64)\n    learning_rate = trial.suggest_loguniform('learning_rate', 0.001, 0.1)\n\n    model = build_model(units1=units1, units2=units2, learning_rate=learning_rate)\n    \n    history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.1, callbacks=[early_stopping], verbose=0)\n    \n    return mean_absolute_error(scaler_y.inverse_transform(y_test.reshape(-1, 1)), scaler_y.inverse_transform(model.predict(X_test).reshape(-1, 1)))\n\n# Perform hyperparameter tuning with Optuna\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=10)\n\n# Get the best hyperparameters\nbest_params = study.best_params\n\n# Train the final model with the best hyperparameters\nbest_model = build_model(**best_params)\nbest_model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.1, callbacks=[early_stopping])\n\n# Plot the training and validation loss\nplt.plot(best_model.history.history['loss'], label='Train Loss')\nplt.plot(best_model.history.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\nplt.show()\n\n# Prediction\ny_pred = best_model.predict(X_test)\n\n# Rescale predictions to the original scale\ny_pred_rescaled = scaler_y.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n\n# Rescale true labels to the original scale\ny_test_rescaled = scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten()\n\n# Evaluate R-squared\nr_squared = r2_score(y_test_rescaled, y_pred_rescaled)\nprint(f\"R-squared: {r_squared}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-12-03T18:43:50.483512Z","iopub.execute_input":"2023-12-03T18:43:50.483912Z","iopub.status.idle":"2023-12-03T18:44:59.982447Z","shell.execute_reply.started":"2023-12-03T18:43:50.483883Z","shell.execute_reply":"2023-12-03T18:44:59.981439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
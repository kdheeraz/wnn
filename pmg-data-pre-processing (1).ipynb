{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6801955,"sourceType":"datasetVersion","datasetId":3909889}],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline,make_pipeline\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Bidirectional\nfrom matplotlib import pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras import backend as K","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:27:32.726455Z","iopub.execute_input":"2024-07-05T13:27:32.726761Z","iopub.status.idle":"2024-07-05T13:27:42.569530Z","shell.execute_reply.started":"2024-07-05T13:27:32.726724Z","shell.execute_reply":"2024-07-05T13:27:42.568630Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"CFG = [\n    '/kaggle/input/software-effort-estimation-datasets/albrecht.csv',\n    '/kaggle/input/software-effort-estimation-datasets/china.csv',\n    '/kaggle/input/software-effort-estimation-datasets/desharnais.csv',\n    '/kaggle/input/software-effort-estimation-datasets/finnish.csv',\n    '/kaggle/input/software-effort-estimation-datasets/isbsg10.csv',\n    '/kaggle/input/software-effort-estimation-datasets/kemerer.csv',\n    '/kaggle/input/software-effort-estimation-datasets/kitchenham.csv',\n    '/kaggle/input/software-effort-estimation-datasets/maxwell.csv',\n    '/kaggle/input/software-effort-estimation-datasets/miyazaki94.csv'\n]","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:28:08.651037Z","iopub.execute_input":"2024-07-05T13:28:08.652278Z","iopub.status.idle":"2024-07-05T13:28:08.658340Z","shell.execute_reply.started":"2024-07-05T13:28:08.652231Z","shell.execute_reply":"2024-07-05T13:28:08.657117Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"code","source":"def data_albrecht():\n    df = pd.read_csv(CFG[0])\n    df_for_training = df.astype(float)\n    #LSTM uses sigmoid and tanh that are sensitive to magnitude so values need to be normalized\n    # normalize the dataset\n    scaler = MinMaxScaler()\n    scaler = scaler.fit(df_for_training)\n    df_for_training_scaled = scaler.transform(df_for_training)\n    return df\n\n\ndef data_china():\n    \n    df = pd.read_csv(CFG[1])\n    df = df.drop(columns=['id','ID'])\n    df_for_training = df.astype(float)\n#     print(df.columns)\n    #LSTM uses sigmoid and tanh that are sensitive to magnitude so values need to be normalized\n    # normalize the dataset\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    scaler = scaler.fit(df_for_training)\n    df_for_training_scaled = scaler.transform(df_for_training)\n    \n    trainX = []\n    trainY = []\n    n_future = 2   # Number of days we want to look into the future based on the past days.\n    n_past = 2 # Number of past days we want to use to predict the future.\n\n    for i in range(n_past, len(df_for_training_scaled) - n_future +1):\n        trainX.append(df_for_training_scaled[i - n_past:i, 0:df_for_training.shape[1]])\n        trainY.append(df_for_training_scaled[i + n_future - 1:i + n_future, df_for_training.shape[1] - 1])\n\n    trainX, trainY = np.array(trainX), np.array(trainY)\n\n    return trainX, trainY\n\n\ndef data_desharnais():\n    df = pd.read_csv(CFG[2])\n    df = df.drop(columns=['Project','YearEnd', 'Language'])\n    df_for_training = df.astype(float)\n    #LSTM uses sigmoid and tanh that are sensitive to magnitude so values need to be normalized\n    # normalize the dataset\n    scaler = StandardScaler()\n    scaler = scaler.fit(df_for_training)\n    df_for_training_scaled = scaler.transform(df_for_training)\n    return df\n\n\ndef data_finnish():\n    df = pd.read_csv(CFG[3])\n    df_for_training = df.astype(float)\n    #LSTM uses sigmoid and tanh that are sensitive to magnitude so values need to be normalized\n    # normalize the dataset\n    scaler = MinMaxScaler()\n    scaler = scaler.fit(df_for_training)\n    df_for_training_scaled = scaler.transform(df_for_training)\n    return df\n\n\ndef data_isbsg10():\n\n    df = pd.read_csv(CFG[4])\n    df = df.drop(columns = ['id', 'ID', 'Data_Quality', 'Year','AG', 'N_effort_level1',\n           'N_PDR1', 'SDR', 'PET', 'PIT', 'I_Date', 'PAS',\n           'Recording_Method', 'Resource_Level', 'MTS', 'ATS', 'R_PWE_NPA',\n           'P_UWE', 'CASE_Tool', 'UM', 'HMA', 'Hardware1',\n           'IDE', 'DT1', 'DBS1', 'CS1', 'WS1', 'MS1', 'OP1', 'RTA',\n           'SP_CMMI', 'SP_ISO', 'SP_TICKIT', 'MIN_Defects', 'MAJ_Defects',\n           'X_Defects', 'TOT_Defects', 'UB_BU', 'UB_L', 'UB_DU', 'UB_CU',\n           'IMarket', 'T_Platform', 'D_Embedded', 'SE', 'SEA', 'SEM', 'E_Estimate',\n           'E_Estimate_Method', 'DDE', 'DDEM', 'C_Estimate', 'CEC', 'CEM',\n           'E_Tool', 'E_Comments', 'EC_Date', 'SR?', 'SR', 'R_FPC', 'R_FPA',\n           'P_Defects', 'D_Defects', 'MIN_B_Defects', 'MAJ_B_Defects',\n           'X_B_Defects', 'TOT_B_Defects', 'MIN_T_Defects', 'MAJ_T_Defects',\n           'X_T_Defects', 'TOT_T_Defects','S_Defects', 'MIN_I_Defects', 'MAJ_I_Defects',\n           'X_I_Defects', 'TOT_I_Defects'], axis = 1)\n    # Replace \"?\" with numpy.nan\n    df.replace('?', np.nan, inplace=True)\n#     print(df.columns)\n    \n    # imputation transformer\n    trf1 = ColumnTransformer([\n        ('impute',SimpleImputer(strategy='constant', fill_value='Missing'),[14,15,16,17,18])\n    ],remainder='passthrough')\n    \n    trf2 = ColumnTransformer([\n    ('ohe',OneHotEncoder(sparse_output=False,drop= 'first'),[0,1,2,3,4,5,6,7,8,9,10,11,12,13,15])\n    ],remainder='passthrough')\n    \n    pipe = Pipeline([\n    ('trf1',trf1),\n    ('trf2',trf2),\n    ])\n    \n    pipe.fit_transform(df)\n    \n    return pipe.fit_transform(df)\n\n\ndef data_kemerer():\n    df = pd.read_csv(CFG[5])\n    df_for_training = df.astype(float)\n    #LSTM uses sigmoid and tanh that are sensitive to magnitude so values need to be normalized\n    # normalize the dataset\n    scaler = StandardScaler()\n    scaler = scaler.fit(df_for_training)\n    df_for_training_scaled = scaler.transform(df_for_training)\n    return df\n    \n\n    \ndef data_kitchenham():\n    df = pd.read_csv(CFG[6])\n    df_for_training = df.astype(float)\n    #LSTM uses sigmoid and tanh that are sensitive to magnitude so values need to be normalized\n    # normalize the dataset\n    scaler = MinMaxScaler()\n    scaler = scaler.fit(df_for_training)\n    df_for_training_scaled = scaler.transform(df_for_training)\n    return df\n\n\ndef data_maxwell():\n    df = pd.read_csv(CFG[7])\n    df_for_training = df.astype(float)\n    #LSTM uses sigmoid and tanh that are sensitive to magnitude so values need to be normalized\n    # normalize the dataset\n    scaler = StandardScaler()\n    scaler = scaler.fit(df_for_training)\n    df_for_training_scaled = scaler.transform(df_for_training)\n    return df\n\n\ndef data_miyazaki():\n    df = pd.read_csv(CFG[8])\n    df_for_training = df.astype(float)\n    #LSTM uses sigmoid and tanh that are sensitive to magnitude so values need to be normalized\n    # normalize the dataset\n    scaler = StandardScaler()\n    scaler = scaler.fit(df_for_training)\n    df_for_training_scaled = scaler.transform(df_for_training)\n    return df\n","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:28:12.388783Z","iopub.execute_input":"2024-07-05T13:28:12.389402Z","iopub.status.idle":"2024-07-05T13:28:12.420779Z","shell.execute_reply.started":"2024-07-05T13:28:12.389359Z","shell.execute_reply":"2024-07-05T13:28:12.419745Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import pywt","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:30:36.246523Z","iopub.execute_input":"2024-07-05T13:30:36.247546Z","iopub.status.idle":"2024-07-05T13:30:36.346397Z","shell.execute_reply.started":"2024-07-05T13:30:36.247500Z","shell.execute_reply":"2024-07-05T13:30:36.345441Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Define the Morlet wavelet activation function\ndef morlet_wavelet(x, w=5.0):\n    return np.cos(w * x) * np.exp(-x**2 / 2)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:30:51.069735Z","iopub.execute_input":"2024-07-05T13:30:51.070607Z","iopub.status.idle":"2024-07-05T13:30:51.076293Z","shell.execute_reply.started":"2024-07-05T13:30:51.070561Z","shell.execute_reply":"2024-07-05T13:30:51.074977Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Layer","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:32:09.599321Z","iopub.execute_input":"2024-07-05T13:32:09.600138Z","iopub.status.idle":"2024-07-05T13:32:09.605488Z","shell.execute_reply.started":"2024-07-05T13:32:09.600092Z","shell.execute_reply":"2024-07-05T13:32:09.604293Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class MorletWaveletLayer(Layer):\n    def __init__(self, units, w=5.0, **kwargs):\n        super(MorletWaveletLayer, self).__init__(**kwargs)\n        self.units = units\n        self.w = w\n\n    def build(self, input_shape):\n        self.kernel = self.add_weight(name='kernel',\n                                      shape=(input_shape[-1], self.units),\n                                      initializer='glorot_uniform',\n                                      trainable=True)\n\n    def call(self, inputs):\n        # Reshape inputs to 2D for the dense operation\n        input_reshaped = tf.reshape(inputs, [-1, inputs.shape[-1]])\n        z = tf.matmul(input_reshaped, self.kernel)\n\n        # Implement Morlet wavelet using TensorFlow operations\n        wavelet_output = tf.cos(self.w * z) * tf.exp(-z**2 / 2)\n\n        # Reshape back to 3D\n        return tf.reshape(wavelet_output, tf.concat([tf.shape(inputs)[:-1], [self.units]], axis=0))","metadata":{"execution":{"iopub.status.busy":"2024-07-05T14:00:17.594700Z","iopub.execute_input":"2024-07-05T14:00:17.595351Z","iopub.status.idle":"2024-07-05T14:00:17.604889Z","shell.execute_reply.started":"2024-07-05T14:00:17.595314Z","shell.execute_reply":"2024-07-05T14:00:17.603880Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"trainX, trainY = data_china()","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:35:15.837314Z","iopub.execute_input":"2024-07-05T13:35:15.837985Z","iopub.status.idle":"2024-07-05T13:35:15.854724Z","shell.execute_reply.started":"2024-07-05T13:35:15.837954Z","shell.execute_reply":"2024-07-05T13:35:15.853927Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"trainX.shape","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:44:16.370996Z","iopub.execute_input":"2024-07-05T13:44:16.371324Z","iopub.status.idle":"2024-07-05T13:44:16.377337Z","shell.execute_reply.started":"2024-07-05T13:44:16.371297Z","shell.execute_reply":"2024-07-05T13:44:16.376380Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"(496, 2, 18)"},"metadata":{}}]},{"cell_type":"code","source":"# Firefly Algorithm for optimization\nclass FireflyAlgorithm:\n    def __init__(self, func, lb, ub, dim, n_fireflies=20, max_iter=100, alpha=0.5, beta=0.2, gamma=1.0):\n        self.func = func\n        self.lb = lb\n        self.ub = ub\n        self.dim = dim\n        self.n_fireflies = n_fireflies\n        self.max_iter = max_iter\n        self.alpha = alpha\n        self.beta = beta\n        self.gamma = gamma\n\n    def optimize(self):\n        fireflies = np.random.uniform(self.lb, self.ub, (self.n_fireflies, self.dim))\n        fitness = np.apply_along_axis(self.func, 1, fireflies)\n        \n        best_firefly = fireflies[np.argmin(fitness)]\n        best_fitness = np.min(fitness)\n\n        for t in range(self.max_iter):\n            for i in range(self.n_fireflies):\n                for j in range(self.n_fireflies):\n                    if fitness[j] < fitness[i]:\n                        r = np.linalg.norm(fireflies[i] - fireflies[j])\n                        beta = self.beta * np.exp(-self.gamma * r ** 2)\n                        fireflies[i] += beta * (fireflies[j] - fireflies[i]) + self.alpha * (np.random.rand(self.dim) - 0.5)\n                        fireflies[i] = np.clip(fireflies[i], self.lb, self.ub)\n                        fitness[i] = self.func(fireflies[i])\n                        \n                        if fitness[i] < best_fitness:\n                            best_firefly = fireflies[i]\n                            best_fitness = fitness[i]\n\n            self.alpha *= 0.97  # Decrease alpha over time\n        \n        return best_firefly, best_fitness","metadata":{"execution":{"iopub.status.busy":"2024-07-05T14:18:54.008058Z","iopub.execute_input":"2024-07-05T14:18:54.009134Z","iopub.status.idle":"2024-07-05T14:18:54.021397Z","shell.execute_reply.started":"2024-07-05T14:18:54.009082Z","shell.execute_reply":"2024-07-05T14:18:54.020422Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"def objective_function(weights, model, X_train, y_train):\n    model.set_weights(weights)\n    predictions = model.predict(X_train)\n    return mean_squared_error(y_train, predictions)","metadata":{"execution":{"iopub.status.busy":"2024-07-05T14:22:57.375978Z","iopub.execute_input":"2024-07-05T14:22:57.376813Z","iopub.status.idle":"2024-07-05T14:22:57.381663Z","shell.execute_reply.started":"2024-07-05T14:22:57.376778Z","shell.execute_reply":"2024-07-05T14:22:57.380560Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"    firefly_optimizer = FireflyAlgorithm(\n        func=lambda weights: objective_function(weights, wnn, trainX, trainY),\n        lb=-1.0,\n        ub=1.0,\n        dim=wnn.count_params(),\n        n_fireflies=20,\n        max_iter=100\n    )","metadata":{"execution":{"iopub.status.busy":"2024-07-05T14:22:59.851442Z","iopub.execute_input":"2024-07-05T14:22:59.852157Z","iopub.status.idle":"2024-07-05T14:22:59.857426Z","shell.execute_reply.started":"2024-07-05T14:22:59.852121Z","shell.execute_reply":"2024-07-05T14:22:59.856605Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"    # Create the Wavelet Neural Network\n    input_shape = trainX.shape[1:]  # Assuming trainX.shape is (496, 2, 18), input_shape should be (2, 18)\n    wnn = Sequential([\n        BatchNormalization(input_shape=input_shape),\n        Dense(64, activation='relu'),\n        Dropout(0.1),\n        Dense(32, activation='relu'),\n        MorletWaveletLayer(10),  # Replace with appropriate units for your use case\n        Dense(trainY.shape[1])  # Adjust output dim to match trainY.shape[1]\n    ])","metadata":{"execution":{"iopub.status.busy":"2024-07-05T14:23:03.590412Z","iopub.execute_input":"2024-07-05T14:23:03.591118Z","iopub.status.idle":"2024-07-05T14:23:03.707465Z","shell.execute_reply.started":"2024-07-05T14:23:03.591084Z","shell.execute_reply":"2024-07-05T14:23:03.706590Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-07-05T14:21:52.531707Z","iopub.execute_input":"2024-07-05T14:21:52.532574Z","iopub.status.idle":"2024-07-05T14:21:52.537319Z","shell.execute_reply.started":"2024-07-05T14:21:52.532541Z","shell.execute_reply":"2024-07-05T14:21:52.536348Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"    # Optimize the model weights using Firefly Algorithm\n    best_weights, best_fitness = firefly_optimizer.optimize()\n    print(best_weights)\n    wnn.set_weights(best_weights)\n\n    # Evaluate the optimized model\n    predictions = wnn.predict(X_test)\n    mse = mean_squared_error(y_test, predictions)\n    print(f'Test Mean Squared Error: {mse}')\n\n    # Plot the results\n    import matplotlib.pyplot as plt\n\n    plt.figure(figsize=(12, 6))\n    plt.plot(y_test.values, label='Actual Effort')\n    plt.plot(predictions, label='Predicted Effort')\n    plt.legend()\n    plt.title('Wavelet Neural Network Predictions with Firefly Algorithm')\n    plt.xlabel('Sample')\n    plt.ylabel('Effort')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-05T14:25:15.257330Z","iopub.execute_input":"2024-07-05T14:25:15.257785Z","iopub.status.idle":"2024-07-05T14:25:15.473828Z","shell.execute_reply.started":"2024-07-05T14:25:15.257748Z","shell.execute_reply":"2024-07-05T14:25:15.472457Z"},"trusted":true},"execution_count":56,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[56], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Optimize the model weights using Firefly Algorithm\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m best_weights, best_fitness \u001b[38;5;241m=\u001b[39m \u001b[43mfirefly_optimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(best_weights)\n\u001b[1;32m      4\u001b[0m wnn\u001b[38;5;241m.\u001b[39mset_weights(best_weights)\n","Cell \u001b[0;32mIn[46], line 16\u001b[0m, in \u001b[0;36mFireflyAlgorithm.optimize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     15\u001b[0m     fireflies \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlb, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mub, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_fireflies, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim))\n\u001b[0;32m---> 16\u001b[0m     fitness \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_along_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfireflies\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     best_firefly \u001b[38;5;241m=\u001b[39m fireflies[np\u001b[38;5;241m.\u001b[39margmin(fitness)]\n\u001b[1;32m     19\u001b[0m     best_fitness \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmin(fitness)\n","File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mapply_along_axis\u001b[0;34m(*args, **kwargs)\u001b[0m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/numpy/lib/shape_base.py:379\u001b[0m, in \u001b[0;36mapply_along_axis\u001b[0;34m(func1d, axis, arr, *args, **kwargs)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    377\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot apply_along_axis when any iteration dimensions are 0\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    378\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 379\u001b[0m res \u001b[38;5;241m=\u001b[39m asanyarray(\u001b[43mfunc1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43minarr_view\u001b[49m\u001b[43m[\u001b[49m\u001b[43mind0\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    381\u001b[0m \u001b[38;5;66;03m# build a buffer for storing evaluations of func1d.\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# remove the requested axis, and add the new ones on the end.\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;66;03m# laid out so that each write is contiguous.\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;66;03m# for a tuple index inds, buff[inds] = func1d(inarr_view[inds])\u001b[39;00m\n\u001b[1;32m    385\u001b[0m buff \u001b[38;5;241m=\u001b[39m zeros(inarr_view\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m res\u001b[38;5;241m.\u001b[39mshape, res\u001b[38;5;241m.\u001b[39mdtype)\n","Cell \u001b[0;32mIn[53], line 2\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(weights)\u001b[0m\n\u001b[1;32m      1\u001b[0m firefly_optimizer \u001b[38;5;241m=\u001b[39m FireflyAlgorithm(\n\u001b[0;32m----> 2\u001b[0m     func\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m weights: \u001b[43mobjective_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainY\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m      3\u001b[0m     lb\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0\u001b[39m,\n\u001b[1;32m      4\u001b[0m     ub\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m,\n\u001b[1;32m      5\u001b[0m     dim\u001b[38;5;241m=\u001b[39mwnn\u001b[38;5;241m.\u001b[39mcount_params(),\n\u001b[1;32m      6\u001b[0m     n_fireflies\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[1;32m      7\u001b[0m     max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[1;32m      8\u001b[0m )\n","Cell \u001b[0;32mIn[52], line 2\u001b[0m, in \u001b[0;36mobjective_function\u001b[0;34m(weights, model, X_train, y_train)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobjective_function\u001b[39m(weights, model, X_train, y_train):\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_train)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mean_squared_error(y_train, predictions)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/engine/base_layer.py:1802\u001b[0m, in \u001b[0;36mLayer.set_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m   1799\u001b[0m         expected_num_weights \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1801\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m expected_num_weights \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(weights):\n\u001b[0;32m-> 1802\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1803\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYou called `set_weights(weights)` on layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1804\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith a weight list of length \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, but the layer was \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1805\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpecting \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m weights. Provided weights: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1806\u001b[0m         \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m   1807\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m   1808\u001b[0m             \u001b[38;5;28mlen\u001b[39m(weights),\n\u001b[1;32m   1809\u001b[0m             expected_num_weights,\n\u001b[1;32m   1810\u001b[0m             \u001b[38;5;28mstr\u001b[39m(weights)[:\u001b[38;5;241m50\u001b[39m],\n\u001b[1;32m   1811\u001b[0m         )\n\u001b[1;32m   1812\u001b[0m     )\n\u001b[1;32m   1814\u001b[0m weight_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1815\u001b[0m weight_value_tuples \u001b[38;5;241m=\u001b[39m []\n","\u001b[0;31mValueError\u001b[0m: You called `set_weights(weights)` on layer \"sequential_25\" with a weight list of length 3699, but the layer was expecting 11 weights. Provided weights: [-0.48388564  0.24313425  0.88920135 ... -0.626771..."],"ename":"ValueError","evalue":"You called `set_weights(weights)` on layer \"sequential_25\" with a weight list of length 3699, but the layer was expecting 11 weights. Provided weights: [-0.48388564  0.24313425  0.88920135 ... -0.626771...","output_type":"error"}]},{"cell_type":"code","source":"if __name__ == \"__main__\":\n\n    # Print the shape of trainX and trainY\n    print('trainX shape == {}.'.format(trainX.shape))\n    print('trainY shape == {}.'.format(trainY.shape))\n\n    # Create the Wavelet Neural Network\n    input_shape = trainX.shape[1:]  # Assuming trainX.shape is (496, 2, 18), input_shape should be (2, 18)\n    wnn = Sequential([\n        BatchNormalization(input_shape=input_shape),\n        Dense(64, activation='relu'),\n        Dropout(0.1),\n        Dense(32, activation='relu'),\n        MorletWaveletLayer(10),  # Replace with appropriate units for your use case\n        Dense(trainY.shape[1])  # Adjust output dim to match trainY.shape[1]\n    ])\n\n    # Compile the model\n    wnn.compile(optimizer='adam', loss='mse')\n    wnn.summary()\n\n    # Train the model\n    history = wnn.fit(trainX, trainY, epochs=100, batch_size=32, validation_split=0.25, verbose=1)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-05T14:15:42.215538Z","iopub.execute_input":"2024-07-05T14:15:42.216565Z","iopub.status.idle":"2024-07-05T14:15:52.882481Z","shell.execute_reply.started":"2024-07-05T14:15:42.216521Z","shell.execute_reply":"2024-07-05T14:15:52.881480Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"trainX shape == (400, 2, 18).\ntrainY shape == (400, 1).\nModel: \"sequential_23\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n batch_normalization_16 (Bat  (None, 2, 18)            72        \n chNormalization)                                                \n                                                                 \n dense_36 (Dense)            (None, 2, 64)             1216      \n                                                                 \n dropout_12 (Dropout)        (None, 2, 64)             0         \n                                                                 \n dense_37 (Dense)            (None, 2, 32)             2080      \n                                                                 \n morlet_wavelet_layer_18 (Mo  (None, 2, 10)            320       \n rletWaveletLayer)                                               \n                                                                 \n dense_38 (Dense)            (None, 2, 1)              11        \n                                                                 \n=================================================================\nTotal params: 3,699\nTrainable params: 3,663\nNon-trainable params: 36\n_________________________________________________________________\nEpoch 1/100\n10/10 [==============================] - 2s 32ms/step - loss: 0.3138 - val_loss: 0.0951\nEpoch 2/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.1943 - val_loss: 0.0752\nEpoch 3/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.1428 - val_loss: 0.0553\nEpoch 4/100\n10/10 [==============================] - 0s 9ms/step - loss: 0.1280 - val_loss: 0.0471\nEpoch 5/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.1185 - val_loss: 0.0401\nEpoch 6/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.1070 - val_loss: 0.0340\nEpoch 7/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.1015 - val_loss: 0.0297\nEpoch 8/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0899 - val_loss: 0.0266\nEpoch 9/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0833 - val_loss: 0.0244\nEpoch 10/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0881 - val_loss: 0.0241\nEpoch 11/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0852 - val_loss: 0.0251\nEpoch 12/100\n10/10 [==============================] - 0s 9ms/step - loss: 0.0856 - val_loss: 0.0256\nEpoch 13/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0618 - val_loss: 0.0248\nEpoch 14/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0629 - val_loss: 0.0255\nEpoch 15/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0656 - val_loss: 0.0238\nEpoch 16/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0577 - val_loss: 0.0235\nEpoch 17/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0633 - val_loss: 0.0280\nEpoch 18/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0551 - val_loss: 0.0322\nEpoch 19/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0520 - val_loss: 0.0291\nEpoch 20/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0589 - val_loss: 0.0220\nEpoch 21/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0481 - val_loss: 0.0241\nEpoch 22/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0434 - val_loss: 0.0265\nEpoch 23/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0467 - val_loss: 0.0220\nEpoch 24/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0449 - val_loss: 0.0213\nEpoch 25/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0440 - val_loss: 0.0253\nEpoch 26/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0374 - val_loss: 0.0297\nEpoch 27/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0415 - val_loss: 0.0228\nEpoch 28/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0356 - val_loss: 0.0205\nEpoch 29/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0416 - val_loss: 0.0205\nEpoch 30/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0398 - val_loss: 0.0209\nEpoch 31/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0349 - val_loss: 0.0196\nEpoch 32/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0343 - val_loss: 0.0187\nEpoch 33/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0350 - val_loss: 0.0202\nEpoch 34/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0337 - val_loss: 0.0219\nEpoch 35/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0344 - val_loss: 0.0202\nEpoch 36/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0332 - val_loss: 0.0202\nEpoch 37/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0279 - val_loss: 0.0220\nEpoch 38/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0287 - val_loss: 0.0260\nEpoch 39/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0324 - val_loss: 0.0257\nEpoch 40/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0264 - val_loss: 0.0271\nEpoch 41/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0257 - val_loss: 0.0293\nEpoch 42/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0258 - val_loss: 0.0323\nEpoch 43/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0232 - val_loss: 0.0298\nEpoch 44/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0253 - val_loss: 0.0266\nEpoch 45/100\n10/10 [==============================] - 0s 9ms/step - loss: 0.0229 - val_loss: 0.0259\nEpoch 46/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0215 - val_loss: 0.0255\nEpoch 47/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0230 - val_loss: 0.0268\nEpoch 48/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0239 - val_loss: 0.0287\nEpoch 49/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0219 - val_loss: 0.0249\nEpoch 50/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0226 - val_loss: 0.0226\nEpoch 51/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0213 - val_loss: 0.0197\nEpoch 52/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0215 - val_loss: 0.0199\nEpoch 53/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0204 - val_loss: 0.0200\nEpoch 54/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0217 - val_loss: 0.0201\nEpoch 55/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0211 - val_loss: 0.0189\nEpoch 56/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0201 - val_loss: 0.0198\nEpoch 57/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0194 - val_loss: 0.0201\nEpoch 58/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0188 - val_loss: 0.0195\nEpoch 59/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0218 - val_loss: 0.0208\nEpoch 60/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0179 - val_loss: 0.0235\nEpoch 61/100\n10/10 [==============================] - 0s 9ms/step - loss: 0.0211 - val_loss: 0.0251\nEpoch 62/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0188 - val_loss: 0.0235\nEpoch 63/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0187 - val_loss: 0.0241\nEpoch 64/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0198 - val_loss: 0.0250\nEpoch 65/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0194 - val_loss: 0.0253\nEpoch 66/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0185 - val_loss: 0.0235\nEpoch 67/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0160 - val_loss: 0.0228\nEpoch 68/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0170 - val_loss: 0.0216\nEpoch 69/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0171 - val_loss: 0.0215\nEpoch 70/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0176 - val_loss: 0.0212\nEpoch 71/100\n10/10 [==============================] - 0s 9ms/step - loss: 0.0157 - val_loss: 0.0215\nEpoch 72/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0179 - val_loss: 0.0211\nEpoch 73/100\n10/10 [==============================] - 0s 11ms/step - loss: 0.0152 - val_loss: 0.0211\nEpoch 74/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0185 - val_loss: 0.0211\nEpoch 75/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0144 - val_loss: 0.0210\nEpoch 76/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0175 - val_loss: 0.0217\nEpoch 77/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0177 - val_loss: 0.0204\nEpoch 78/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0144 - val_loss: 0.0198\nEpoch 79/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0166 - val_loss: 0.0212\nEpoch 80/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0151 - val_loss: 0.0225\nEpoch 81/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0170 - val_loss: 0.0207\nEpoch 82/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0149 - val_loss: 0.0196\nEpoch 83/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0161 - val_loss: 0.0206\nEpoch 84/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0142 - val_loss: 0.0212\nEpoch 85/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0140 - val_loss: 0.0215\nEpoch 86/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0160 - val_loss: 0.0229\nEpoch 87/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0155 - val_loss: 0.0233\nEpoch 88/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0143 - val_loss: 0.0229\nEpoch 89/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0148 - val_loss: 0.0212\nEpoch 90/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0149 - val_loss: 0.0210\nEpoch 91/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0163 - val_loss: 0.0224\nEpoch 92/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0165 - val_loss: 0.0206\nEpoch 93/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0142 - val_loss: 0.0215\nEpoch 94/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0140 - val_loss: 0.0226\nEpoch 95/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0140 - val_loss: 0.0237\nEpoch 96/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0152 - val_loss: 0.0228\nEpoch 97/100\n10/10 [==============================] - 0s 9ms/step - loss: 0.0152 - val_loss: 0.0226\nEpoch 98/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0131 - val_loss: 0.0218\nEpoch 99/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0136 - val_loss: 0.0228\nEpoch 100/100\n10/10 [==============================] - 0s 8ms/step - loss: 0.0129 - val_loss: 0.0243\n","output_type":"stream"}]},{"cell_type":"code","source":"    # Plot training history\n    plt.plot(history.history['loss'], label='Training loss')\n    plt.plot(history.history['val_loss'], label='Validation loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-05T14:15:57.199800Z","iopub.execute_input":"2024-07-05T14:15:57.200155Z","iopub.status.idle":"2024-07-05T14:15:57.476505Z","shell.execute_reply.started":"2024-07-05T14:15:57.200125Z","shell.execute_reply":"2024-07-05T14:15:57.475484Z"},"trusted":true},"execution_count":45,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score","metadata":{"execution":{"iopub.status.busy":"2024-07-05T14:09:14.423848Z","iopub.execute_input":"2024-07-05T14:09:14.424698Z","iopub.status.idle":"2024-07-05T14:09:14.428909Z","shell.execute_reply.started":"2024-07-05T14:09:14.424666Z","shell.execute_reply":"2024-07-05T14:09:14.428031Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"def mean_magnitude_relative_error(y_true, y_pred):\n    # Calculate relative error\n    relative_error = np.abs((y_true - y_pred) / y_true)\n    \n    # Calculate mean magnitude of relative error (MMRE)\n    mmre = np.mean(relative_error)\n    \n    return mmre\n\nif __name__ == \"__main__\":\n    # Load or generate your data\n    X, y = data_china()\n\n    # Assuming trainX and trainY are defined appropriately\n    trainX = X[:400]  # Example: using first 400 samples for training\n    trainY = y[:400]  # Example: using first 400 targets for training\n    testX = X[400:]   # Example: using remaining samples for testing\n    testY = y[400:]   # Example: using remaining targets for testing\n\n    # Create the Wavelet Neural Network\n    input_shape = trainX.shape[1:]  # Assuming trainX.shape is (496, 2, 18), input_shape should be (2, 18)\n    wnn = Sequential([\n        BatchNormalization(input_shape=input_shape),\n        Dense(64, activation='relu'),\n        Dropout(0.1),\n        Dense(32, activation='relu'),\n        MorletWaveletLayer(10),  # Replace with appropriate units for your use case\n        Dense(trainY.shape[1])  # Adjust output dim to match trainY.shape[1]\n    ])\n\n    # Compile the model\n    wnn.compile(optimizer='adam', loss='mse')\n    wnn.summary()\n\n    # Train the model\n    history = wnn.fit(trainX, trainY, epochs=50, batch_size=32, validation_split=0.1, verbose=1)\n\n    # Evaluate the model on test data\n    predictions = wnn.predict(testX)\n\n    # Calculate evaluation metrics\n    mae = mean_absolute_error(testY, predictions)\n    mse = mean_squared_error(testY, predictions)\n    r2 = r2_score(testY, predictions)\n    mmre = mean_magnitude_relative_error(testY, predictions)\n\n    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n    print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n    print(f\"R-squared (R2) Score: {r2:.4f}\")\n    print(f\"Mean Magnitude of Relative Error (MMRE): {mmre:.4f}\")\n\n    # Plot training history\n    plt.plot(history.history['loss'], label='Training loss')\n    plt.plot(history.history['val_loss'], label='Validation loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-05T14:09:24.091060Z","iopub.execute_input":"2024-07-05T14:09:24.091848Z","iopub.status.idle":"2024-07-05T14:09:31.381000Z","shell.execute_reply.started":"2024-07-05T14:09:24.091812Z","shell.execute_reply":"2024-07-05T14:09:31.379767Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"Model: \"sequential_21\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n batch_normalization_14 (Bat  (None, 2, 18)            72        \n chNormalization)                                                \n                                                                 \n dense_30 (Dense)            (None, 2, 64)             1216      \n                                                                 \n dropout_10 (Dropout)        (None, 2, 64)             0         \n                                                                 \n dense_31 (Dense)            (None, 2, 32)             2080      \n                                                                 \n morlet_wavelet_layer_16 (Mo  (None, 2, 10)            320       \n rletWaveletLayer)                                               \n                                                                 \n dense_32 (Dense)            (None, 2, 1)              11        \n                                                                 \n=================================================================\nTotal params: 3,699\nTrainable params: 3,663\nNon-trainable params: 36\n_________________________________________________________________\nEpoch 1/50\n12/12 [==============================] - 2s 26ms/step - loss: 1.9574 - val_loss: 5.2169\nEpoch 2/50\n12/12 [==============================] - 0s 8ms/step - loss: 0.3515 - val_loss: 3.4320\nEpoch 3/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.2989 - val_loss: 3.2132\nEpoch 4/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.2254 - val_loss: 3.1398\nEpoch 5/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.2014 - val_loss: 2.7803\nEpoch 6/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.1621 - val_loss: 2.5174\nEpoch 7/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.1503 - val_loss: 2.4134\nEpoch 8/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.1502 - val_loss: 2.2279\nEpoch 9/50\n12/12 [==============================] - 0s 8ms/step - loss: 0.1470 - val_loss: 2.0357\nEpoch 10/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.1446 - val_loss: 1.8222\nEpoch 11/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.1169 - val_loss: 1.5721\nEpoch 12/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.1122 - val_loss: 1.4149\nEpoch 13/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.1040 - val_loss: 1.3378\nEpoch 14/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.1184 - val_loss: 1.1791\nEpoch 15/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.1110 - val_loss: 1.0211\nEpoch 16/50\n12/12 [==============================] - 0s 8ms/step - loss: 0.0902 - val_loss: 0.9027\nEpoch 17/50\n12/12 [==============================] - 0s 8ms/step - loss: 0.0994 - val_loss: 0.8242\nEpoch 18/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.0955 - val_loss: 0.7465\nEpoch 19/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.1029 - val_loss: 0.5683\nEpoch 20/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.0785 - val_loss: 0.5133\nEpoch 21/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.0864 - val_loss: 0.4684\nEpoch 22/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.0801 - val_loss: 0.3991\nEpoch 23/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.0859 - val_loss: 0.3309\nEpoch 24/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.0809 - val_loss: 0.3357\nEpoch 25/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.0654 - val_loss: 0.2780\nEpoch 26/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.0660 - val_loss: 0.2634\nEpoch 27/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.0734 - val_loss: 0.2242\nEpoch 28/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.0753 - val_loss: 0.1879\nEpoch 29/50\n12/12 [==============================] - 0s 8ms/step - loss: 0.0689 - val_loss: 0.1515\nEpoch 30/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.0650 - val_loss: 0.1577\nEpoch 31/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.0644 - val_loss: 0.1275\nEpoch 32/50\n12/12 [==============================] - 0s 8ms/step - loss: 0.0530 - val_loss: 0.1132\nEpoch 33/50\n12/12 [==============================] - 0s 8ms/step - loss: 0.0561 - val_loss: 0.1140\nEpoch 34/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.0522 - val_loss: 0.1027\nEpoch 35/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.0550 - val_loss: 0.0935\nEpoch 36/50\n12/12 [==============================] - 0s 8ms/step - loss: 0.0477 - val_loss: 0.0826\nEpoch 37/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.0529 - val_loss: 0.0802\nEpoch 38/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.0587 - val_loss: 0.0821\nEpoch 39/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.0522 - val_loss: 0.0885\nEpoch 40/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.0524 - val_loss: 0.0857\nEpoch 41/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.0496 - val_loss: 0.0784\nEpoch 42/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.0470 - val_loss: 0.0712\nEpoch 43/50\n12/12 [==============================] - 0s 8ms/step - loss: 0.0396 - val_loss: 0.0733\nEpoch 44/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.0470 - val_loss: 0.0709\nEpoch 45/50\n12/12 [==============================] - 0s 10ms/step - loss: 0.0432 - val_loss: 0.0736\nEpoch 46/50\n12/12 [==============================] - 0s 8ms/step - loss: 0.0388 - val_loss: 0.0684\nEpoch 47/50\n12/12 [==============================] - 0s 10ms/step - loss: 0.0400 - val_loss: 0.0611\nEpoch 48/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.0420 - val_loss: 0.0677\nEpoch 49/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.0412 - val_loss: 0.0605\nEpoch 50/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.0460 - val_loss: 0.0592\n3/3 [==============================] - 0s 3ms/step\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[40], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m predictions \u001b[38;5;241m=\u001b[39m wnn\u001b[38;5;241m.\u001b[39mpredict(testX)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Calculate evaluation metrics\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m mae \u001b[38;5;241m=\u001b[39m \u001b[43mmean_absolute_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtestY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m mse \u001b[38;5;241m=\u001b[39m mean_squared_error(testY, predictions)\n\u001b[1;32m     44\u001b[0m r2 \u001b[38;5;241m=\u001b[39m r2_score(testY, predictions)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_regression.py:196\u001b[0m, in \u001b[0;36mmean_absolute_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmean_absolute_error\u001b[39m(\n\u001b[1;32m    142\u001b[0m     y_true, y_pred, \u001b[38;5;241m*\u001b[39m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, multioutput\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muniform_average\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    143\u001b[0m ):\n\u001b[1;32m    144\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Mean absolute error regression loss.\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \n\u001b[1;32m    146\u001b[0m \u001b[38;5;124;03m    Read more in the :ref:`User Guide <mean_absolute_error>`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;03m    0.85...\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     y_type, y_true, y_pred, multioutput \u001b[38;5;241m=\u001b[39m \u001b[43m_check_reg_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m     check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    200\u001b[0m     output_errors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39maverage(np\u001b[38;5;241m.\u001b[39mabs(y_pred \u001b[38;5;241m-\u001b[39m y_true), weights\u001b[38;5;241m=\u001b[39msample_weight, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_regression.py:102\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[1;32m    101\u001b[0m y_true \u001b[38;5;241m=\u001b[39m check_array(y_true, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m--> 102\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_true\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    105\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m y_true\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:915\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    912\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    913\u001b[0m     )\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nd \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m--> 915\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    916\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    917\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    918\u001b[0m     )\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m    921\u001b[0m     _assert_all_finite(\n\u001b[1;32m    922\u001b[0m         array,\n\u001b[1;32m    923\u001b[0m         input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[1;32m    924\u001b[0m         estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[1;32m    925\u001b[0m         allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    926\u001b[0m     )\n","\u001b[0;31mValueError\u001b[0m: Found array with dim 3. None expected <= 2."],"ename":"ValueError","evalue":"Found array with dim 3. None expected <= 2.","output_type":"error"}]},{"cell_type":"code","source":"# Assuming testX and testY are defined appropriately\ntestX = X[400:]   # Example: using remaining samples for testing\ntestY = y[400:]   # Example: using remaining targets for testing\n\n# Evaluate the model on test data\npredictions = wnn.predict(testX)\n\n# Reshape predictions and testY if necessary\nif predictions.ndim > 2:\n    predictions = np.squeeze(predictions)\nif testY.ndim > 2:\n    testY = np.squeeze(testY)\n\n# Calculate evaluation metrics\nmae = np.mean(np.abs(predictions - testY))\nmse = np.mean((predictions - testY)**2)\nr2 = r2_score(testY, predictions)\nmmre = mean_magnitude_relative_error(testY, predictions)\n\nprint(f\"Mean Absolute Error (MAE): {mae:.4f}\")\nprint(f\"Mean Squared Error (MSE): {mse:.4f}\")\nprint(f\"R-squared (R2) Score: {r2:.4f}\")\nprint(f\"Mean Magnitude of Relative Error (MMRE): {mmre:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-05T14:11:47.582532Z","iopub.execute_input":"2024-07-05T14:11:47.582945Z","iopub.status.idle":"2024-07-05T14:11:47.731687Z","shell.execute_reply.started":"2024-07-05T14:11:47.582914Z","shell.execute_reply":"2024-07-05T14:11:47.730279Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"3/3 [==============================] - 0s 3ms/step\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[41], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m mae \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39mabs(predictions \u001b[38;5;241m-\u001b[39m testY))\n\u001b[1;32m     16\u001b[0m mse \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean((predictions \u001b[38;5;241m-\u001b[39m testY)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m r2 \u001b[38;5;241m=\u001b[39m \u001b[43mr2_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtestY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m mmre \u001b[38;5;241m=\u001b[39m mean_magnitude_relative_error(testY, predictions)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean Absolute Error (MAE): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmae\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_regression.py:911\u001b[0m, in \u001b[0;36mr2_score\u001b[0;34m(y_true, y_pred, sample_weight, multioutput, force_finite)\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mr2_score\u001b[39m(\n\u001b[1;32m    785\u001b[0m     y_true,\n\u001b[1;32m    786\u001b[0m     y_pred,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m     force_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    791\u001b[0m ):\n\u001b[1;32m    792\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\":math:`R^2` (coefficient of determination) regression score function.\u001b[39;00m\n\u001b[1;32m    793\u001b[0m \n\u001b[1;32m    794\u001b[0m \u001b[38;5;124;03m    Best possible score is 1.0 and it can be negative (because the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[38;5;124;03m    -inf\u001b[39;00m\n\u001b[1;32m    910\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 911\u001b[0m     y_type, y_true, y_pred, multioutput \u001b[38;5;241m=\u001b[39m \u001b[43m_check_reg_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    912\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\n\u001b[1;32m    913\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    914\u001b[0m     check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    916\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _num_samples(y_pred) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_regression.py:111\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[1;32m    108\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m y_pred\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_true\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m y_pred\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m--> 111\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    112\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true and y_pred have different number of output (\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m!=\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    113\u001b[0m             y_true\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], y_pred\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    114\u001b[0m         )\n\u001b[1;32m    115\u001b[0m     )\n\u001b[1;32m    117\u001b[0m n_outputs \u001b[38;5;241m=\u001b[39m y_true\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    118\u001b[0m allowed_multioutput_str \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_values\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muniform_average\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvariance_weighted\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mValueError\u001b[0m: y_true and y_pred have different number of output (1!=2)"],"ename":"ValueError","evalue":"y_true and y_pred have different number of output (1!=2)","output_type":"error"}]},{"cell_type":"markdown","source":"# LSTM Model","metadata":{}},{"cell_type":"code","source":"trainX, trainY = data_china()\n\n\nprint('trainX shape == {}.'.format(trainX.shape))\nprint('trainY shape == {}.'.format(trainY.shape))\n\n# define the Autoencoder model\n\nmodel = Sequential()\nmodel.add(BatchNormalization(input_shape=(trainX.shape[1], trainX.shape[2])))\nmodel.add(Bidirectional(LSTM(64, activation='relu', return_sequences=True)))\nmodel.add(LSTM(32, activation='relu', return_sequences=False))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(trainY.shape[1]))\n\nmodel.compile(optimizer='sgd', loss='mse')\nmodel.summary()\n\n\n# fit the model\nhistory = model.fit(trainX, trainY, epochs=50, batch_size=32, validation_split=0.1, verbose=1)\n\nplt.plot(history.history['loss'], label='Training loss')\nplt.plot(history.history['val_loss'], label='Validation loss')\n","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:28:29.569473Z","iopub.execute_input":"2024-07-05T13:28:29.570182Z","iopub.status.idle":"2024-07-05T13:28:55.714705Z","shell.execute_reply.started":"2024-07-05T13:28:29.570135Z","shell.execute_reply":"2024-07-05T13:28:55.713720Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"trainX shape == (496, 2, 18).\ntrainY shape == (496, 1).\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n batch_normalization (BatchN  (None, 2, 18)            72        \n ormalization)                                                   \n                                                                 \n bidirectional (Bidirectiona  (None, 2, 128)           42496     \n l)                                                              \n                                                                 \n lstm_1 (LSTM)               (None, 32)                20608     \n                                                                 \n dropout (Dropout)           (None, 32)                0         \n                                                                 \n dense (Dense)               (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 63,209\nTrainable params: 63,173\nNon-trainable params: 36\n_________________________________________________________________\nEpoch 1/50\n14/14 [==============================] - 6s 59ms/step - loss: 0.0169 - val_loss: 0.0143\nEpoch 2/50\n14/14 [==============================] - 0s 20ms/step - loss: 0.0159 - val_loss: 0.0133\nEpoch 3/50\n14/14 [==============================] - 0s 20ms/step - loss: 0.0152 - val_loss: 0.0127\nEpoch 4/50\n14/14 [==============================] - 0s 20ms/step - loss: 0.0151 - val_loss: 0.0123\nEpoch 5/50\n14/14 [==============================] - 0s 20ms/step - loss: 0.0151 - val_loss: 0.0121\nEpoch 6/50\n14/14 [==============================] - 0s 21ms/step - loss: 0.0150 - val_loss: 0.0120\nEpoch 7/50\n14/14 [==============================] - 0s 20ms/step - loss: 0.0146 - val_loss: 0.0119\nEpoch 8/50\n14/14 [==============================] - 0s 20ms/step - loss: 0.0147 - val_loss: 0.0118\nEpoch 9/50\n14/14 [==============================] - 0s 23ms/step - loss: 0.0146 - val_loss: 0.0118\nEpoch 10/50\n14/14 [==============================] - 0s 25ms/step - loss: 0.0146 - val_loss: 0.0118\nEpoch 11/50\n14/14 [==============================] - 0s 24ms/step - loss: 0.0149 - val_loss: 0.0117\nEpoch 12/50\n14/14 [==============================] - 0s 27ms/step - loss: 0.0149 - val_loss: 0.0117\nEpoch 13/50\n14/14 [==============================] - 0s 24ms/step - loss: 0.0146 - val_loss: 0.0117\nEpoch 14/50\n14/14 [==============================] - 0s 21ms/step - loss: 0.0148 - val_loss: 0.0117\nEpoch 15/50\n14/14 [==============================] - 0s 20ms/step - loss: 0.0146 - val_loss: 0.0117\nEpoch 16/50\n14/14 [==============================] - 0s 21ms/step - loss: 0.0148 - val_loss: 0.0117\nEpoch 17/50\n14/14 [==============================] - 0s 21ms/step - loss: 0.0146 - val_loss: 0.0117\nEpoch 18/50\n14/14 [==============================] - 0s 21ms/step - loss: 0.0147 - val_loss: 0.0117\nEpoch 19/50\n14/14 [==============================] - 0s 21ms/step - loss: 0.0148 - val_loss: 0.0117\nEpoch 20/50\n14/14 [==============================] - 0s 20ms/step - loss: 0.0146 - val_loss: 0.0117\nEpoch 21/50\n14/14 [==============================] - 0s 21ms/step - loss: 0.0147 - val_loss: 0.0117\nEpoch 22/50\n14/14 [==============================] - 0s 23ms/step - loss: 0.0146 - val_loss: 0.0117\nEpoch 23/50\n14/14 [==============================] - 0s 21ms/step - loss: 0.0145 - val_loss: 0.0117\nEpoch 24/50\n14/14 [==============================] - 0s 22ms/step - loss: 0.0147 - val_loss: 0.0117\nEpoch 25/50\n14/14 [==============================] - 0s 21ms/step - loss: 0.0146 - val_loss: 0.0116\nEpoch 26/50\n14/14 [==============================] - 0s 20ms/step - loss: 0.0146 - val_loss: 0.0116\nEpoch 27/50\n14/14 [==============================] - 0s 22ms/step - loss: 0.0145 - val_loss: 0.0116\nEpoch 28/50\n14/14 [==============================] - 0s 20ms/step - loss: 0.0147 - val_loss: 0.0116\nEpoch 29/50\n14/14 [==============================] - 0s 20ms/step - loss: 0.0147 - val_loss: 0.0116\nEpoch 30/50\n14/14 [==============================] - 0s 21ms/step - loss: 0.0145 - val_loss: 0.0116\nEpoch 31/50\n14/14 [==============================] - 0s 20ms/step - loss: 0.0145 - val_loss: 0.0116\nEpoch 32/50\n14/14 [==============================] - 0s 21ms/step - loss: 0.0147 - val_loss: 0.0116\nEpoch 33/50\n14/14 [==============================] - 0s 21ms/step - loss: 0.0146 - val_loss: 0.0116\nEpoch 34/50\n14/14 [==============================] - 0s 21ms/step - loss: 0.0145 - val_loss: 0.0116\nEpoch 35/50\n14/14 [==============================] - 0s 20ms/step - loss: 0.0146 - val_loss: 0.0116\nEpoch 36/50\n14/14 [==============================] - 0s 20ms/step - loss: 0.0145 - val_loss: 0.0116\nEpoch 37/50\n14/14 [==============================] - 0s 21ms/step - loss: 0.0146 - val_loss: 0.0116\nEpoch 38/50\n14/14 [==============================] - 0s 21ms/step - loss: 0.0147 - val_loss: 0.0116\nEpoch 39/50\n14/14 [==============================] - 0s 21ms/step - loss: 0.0146 - val_loss: 0.0116\nEpoch 40/50\n14/14 [==============================] - 0s 20ms/step - loss: 0.0146 - val_loss: 0.0116\nEpoch 41/50\n14/14 [==============================] - 0s 20ms/step - loss: 0.0146 - val_loss: 0.0116\nEpoch 42/50\n14/14 [==============================] - 0s 20ms/step - loss: 0.0144 - val_loss: 0.0116\nEpoch 43/50\n14/14 [==============================] - 0s 21ms/step - loss: 0.0145 - val_loss: 0.0116\nEpoch 44/50\n14/14 [==============================] - 0s 19ms/step - loss: 0.0146 - val_loss: 0.0116\nEpoch 45/50\n14/14 [==============================] - 0s 21ms/step - loss: 0.0145 - val_loss: 0.0116\nEpoch 46/50\n14/14 [==============================] - 0s 23ms/step - loss: 0.0146 - val_loss: 0.0116\nEpoch 47/50\n14/14 [==============================] - 0s 19ms/step - loss: 0.0146 - val_loss: 0.0116\nEpoch 48/50\n14/14 [==============================] - 0s 21ms/step - loss: 0.0145 - val_loss: 0.0116\nEpoch 49/50\n14/14 [==============================] - 0s 20ms/step - loss: 0.0145 - val_loss: 0.0116\nEpoch 50/50\n14/14 [==============================] - 0s 21ms/step - loss: 0.0145 - val_loss: 0.0116\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"[<matplotlib.lines.Line2D at 0x7b0c23f7dde0>]"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"def root_mean_squared_error(y_true, y_pred):\n    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n","metadata":{"execution":{"iopub.status.busy":"2023-12-03T18:40:48.855939Z","iopub.execute_input":"2023-12-03T18:40:48.85649Z","iopub.status.idle":"2023-12-03T18:40:48.86059Z","shell.execute_reply.started":"2023-12-03T18:40:48.856458Z","shell.execute_reply":"2023-12-03T18:40:48.859544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# trainY.shape[1]","metadata":{"execution":{"iopub.status.busy":"2023-12-03T18:40:48.862382Z","iopub.execute_input":"2023-12-03T18:40:48.863037Z","iopub.status.idle":"2023-12-03T18:40:48.871846Z","shell.execute_reply.started":"2023-12-03T18:40:48.86301Z","shell.execute_reply":"2023-12-03T18:40:48.870747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainX, trainY = data_china()\n# def create_and_train_autoencoder(trainX, trainY, epochs, batch_size, validation_split, \n#                                  learning_rate, dropout_rate, activation, \n#                                  optimizer, loss):\n    \n#     model = Sequential()\n#     model.add(BatchNormalization(input_shape=(trainX.shape[1], trainX.shape[2])))\n#     model.add(Bidirectional(LSTM(64, activation=activation, return_sequences=True)))\n#     model.add(LSTM(32, activation=activation, return_sequences=False))\n#     model.add(Dropout(dropout_rate))\n#     model.add(Dense(trainY.shape[1]))\n\n#     model.compile(optimizer=optimizer, loss=loss)\n#     model.summary()\n\n\n#     # fit the model\n#     history = model.fit(trainX, trainY, epochs=epochs, batch_size=batch_size, validation_split=validation_split, verbose=1)\n\n#     plt.plot(history.history['loss'], label='Training loss')\n#     plt.plot(history.history['val_loss'], label='Validation loss')\n\n#     return model, history\n\ndef create_and_train_autoencoder(trainX, trainY, epochs, batch_size, validation_split, \n                                 learning_rate, dropout_rate, lstm_units, activation, \n                                 optimizer, loss):\n    \n    model = Sequential()\n    model.add(BatchNormalization(input_shape=(trainX.shape[1], trainX.shape[2])))\n    model.add(Bidirectional(LSTM(lstm_units, activation=activation, return_sequences=True)))\n    model.add(LSTM(lstm_units // 2, activation=activation, return_sequences=True))\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(trainY.shape[1]))\n\n\n\n    \n    model.compile(optimizer=optimizer, loss=loss)\n    model.summary()\n\n    # Define early stopping\n    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n\n    # fit the model with early stopping\n    history = model.fit(trainX, trainY, epochs=epochs, batch_size=batch_size, \n                        validation_split=validation_split, verbose=1, callbacks=[early_stopping])\n   \n    plt.plot(history.history['loss'], label='Training loss')\n    plt.plot(history.history['val_loss'], label='Validation loss')\n\n    return model, history\n\n\n# Example usage:\n# Adjust hyperparameters as needed\n# autoencoder_model, training_history = create_and_train_autoencoder(trainX, trainY, \n#                                                                     epochs=500, batch_size=32, \n#                                                                     validation_split=0.1, \n#                                                                     learning_rate=best_hyperparameters[0], \n#                                                                     dropout_rate=best_hyperparameters[1], \n#                                                                     lstm_units=int(best_hyperparameters[2]),\n#                                                                     activation='relu', \n#                                                                     optimizer='adam', \n#                                                                     loss='mse')\n\nautoencoder_model, training_history = create_and_train_autoencoder(trainX, trainY, \n                                                                    epochs=100, batch_size=16, \n                                                                    validation_split=0.1, \n                                                                    learning_rate=0.01, \n                                                                    dropout_rate=0.1,\n                                                                    lstm_units= 64,\n                                                                    activation='sigmoid', \n                                                                    optimizer='adam', \n                                                                    loss= 'mae',\n                                                                    \n                                                                  )","metadata":{"execution":{"iopub.status.busy":"2023-12-03T18:40:48.873077Z","iopub.execute_input":"2023-12-03T18:40:48.873343Z","iopub.status.idle":"2023-12-03T18:40:56.321274Z","shell.execute_reply.started":"2023-12-03T18:40:48.873322Z","shell.execute_reply":"2023-12-03T18:40:56.320228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\ndef harmonic_search(objective_function, search_space, num_iterations):\n    # Initialize parameters\n    num_harmonics = 10\n    pitch_adjustment_rate = 0.01\n\n    # Initialize random solutions within the search space\n    solutions = np.random.uniform(low=search_space[:, 0], high=search_space[:, 1], size=(num_harmonics, len(search_space)))\n\n    for iteration in range(num_iterations):\n        # Evaluate the performance of each solution\n        scores = [objective_function(solution) for solution in solutions]\n\n        # Select the top-performing solutions as parents\n        parents = solutions[np.argsort(scores)[:2]]\n\n        # Generate new candidate solutions by combining and modifying parents\n        new_solutions = parents[0] + np.random.uniform(low=-pitch_adjustment_rate, high=pitch_adjustment_rate, size=parents.shape)\n\n        # Clip new solutions to the search space\n        new_solutions = np.clip(new_solutions, search_space[:, 0], search_space[:, 1])\n\n        # Replace the worst solutions with the new ones\n        worst_index = np.argmax(scores)\n        solutions[worst_index] = new_solutions[0]  # Take the first parent as the new solution\n\n    # Return the best solution found\n    best_solution = solutions[np.argmin(scores)]\n    return best_solution\n\n# Example usage:\n# Define the search space for hyperparameters\nsearch_space = np.array([\n    [0.001, 0.1],  # Learning Rate\n    [0.1, 0.9],    # Dropout Rate\n    [16, 64],     # Number of LSTM units\n])\n\n# Define your objective function (replace with your actual training and evaluation logic)\ndef objective_function(hyperparameters):\n    learning_rate, dropout_rate, lstm_units = hyperparameters\n    \n    try:\n        # Create and train LSTM model with the given hyperparameters\n        # Return the performance metric to be minimized (e.g., validation loss)\n        autoencoder_model, training_history = create_and_train_autoencoder(trainX, trainY, \n                                                                            epochs=100, batch_size=16, \n                                                                            validation_split=0.1, \n                                                                            learning_rate=learning_rate, \n                                                                            dropout_rate=dropout_rate, \n                                                                            lstm_units=int(lstm_units),\n                                                                            activation='sigmoid', \n                                                                            optimizer='adam')\n        \n        # Retrieve the performance metric (e.g., validation loss) from the training history\n        metric = min(training_history.history['val_loss'])  # Assuming 'val_loss' is the relevant metric\n\n        return metric\n    except Exception as e:\n        # Return a large value in case of an error\n        return float('inf')\n\n# Run harmonic search\nbest_hyperparameters = harmonic_search(objective_function, search_space, num_iterations=50)\n\n# Print or log the best hyperparameters\nprint(\"Best Hyperparameters:\")\nprint(\"Learning Rate:\", best_hyperparameters[0])\nprint(\"Dropout Rate:\", best_hyperparameters[1])\nprint(\"LSTM Units:\", int(best_hyperparameters[2]))\n\n# Update your LSTM model with the best hyperparameters\nautoencoder_model, training_history = create_and_train_autoencoder(trainX, trainY, \n                                                                    epochs=100, batch_size=16, \n                                                                    validation_split=0.1, \n                                                                    learning_rate=best_hyperparameters[0], \n                                                                    dropout_rate=best_hyperparameters[1], \n                                                                    lstm_units=int(best_hyperparameters[2]),\n                                                                    activation='sigmoid', \n                                                                    optimizer='adam', \n                                                                    loss='mae',\n                                                                    \n                                                                  )\n\n# Optionally, you can also print or log other relevant information, such as the best performance metric\nbest_metric = objective_function(best_hyperparameters)\nprint(\"Best Performance Metric:\", best_metric)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-03T18:40:56.324666Z","iopub.execute_input":"2023-12-03T18:40:56.325943Z","iopub.status.idle":"2023-12-03T18:41:08.408575Z","shell.execute_reply.started":"2023-12-03T18:40:56.325879Z","shell.execute_reply":"2023-12-03T18:41:08.407902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#     from sklearn.preprocessing import MinMaxScaler\n#     df = pd.read_csv(CFG[1])\n# #     x = df.drop(columns=['id','ID'])\n#     # Assuming your dataset is stored in a DataFrame named 'df'\n#     # If not, replace 'df' with the actual name of your DataFrame\n\n#     # Extract the target variable\n#     target_variable = 'Effort'\n#     y = df[target_variable]\n\n#     # Extract features (excluding target variable)\n#     X = df.drop(columns=['id','ID', 'Effort'])\n\n#     # Normalize the dataset\n#     scaler = MinMaxScaler()\n#     X_normalized = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n\n#     # Split the dataset into training and testing sets\n#     X_train, X_test, y_train, y_test = train_test_split(X_normalized, y, test_size=0.2, random_state=42)\n\n#     # Check the shapes of the resulting sets\n#     print(\"X_train shape:\", X_train.shape)\n#     print(\"X_test shape:\", X_test.shape)\n#     print(\"y_train shape:\", y_train.shape)\n#     print(\"y_test shape:\", y_test.shape)\n# df","metadata":{"execution":{"iopub.status.busy":"2023-12-03T18:41:08.409879Z","iopub.execute_input":"2023-12-03T18:41:08.410479Z","iopub.status.idle":"2023-12-03T18:41:08.415774Z","shell.execute_reply.started":"2023-12-03T18:41:08.410454Z","shell.execute_reply":"2023-12-03T18:41:08.414586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn import datasets\n# from sklearn.model_selection import train_test_split\n# from sklearn.svm import SVR\n# from sklearn.metrics import  mean_absolute_error\n\n\n\n# # Create an SVM regressor\n# svm_regressor = SVR(kernel='linear', C=1)\n\n# svm_regressor.fit(X_train, y_train)\n\n# # Make predictions on the test set\n# y_pred = svm_regressor.predict(X_test)\n\n# # Evaluate mean squared error\n# mae = mean_absolute_error(y_test, y_pred)\n# print(f\"Mean Squared Error: {mae}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-12-03T18:41:08.41725Z","iopub.execute_input":"2023-12-03T18:41:08.417544Z","iopub.status.idle":"2023-12-03T18:41:08.425814Z","shell.execute_reply.started":"2023-12-03T18:41:08.417521Z","shell.execute_reply":"2023-12-03T18:41:08.424822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# trainX.shape","metadata":{"execution":{"iopub.status.busy":"2023-12-03T18:41:08.427028Z","iopub.execute_input":"2023-12-03T18:41:08.427281Z","iopub.status.idle":"2023-12-03T18:41:08.435802Z","shell.execute_reply.started":"2023-12-03T18:41:08.42726Z","shell.execute_reply":"2023-12-03T18:41:08.434271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Attention, Activation\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport matplotlib.pyplot as plt\ndf = pd.read_csv(CFG[1])\n# Load your dataset (assuming it's in a DataFrame named 'df')\n# Replace 'Effort' with the actual column name you want to predict\ntarget_column = 'Effort'\n\n# Separate features (X) and target variable (y)\nX = df.drop(target_column, axis=1)\ny = df[target_column]\n\n# Data Preprocessing\nscaler_X = MinMaxScaler(feature_range=(0, 1))\nscaler_y = MinMaxScaler(feature_range=(0, 1))\n\nX_normalized = scaler_X.fit_transform(X.values)\ny_normalized = scaler_y.fit_transform(y.values.reshape(-1, 1))\n\n# Reshape X for LSTM input (assuming you have a sequence length)\nsequence_length = 1  # Adjust based on your dataset\nX_reshaped = X_normalized.reshape((X_normalized.shape[0], sequence_length, X_normalized.shape[1]))\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_reshaped, y_normalized, test_size=0.2, random_state=42)\n\n# Model Construction and Training\ninp = Input(shape=(X_reshaped.shape[1], X_reshaped.shape[2]))\ne = Sequential()(inp)\ng = LSTM(units=64, return_sequences=True)(e)\nh = LSTM(units=32, return_sequences=True)(g)\ns = Attention(use_scale=True)([h, h])\nf = Activation('relu')(s)\nout = Dense(1)(f)  # Assuming a regression task\n\nmodel = Model(inputs=inp, outputs=out)\nmodel.compile(optimizer='adam', loss='mean_squared_error')  # You can use other loss functions for regression\n\n# Implement Early Stopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n\n# Train the model and monitor the training history\nhistory = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.1, callbacks=[early_stopping])\n\n# Plot the training and validation loss\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\nplt.show()\n\n\n# Prediction\ny_pred = model.predict(X_test)\n\n# Rescale predictions to the original scale\ny_pred_rescaled = scaler_y.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n\n# Rescale true labels to the original scale\ny_test_rescaled = scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten()\n\n# Evaluate mean absolute error\nmae = np.mean(np.abs(y_test_rescaled - y_pred_rescaled))\nprint(f\"Mean Absolute Error: {mae}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-12-03T18:41:08.437576Z","iopub.execute_input":"2023-12-03T18:41:08.438828Z","iopub.status.idle":"2023-12-03T18:41:22.808409Z","shell.execute_reply.started":"2023-12-03T18:41:08.438755Z","shell.execute_reply":"2023-12-03T18:41:22.807583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(CFG[1])\n# df = df.drop(columns=['id','ID'])\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2023-12-03T18:41:22.810752Z","iopub.execute_input":"2023-12-03T18:41:22.811034Z","iopub.status.idle":"2023-12-03T18:41:22.823386Z","shell.execute_reply.started":"2023-12-03T18:41:22.811013Z","shell.execute_reply":"2023-12-03T18:41:22.822388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Attention, Activation\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.metrics import mean_absolute_error\nimport optuna\n\ndf = pd.read_csv(CFG[7])\n# df = df.drop(columns=['id','ID'])\n# Load your dataset (assuming it's in a DataFrame named 'df')\n# Replace 'Effort' with the actual column name you want to predict\ntarget_column = 'Effort'\n\n# Separate features (X) and target variable (y)\nX = df.drop(target_column, axis=1)\ny = df[target_column]\n\n# Data Preprocessing\nscaler_X = MinMaxScaler(feature_range=(0, 1))\nscaler_y = MinMaxScaler(feature_range=(0, 1))\n\nX_normalized = scaler_X.fit_transform(X.values)\ny_normalized = scaler_y.fit_transform(y.values.reshape(-1, 1))\n\n# Reshape X for LSTM input (assuming you have a sequence length)\nsequence_length = 1  # Adjust based on your dataset\nX_reshaped = X_normalized.reshape((X_normalized.shape[0], sequence_length, X_normalized.shape[1]))\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_reshaped, y_normalized, test_size=0.2, random_state=42)\n\n# Model Construction and Training\ndef build_model(units1, units2, learning_rate):\n    inp = Input(shape=(X_reshaped.shape[1], X_reshaped.shape[2]))\n    e = Sequential()(inp)\n    g = LSTM(units=units1, return_sequences=True)(e)\n    h = LSTM(units=units2, return_sequences=True)(g)\n    s = Attention(use_scale=True)([h, h])\n    f = Activation('relu')(s)\n    out = Dense(1)(f)  # Assuming a regression task\n\n    model = Model(inputs=inp, outputs=out)\n    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n    model.compile(optimizer=optimizer, loss='mean_squared_error')  # You can use other loss functions for regression\n    \n    return model\n\n# Implement Early Stopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n\n# Objective function for Optuna\ndef objective(trial):\n    units1 = trial.suggest_int('units1', 32, 128)\n    units2 = trial.suggest_int('units2', 16, 64)\n    learning_rate = trial.suggest_loguniform('learning_rate', 0.001, 0.1)\n\n    model = build_model(units1=units1, units2=units2, learning_rate=learning_rate)\n    \n    history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.1, callbacks=[early_stopping], verbose=0)\n    \n    return mean_absolute_error(scaler_y.inverse_transform(y_test.reshape(-1, 1)), scaler_y.inverse_transform(model.predict(X_test).reshape(-1, 1)))\n\n# Perform hyperparameter tuning with Optuna\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=10)\n\n# Get the best hyperparameters\nbest_params = study.best_params\n\n# Train the final model with the best hyperparameters\nbest_model = build_model(**best_params)\nbest_model.fit(X_train, y_train, epochs=200, batch_size=32, validation_split=0.1, callbacks=[early_stopping])\n\n# Plot the training and validation loss\nplt.plot(best_model.history.history['loss'], label='Train Loss')\nplt.plot(best_model.history.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\nplt.show()\n\n# Prediction\ny_pred = best_model.predict(X_test)\n\n# Rescale predictions to the original scale\ny_pred_rescaled = scaler_y.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n\n# Rescale true labels to the original scale\ny_test_rescaled = scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten()\n\n# Evaluate mean absolute error\nmae = mean_absolute_error(y_test_rescaled, y_pred_rescaled)\nprint(f\"Mean Absolute Error: {mae}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-12-03T18:41:22.824773Z","iopub.execute_input":"2023-12-03T18:41:22.825062Z","iopub.status.idle":"2023-12-03T18:42:31.7374Z","shell.execute_reply.started":"2023-12-03T18:41:22.82504Z","shell.execute_reply":"2023-12-03T18:42:31.736504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Attention, Activation\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\n\n# Load your dataset (assuming it's in a DataFrame named 'df')\n# Replace 'Effort' with the actual column name you want to predict\ntarget_column = 'Effort'\n\n# Separate features (X) and target variable (y)\nX = df.drop(target_column, axis=1)\ny = df[target_column]\n\n# Data Preprocessing\nscaler_X = MinMaxScaler(feature_range=(0, 1))\nscaler_y = MinMaxScaler(feature_range=(0, 1))\n\nX_normalized = scaler_X.fit_transform(X.values)\ny_normalized = scaler_y.fit_transform(y.values.reshape(-1, 1))\n\n# Reshape X for LSTM input (assuming you have a sequence length)\nsequence_length = 1  # Adjust based on your dataset\nX_reshaped = X_normalized.reshape((X_normalized.shape[0], sequence_length, X_normalized.shape[1]))\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_reshaped, y_normalized, test_size=0.2, random_state=42)\n\n# Model Construction and Training\ninp = Input(shape=(X_reshaped.shape[1], X_reshaped.shape[2]))\ne = Sequential()(inp)\ng = LSTM(units=64, return_sequences=True)(e)\nh = LSTM(units=32, return_sequences=True)(g)\ns = Attention(use_scale=True)([h, h])\nf = Activation('relu')(s)\nout = Dense(1)(f)  # Assuming a regression task\n\nmodel = Model(inputs=inp, outputs=out)\nmodel.compile(optimizer='adam', loss='mean_squared_error')  # You can use other loss functions for regression\n\n# Implement Early Stopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n\n# Train the model and monitor the training history\nhistory = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.1, callbacks=[early_stopping])\n\n# Plot the training and validation loss\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\nplt.show()\n\n# Prediction\ny_pred = model.predict(X_test)\n\n# Rescale predictions to the original scale\ny_pred_rescaled = scaler_y.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n\n# Rescale true labels to the original scale\ny_test_rescaled = scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten()\n\n# Evaluate root mean squared error (RMSE)\nrmse = np.sqrt(mean_squared_error(y_test_rescaled, y_pred_rescaled))\nprint(f\"Root Mean Squared Error (RMSE): {rmse}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-12-03T18:42:31.739012Z","iopub.execute_input":"2023-12-03T18:42:31.739328Z","iopub.status.idle":"2023-12-03T18:42:37.258957Z","shell.execute_reply.started":"2023-12-03T18:42:31.7393Z","shell.execute_reply":"2023-12-03T18:42:37.258028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Attention, Activation\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.metrics import mean_squared_error\nimport optuna\n\n# Load your dataset (assuming it's in a DataFrame named 'df')\n# Replace 'Effort' with the actual column name you want to predict\ntarget_column = 'Effort'\n\n# Separate features (X) and target variable (y)\nX = df.drop(target_column, axis=1)\ny = df[target_column]\n\n# Data Preprocessing\nscaler_X = MinMaxScaler(feature_range=(0, 1))\nscaler_y = MinMaxScaler(feature_range=(0, 1))\n\nX_normalized = scaler_X.fit_transform(X.values)\ny_normalized = scaler_y.fit_transform(y.values.reshape(-1, 1))\n\n# Reshape X for LSTM input (assuming you have a sequence length)\nsequence_length = 1  # Adjust based on your dataset\nX_reshaped = X_normalized.reshape((X_normalized.shape[0], sequence_length, X_normalized.shape[1]))\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_reshaped, y_normalized, test_size=0.2, random_state=42)\n\n# Model Construction and Training\ndef build_model(units1, units2, learning_rate):\n    inp = Input(shape=(X_reshaped.shape[1], X_reshaped.shape[2]))\n    e = Sequential()(inp)\n    g = LSTM(units=units1, return_sequences=True)(e)\n    h = LSTM(units=units2, return_sequences=True)(g)\n    s = Attention(use_scale=True)([h, h])\n    f = Activation('relu')(s)\n    out = Dense(1)(f)  # Assuming a regression task\n\n    model = Model(inputs=inp, outputs=out)\n    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n    model.compile(optimizer=optimizer, loss='mean_squared_error')  # You can use other loss functions for regression\n    \n    return model\n\n# Implement Early Stopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n\n# Objective function for Optuna\ndef objective(trial):\n    units1 = trial.suggest_int('units1', 32, 128)\n    units2 = trial.suggest_int('units2', 16, 64)\n    learning_rate = trial.suggest_loguniform('learning_rate', 0.001, 0.1)\n\n    model = build_model(units1=units1, units2=units2, learning_rate=learning_rate)\n    \n    history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.1, callbacks=[early_stopping], verbose=0)\n    \n    return mean_absolute_error(scaler_y.inverse_transform(y_test.reshape(-1, 1)), scaler_y.inverse_transform(model.predict(X_test).reshape(-1, 1)))\n\n# Perform hyperparameter tuning with Optuna\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=10)\n\n# Get the best hyperparameters\nbest_params = study.best_params\n\n# Train the final model with the best hyperparameters\nbest_model = build_model(**best_params)\nbest_model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.1, callbacks=[early_stopping])\n\n# Plot the training and validation loss\nplt.plot(best_model.history.history['loss'], label='Train Loss')\nplt.plot(best_model.history.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\nplt.show()\n\n# Prediction\ny_pred = best_model.predict(X_test)\n\n# Rescale predictions to the original scale\ny_pred_rescaled = scaler_y.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n\n# Rescale true labels to the original scale\ny_test_rescaled = scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten()\n\n# Evaluate root mean squared error (RMSE)\nrmse = np.sqrt(mean_squared_error(y_test_rescaled, y_pred_rescaled))\nprint(f\"Root Mean Squared Error (RMSE): {rmse}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-12-03T18:42:37.260272Z","iopub.execute_input":"2023-12-03T18:42:37.260566Z","iopub.status.idle":"2023-12-03T18:43:50.480486Z","shell.execute_reply.started":"2023-12-03T18:42:37.260543Z","shell.execute_reply":"2023-12-03T18:43:50.479477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Attention, Activation\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport optuna\n\n# Load your dataset (assuming it's in a DataFrame named 'df')\n# Replace 'Effort' with the actual column name you want to predict\ntarget_column = 'Effort'\n\n# Separate features (X) and target variable (y)\nX = df.drop(target_column, axis=1)\ny = df[target_column]\n\n# Data Preprocessing\nscaler_X = MinMaxScaler(feature_range=(0, 1))\nscaler_y = MinMaxScaler(feature_range=(0, 1))\n\nX_normalized = scaler_X.fit_transform(X.values)\ny_normalized = scaler_y.fit_transform(y.values.reshape(-1, 1))\n\n# Reshape X for LSTM input (assuming you have a sequence length)\nsequence_length = 1  # Adjust based on your dataset\nX_reshaped = X_normalized.reshape((X_normalized.shape[0], sequence_length, X_normalized.shape[1]))\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_reshaped, y_normalized, test_size=0.2, random_state=42)\n\n# Model Construction and Training\ndef build_model(units1, units2, learning_rate):\n    inp = Input(shape=(X_reshaped.shape[1], X_reshaped.shape[2]))\n    e = Sequential()(inp)\n    g = LSTM(units=units1, return_sequences=True)(e)\n    h = LSTM(units=units2, return_sequences=True)(g)\n    s = Attention(use_scale=True)([h, h])\n    f = Activation('relu')(s)\n    out = Dense(1)(f)  # Assuming a regression task\n\n    model = Model(inputs=inp, outputs=out)\n    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n    model.compile(optimizer=optimizer, loss='mean_squared_error')  # You can use other loss functions for regression\n    \n    return model\n\n# Implement Early Stopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n\n# Objective function for Optuna\ndef objective(trial):\n    units1 = trial.suggest_int('units1', 32, 128)\n    units2 = trial.suggest_int('units2', 16, 64)\n    learning_rate = trial.suggest_loguniform('learning_rate', 0.001, 0.1)\n\n    model = build_model(units1=units1, units2=units2, learning_rate=learning_rate)\n    \n    history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.1, callbacks=[early_stopping], verbose=0)\n    \n    return mean_absolute_error(scaler_y.inverse_transform(y_test.reshape(-1, 1)), scaler_y.inverse_transform(model.predict(X_test).reshape(-1, 1)))\n\n# Perform hyperparameter tuning with Optuna\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=10)\n\n# Get the best hyperparameters\nbest_params = study.best_params\n\n# Train the final model with the best hyperparameters\nbest_model = build_model(**best_params)\nbest_model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.1, callbacks=[early_stopping])\n\n# Plot the training and validation loss\nplt.plot(best_model.history.history['loss'], label='Train Loss')\nplt.plot(best_model.history.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\nplt.show()\n\n# Prediction\ny_pred = best_model.predict(X_test)\n\n# Rescale predictions to the original scale\ny_pred_rescaled = scaler_y.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n\n# Rescale true labels to the original scale\ny_test_rescaled = scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten()\n\n# Evaluate R-squared\nr_squared = r2_score(y_test_rescaled, y_pred_rescaled)\nprint(f\"R-squared: {r_squared}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-12-03T18:43:50.483512Z","iopub.execute_input":"2023-12-03T18:43:50.483912Z","iopub.status.idle":"2023-12-03T18:44:59.982447Z","shell.execute_reply.started":"2023-12-03T18:43:50.483883Z","shell.execute_reply":"2023-12-03T18:44:59.981439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
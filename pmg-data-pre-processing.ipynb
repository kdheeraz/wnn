{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6801955,"sourceType":"datasetVersion","datasetId":3909889}],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline,make_pipeline\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Bidirectional\nfrom matplotlib import pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras import backend as K","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:27:32.726455Z","iopub.execute_input":"2024-07-05T13:27:32.726761Z","iopub.status.idle":"2024-07-05T13:27:42.569530Z","shell.execute_reply.started":"2024-07-05T13:27:32.726724Z","shell.execute_reply":"2024-07-05T13:27:42.568630Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"CFG = [\n    '/kaggle/input/software-effort-estimation-datasets/albrecht.csv',\n    '/kaggle/input/software-effort-estimation-datasets/china.csv',\n    '/kaggle/input/software-effort-estimation-datasets/desharnais.csv',\n    '/kaggle/input/software-effort-estimation-datasets/finnish.csv',\n    '/kaggle/input/software-effort-estimation-datasets/isbsg10.csv',\n    '/kaggle/input/software-effort-estimation-datasets/kemerer.csv',\n    '/kaggle/input/software-effort-estimation-datasets/kitchenham.csv',\n    '/kaggle/input/software-effort-estimation-datasets/maxwell.csv',\n    '/kaggle/input/software-effort-estimation-datasets/miyazaki94.csv'\n]","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:28:08.651037Z","iopub.execute_input":"2024-07-05T13:28:08.652278Z","iopub.status.idle":"2024-07-05T13:28:08.658340Z","shell.execute_reply.started":"2024-07-05T13:28:08.652231Z","shell.execute_reply":"2024-07-05T13:28:08.657117Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"code","source":"def data_albrecht():\n    df = pd.read_csv(CFG[0])\n    df_for_training = df.astype(float)\n    #LSTM uses sigmoid and tanh that are sensitive to magnitude so values need to be normalized\n    # normalize the dataset\n    scaler = MinMaxScaler()\n    scaler = scaler.fit(df_for_training)\n    df_for_training_scaled = scaler.transform(df_for_training)\n    return df\n\n\ndef data_china():\n    \n    df = pd.read_csv(CFG[1])\n    df = df.drop(columns=['id','ID'])\n    df_for_training = df.astype(float)\n#     print(df.columns)\n    #LSTM uses sigmoid and tanh that are sensitive to magnitude so values need to be normalized\n    # normalize the dataset\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    scaler = scaler.fit(df_for_training)\n    df_for_training_scaled = scaler.transform(df_for_training)\n    \n    trainX = []\n    trainY = []\n    n_future = 2   # Number of days we want to look into the future based on the past days.\n    n_past = 2 # Number of past days we want to use to predict the future.\n\n    for i in range(n_past, len(df_for_training_scaled) - n_future +1):\n        trainX.append(df_for_training_scaled[i - n_past:i, 0:df_for_training.shape[1]])\n        trainY.append(df_for_training_scaled[i + n_future - 1:i + n_future, df_for_training.shape[1] - 1])\n\n    trainX, trainY = np.array(trainX), np.array(trainY)\n\n    return trainX, trainY\n\n\ndef data_desharnais():\n    df = pd.read_csv(CFG[2])\n    df = df.drop(columns=['Project','YearEnd', 'Language'])\n    df_for_training = df.astype(float)\n    #LSTM uses sigmoid and tanh that are sensitive to magnitude so values need to be normalized\n    # normalize the dataset\n    scaler = StandardScaler()\n    scaler = scaler.fit(df_for_training)\n    df_for_training_scaled = scaler.transform(df_for_training)\n    return df\n\n\ndef data_finnish():\n    df = pd.read_csv(CFG[3])\n    df_for_training = df.astype(float)\n    #LSTM uses sigmoid and tanh that are sensitive to magnitude so values need to be normalized\n    # normalize the dataset\n    scaler = MinMaxScaler()\n    scaler = scaler.fit(df_for_training)\n    df_for_training_scaled = scaler.transform(df_for_training)\n    return df\n\n\ndef data_isbsg10():\n\n    df = pd.read_csv(CFG[4])\n    df = df.drop(columns = ['id', 'ID', 'Data_Quality', 'Year','AG', 'N_effort_level1',\n           'N_PDR1', 'SDR', 'PET', 'PIT', 'I_Date', 'PAS',\n           'Recording_Method', 'Resource_Level', 'MTS', 'ATS', 'R_PWE_NPA',\n           'P_UWE', 'CASE_Tool', 'UM', 'HMA', 'Hardware1',\n           'IDE', 'DT1', 'DBS1', 'CS1', 'WS1', 'MS1', 'OP1', 'RTA',\n           'SP_CMMI', 'SP_ISO', 'SP_TICKIT', 'MIN_Defects', 'MAJ_Defects',\n           'X_Defects', 'TOT_Defects', 'UB_BU', 'UB_L', 'UB_DU', 'UB_CU',\n           'IMarket', 'T_Platform', 'D_Embedded', 'SE', 'SEA', 'SEM', 'E_Estimate',\n           'E_Estimate_Method', 'DDE', 'DDEM', 'C_Estimate', 'CEC', 'CEM',\n           'E_Tool', 'E_Comments', 'EC_Date', 'SR?', 'SR', 'R_FPC', 'R_FPA',\n           'P_Defects', 'D_Defects', 'MIN_B_Defects', 'MAJ_B_Defects',\n           'X_B_Defects', 'TOT_B_Defects', 'MIN_T_Defects', 'MAJ_T_Defects',\n           'X_T_Defects', 'TOT_T_Defects','S_Defects', 'MIN_I_Defects', 'MAJ_I_Defects',\n           'X_I_Defects', 'TOT_I_Defects'], axis = 1)\n    # Replace \"?\" with numpy.nan\n    df.replace('?', np.nan, inplace=True)\n#     print(df.columns)\n    \n    # imputation transformer\n    trf1 = ColumnTransformer([\n        ('impute',SimpleImputer(strategy='constant', fill_value='Missing'),[14,15,16,17,18])\n    ],remainder='passthrough')\n    \n    trf2 = ColumnTransformer([\n    ('ohe',OneHotEncoder(sparse_output=False,drop= 'first'),[0,1,2,3,4,5,6,7,8,9,10,11,12,13,15])\n    ],remainder='passthrough')\n    \n    pipe = Pipeline([\n    ('trf1',trf1),\n    ('trf2',trf2),\n    ])\n    \n    pipe.fit_transform(df)\n    \n    return pipe.fit_transform(df)\n\n\ndef data_kemerer():\n    df = pd.read_csv(CFG[5])\n    df_for_training = df.astype(float)\n    #LSTM uses sigmoid and tanh that are sensitive to magnitude so values need to be normalized\n    # normalize the dataset\n    scaler = StandardScaler()\n    scaler = scaler.fit(df_for_training)\n    df_for_training_scaled = scaler.transform(df_for_training)\n    return df\n    \n\n    \ndef data_kitchenham():\n    df = pd.read_csv(CFG[6])\n    df_for_training = df.astype(float)\n    #LSTM uses sigmoid and tanh that are sensitive to magnitude so values need to be normalized\n    # normalize the dataset\n    scaler = MinMaxScaler()\n    scaler = scaler.fit(df_for_training)\n    df_for_training_scaled = scaler.transform(df_for_training)\n    return df\n\n\ndef data_maxwell():\n    df = pd.read_csv(CFG[7])\n    df_for_training = df.astype(float)\n    #LSTM uses sigmoid and tanh that are sensitive to magnitude so values need to be normalized\n    # normalize the dataset\n    scaler = StandardScaler()\n    scaler = scaler.fit(df_for_training)\n    df_for_training_scaled = scaler.transform(df_for_training)\n    return df\n\n\ndef data_miyazaki():\n    df = pd.read_csv(CFG[8])\n    df_for_training = df.astype(float)\n    #LSTM uses sigmoid and tanh that are sensitive to magnitude so values need to be normalized\n    # normalize the dataset\n    scaler = StandardScaler()\n    scaler = scaler.fit(df_for_training)\n    df_for_training_scaled = scaler.transform(df_for_training)\n    return df\n","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:28:12.388783Z","iopub.execute_input":"2024-07-05T13:28:12.389402Z","iopub.status.idle":"2024-07-05T13:28:12.420779Z","shell.execute_reply.started":"2024-07-05T13:28:12.389359Z","shell.execute_reply":"2024-07-05T13:28:12.419745Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import pywt","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:30:36.246523Z","iopub.execute_input":"2024-07-05T13:30:36.247546Z","iopub.status.idle":"2024-07-05T13:30:36.346397Z","shell.execute_reply.started":"2024-07-05T13:30:36.247500Z","shell.execute_reply":"2024-07-05T13:30:36.345441Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Define the Morlet wavelet activation function\ndef morlet_wavelet(x, w=5.0):\n    return np.cos(w * x) * np.exp(-x**2 / 2)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:30:51.069735Z","iopub.execute_input":"2024-07-05T13:30:51.070607Z","iopub.status.idle":"2024-07-05T13:30:51.076293Z","shell.execute_reply.started":"2024-07-05T13:30:51.070561Z","shell.execute_reply":"2024-07-05T13:30:51.074977Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Layer","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:32:09.599321Z","iopub.execute_input":"2024-07-05T13:32:09.600138Z","iopub.status.idle":"2024-07-05T13:32:09.605488Z","shell.execute_reply.started":"2024-07-05T13:32:09.600092Z","shell.execute_reply":"2024-07-05T13:32:09.604293Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class MorletWaveletLayer(Layer):\n    def __init__(self, units, w=5.0, **kwargs):\n        super(MorletWaveletLayer, self).__init__(**kwargs)\n        self.units = units\n        self.w = w\n\n    def build(self, input_shape):\n        self.kernel = self.add_weight(name='kernel',\n                                      shape=(input_shape[-1], self.units),\n                                      initializer='glorot_uniform',\n                                      trainable=True)\n\n    def call(self, inputs):\n        # Reshape inputs to 2D for the dense operation\n        input_reshaped = tf.reshape(inputs, [-1, inputs.shape[-1]])\n        z = tf.matmul(input_reshaped, self.kernel)\n\n        # Implement Morlet wavelet using TensorFlow operations\n        wavelet_output = tf.cos(self.w * z) * tf.exp(-z**2 / 2)\n\n        # Reshape back to 3D\n        return tf.reshape(wavelet_output, tf.concat([tf.shape(inputs)[:-1], [self.units]], axis=0))","metadata":{"execution":{"iopub.status.busy":"2024-07-05T14:00:17.594700Z","iopub.execute_input":"2024-07-05T14:00:17.595351Z","iopub.status.idle":"2024-07-05T14:00:17.604889Z","shell.execute_reply.started":"2024-07-05T14:00:17.595314Z","shell.execute_reply":"2024-07-05T14:00:17.603880Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"trainX, trainY = data_china()","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:35:15.837314Z","iopub.execute_input":"2024-07-05T13:35:15.837985Z","iopub.status.idle":"2024-07-05T13:35:15.854724Z","shell.execute_reply.started":"2024-07-05T13:35:15.837954Z","shell.execute_reply":"2024-07-05T13:35:15.853927Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"trainX.shape","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:44:16.370996Z","iopub.execute_input":"2024-07-05T13:44:16.371324Z","iopub.status.idle":"2024-07-05T13:44:16.377337Z","shell.execute_reply.started":"2024-07-05T13:44:16.371297Z","shell.execute_reply":"2024-07-05T13:44:16.376380Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"(496, 2, 18)"},"metadata":{}}]},{"cell_type":"code","source":"if __name__ == \"__main__\":\n\n    # Print the shape of trainX and trainY\n    print('trainX shape == {}.'.format(trainX.shape))\n    print('trainY shape == {}.'.format(trainY.shape))\n\n    # Create the Wavelet Neural Network\n    input_shape = trainX.shape[1:]  # Assuming trainX.shape is (496, 2, 18), input_shape should be (2, 18)\n    wnn = Sequential([\n        BatchNormalization(input_shape=input_shape),\n        Dense(64, activation='relu'),\n        Dropout(0.1),\n        Dense(32, activation='relu'),\n        MorletWaveletLayer(10),  # Replace with appropriate units for your use case\n        Dense(trainY.shape[1])  # Adjust output dim to match trainY.shape[1]\n    ])\n\n    # Compile the model\n    wnn.compile(optimizer='adam', loss='mse')\n    wnn.summary()\n\n    # Train the model\n    history = wnn.fit(trainX, trainY, epochs=100, batch_size=32, validation_split=0.25, verbose=1)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-05T14:04:08.861981Z","iopub.execute_input":"2024-07-05T14:04:08.862800Z","iopub.status.idle":"2024-07-05T14:04:20.523485Z","shell.execute_reply.started":"2024-07-05T14:04:08.862770Z","shell.execute_reply":"2024-07-05T14:04:20.522643Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"trainX shape == (496, 2, 18).\ntrainY shape == (496, 1).\nModel: \"sequential_19\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n batch_normalization_12 (Bat  (None, 2, 18)            72        \n chNormalization)                                                \n                                                                 \n dense_24 (Dense)            (None, 2, 64)             1216      \n                                                                 \n dropout_8 (Dropout)         (None, 2, 64)             0         \n                                                                 \n dense_25 (Dense)            (None, 2, 32)             2080      \n                                                                 \n morlet_wavelet_layer_14 (Mo  (None, 2, 10)            320       \n rletWaveletLayer)                                               \n                                                                 \n dense_26 (Dense)            (None, 2, 1)              11        \n                                                                 \n=================================================================\nTotal params: 3,699\nTrainable params: 3,663\nNon-trainable params: 36\n_________________________________________________________________\nEpoch 1/100\n12/12 [==============================] - 2s 27ms/step - loss: 0.1957 - val_loss: 0.1573\nEpoch 2/100\n12/12 [==============================] - 0s 8ms/step - loss: 0.1130 - val_loss: 0.1057\nEpoch 3/100\n12/12 [==============================] - 0s 7ms/step - loss: 0.0968 - val_loss: 0.0967\nEpoch 4/100\n12/12 [==============================] - 0s 8ms/step - loss: 0.0700 - val_loss: 0.0720\nEpoch 5/100\n12/12 [==============================] - 0s 8ms/step - loss: 0.0606 - val_loss: 0.0572\nEpoch 6/100\n12/12 [==============================] - 0s 8ms/step - loss: 0.0622 - val_loss: 0.0473\nEpoch 7/100\n12/12 [==============================] - 0s 7ms/step - loss: 0.0563 - val_loss: 0.0402\nEpoch 8/100\n12/12 [==============================] - 0s 7ms/step - loss: 0.0442 - val_loss: 0.0400\nEpoch 9/100\n12/12 [==============================] - 0s 8ms/step - loss: 0.0485 - val_loss: 0.0334\nEpoch 10/100\n12/12 [==============================] - 0s 8ms/step - loss: 0.0439 - val_loss: 0.0314\nEpoch 11/100\n12/12 [==============================] - 0s 7ms/step - loss: 0.0377 - val_loss: 0.0308\nEpoch 12/100\n12/12 [==============================] - 0s 7ms/step - loss: 0.0418 - val_loss: 0.0288\nEpoch 13/100\n12/12 [==============================] - 0s 7ms/step - loss: 0.0345 - val_loss: 0.0268\nEpoch 14/100\n12/12 [==============================] - 0s 8ms/step - loss: 0.0315 - val_loss: 0.0268\nEpoch 15/100\n12/12 [==============================] - 0s 8ms/step - loss: 0.0308 - val_loss: 0.0265\nEpoch 16/100\n12/12 [==============================] - 0s 7ms/step - loss: 0.0291 - val_loss: 0.0254\nEpoch 17/100\n12/12 [==============================] - 0s 7ms/step - loss: 0.0286 - val_loss: 0.0241\nEpoch 18/100\n12/12 [==============================] - 0s 8ms/step - loss: 0.0252 - val_loss: 0.0238\nEpoch 19/100\n12/12 [==============================] - 0s 7ms/step - loss: 0.0238 - val_loss: 0.0237\nEpoch 20/100\n12/12 [==============================] - 0s 7ms/step - loss: 0.0228 - val_loss: 0.0230\nEpoch 21/100\n12/12 [==============================] - 0s 8ms/step - loss: 0.0226 - val_loss: 0.0229\nEpoch 22/100\n12/12 [==============================] - 0s 8ms/step - loss: 0.0216 - val_loss: 0.0231\nEpoch 23/100\n12/12 [==============================] - 0s 7ms/step - loss: 0.0213 - val_loss: 0.0230\nEpoch 24/100\n12/12 [==============================] - 0s 7ms/step - loss: 0.0211 - val_loss: 0.0231\nEpoch 25/100\n12/12 [==============================] - 0s 8ms/step - loss: 0.0214 - val_loss: 0.0228\nEpoch 26/100\n12/12 [==============================] - 0s 8ms/step - loss: 0.0204 - val_loss: 0.0231\nEpoch 27/100\n12/12 [==============================] - 0s 8ms/step - loss: 0.0213 - val_loss: 0.0228\nEpoch 28/100\n12/12 [==============================] - 0s 8ms/step - loss: 0.0179 - val_loss: 0.0226\nEpoch 29/100\n12/12 [==============================] - 0s 8ms/step - loss: 0.0189 - val_loss: 0.0225\nEpoch 30/100\n12/12 [==============================] - 0s 8ms/step - loss: 0.0170 - val_loss: 0.0226\nEpoch 31/100\n12/12 [==============================] - 0s 8ms/step - loss: 0.0178 - val_loss: 0.0226\nEpoch 32/100\n12/12 [==============================] - 0s 7ms/step - loss: 0.0190 - val_loss: 0.0226\nEpoch 33/100\n12/12 [==============================] - 0s 7ms/step - loss: 0.0171 - val_loss: 0.0225\nEpoch 34/100\n12/12 [==============================] - 0s 7ms/step - loss: 0.0185 - val_loss: 0.0222\nEpoch 35/100\n12/12 [==============================] - 0s 8ms/step - loss: 0.0162 - val_loss: 0.0217\nEpoch 36/100\n12/12 [==============================] - 0s 8ms/step - loss: 0.0167 - val_loss: 0.0217\nEpoch 37/100\n12/12 [==============================] - 0s 8ms/step - loss: 0.0154 - val_loss: 0.0215\nEpoch 38/100\n12/12 [==============================] - 0s 7ms/step - loss: 0.0164 - val_loss: 0.0218\nEpoch 39/100\n12/12 [==============================] - 0s 8ms/step - loss: 0.0171 - val_loss: 0.0221\nEpoch 40/100\n12/12 [==============================] - 0s 8ms/step - loss: 0.0154 - val_loss: 0.0214\nEpoch 41/100\n12/12 [==============================] - 0s 7ms/step - loss: 0.0149 - val_loss: 0.0215\nEpoch 42/100\n12/12 [==============================] - 0s 8ms/step - loss: 0.0157 - val_loss: 0.0221\nEpoch 43/100\n12/12 [==============================] - 0s 8ms/step - loss: 0.0158 - val_loss: 0.0220\nEpoch 44/100\n12/12 [==============================] - 0s 8ms/step - loss: 0.0150 - val_loss: 0.0220\nEpoch 45/100\n12/12 [==============================] - 0s 7ms/step - loss: 0.0140 - val_loss: 0.0216\nEpoch 46/100\n12/12 [==============================] - 0s 7ms/step - loss: 0.0148 - val_loss: 0.0218\nEpoch 47/100\n12/12 [==============================] - 0s 8ms/step - loss: 0.0149 - val_loss: 0.0211\nEpoch 48/100\n12/12 [==============================] - 0s 8ms/step - loss: 0.0148 - val_loss: 0.0213\nEpoch 49/100\n12/12 [==============================] - 0s 7ms/step - loss: 0.0147 - val_loss: 0.0219\nEpoch 50/100\n12/12 [==============================] - 0s 7ms/step - loss: 0.0145 - val_loss: 0.0217\nEpoch 51/100\n12/12 [==============================] - 0s 7ms/step - loss: 0.0137 - val_loss: 0.0212\nEpoch 52/100\n12/12 [==============================] - 0s 7ms/step - loss: 0.0137 - val_loss: 0.0217\nEpoch 53/100\n12/12 [==============================] - 0s 8ms/step - loss: 0.0138 - val_loss: 0.0221\nEpoch 54/100\n12/12 [==============================] - 0s 8ms/step - loss: 0.0141 - val_loss: 0.0218\nEpoch 55/100\n12/12 [==============================] - 0s 8ms/step - loss: 0.0138 - val_loss: 0.0221\nEpoch 56/100\n12/12 [==============================] - 0s 8ms/step - loss: 0.0137 - val_loss: 0.0221\nEpoch 57/100\n12/12 [==============================] - 0s 7ms/step - loss: 0.0134 - val_loss: 0.0219\nEpoch 58/100\n12/12 [==============================] - 0s 8ms/step - loss: 0.0140 - val_loss: 0.0217\nEpoch 59/100\n12/12 [==============================] - 0s 7ms/step - loss: 0.0132 - val_loss: 0.0215\nEpoch 60/100\n12/12 [==============================] - 0s 8ms/step - loss: 0.0134 - val_loss: 0.0217\nEpoch 61/100\n12/12 [==============================] - 0s 8ms/step - loss: 0.0139 - val_loss: 0.0217\nEpoch 62/100\n12/12 [==============================] - 0s 8ms/step - loss: 0.0133 - val_loss: 0.0216\nEpoch 63/100\n12/12 [==============================] - 0s 8ms/step - loss: 0.0133 - val_loss: 0.0216\nEpoch 64/100\n12/12 [==============================] - 0s 8ms/step - loss: 0.0135 - val_loss: 0.0215\nEpoch 65/100\n12/12 [==============================] - 0s 7ms/step - loss: 0.0141 - val_loss: 0.0217\nEpoch 66/100\n12/12 [==============================] - 0s 7ms/step - loss: 0.0131 - val_loss: 0.0223\nEpoch 67/100\n12/12 [==============================] - 0s 7ms/step - loss: 0.0139 - val_loss: 0.0214\nEpoch 68/100\n12/12 [==============================] - 0s 7ms/step - loss: 0.0133 - val_loss: 0.0214\nEpoch 69/100\n12/12 [==============================] - 0s 8ms/step - loss: 0.0134 - val_loss: 0.0216\nEpoch 70/100\n12/12 [==============================] - 0s 8ms/step - loss: 0.0125 - val_loss: 0.0219\nEpoch 71/100\n12/12 [==============================] - 0s 7ms/step - loss: 0.0130 - val_loss: 0.0217\nEpoch 72/100\n12/12 [==============================] - 0s 8ms/step - loss: 0.0120 - val_loss: 0.0213\nEpoch 73/100\n12/12 [==============================] - 0s 8ms/step - loss: 0.0130 - val_loss: 0.0215\nEpoch 74/100\n12/12 [==============================] - 0s 7ms/step - loss: 0.0128 - val_loss: 0.0214\nEpoch 75/100\n12/12 [==============================] - 0s 8ms/step - loss: 0.0133 - val_loss: 0.0214\nEpoch 76/100\n12/12 [==============================] - 0s 8ms/step - loss: 0.0127 - val_loss: 0.0214\nEpoch 77/100\n12/12 [==============================] - 0s 7ms/step - loss: 0.0128 - val_loss: 0.0216\nEpoch 78/100\n12/12 [==============================] - 0s 8ms/step - loss: 0.0127 - val_loss: 0.0212\nEpoch 79/100\n12/12 [==============================] - 0s 8ms/step - loss: 0.0127 - val_loss: 0.0218\nEpoch 80/100\n12/12 [==============================] - 0s 8ms/step - loss: 0.0123 - val_loss: 0.0215\nEpoch 81/100\n12/12 [==============================] - 0s 7ms/step - loss: 0.0123 - val_loss: 0.0213\nEpoch 82/100\n12/12 [==============================] - 0s 7ms/step - loss: 0.0125 - val_loss: 0.0214\nEpoch 83/100\n12/12 [==============================] - 0s 7ms/step - loss: 0.0123 - val_loss: 0.0216\nEpoch 84/100\n12/12 [==============================] - 0s 8ms/step - loss: 0.0128 - val_loss: 0.0214\nEpoch 85/100\n12/12 [==============================] - 0s 8ms/step - loss: 0.0128 - val_loss: 0.0218\nEpoch 86/100\n12/12 [==============================] - 0s 7ms/step - loss: 0.0127 - val_loss: 0.0217\nEpoch 87/100\n12/12 [==============================] - 0s 7ms/step - loss: 0.0124 - val_loss: 0.0216\nEpoch 88/100\n12/12 [==============================] - 0s 8ms/step - loss: 0.0124 - val_loss: 0.0213\nEpoch 89/100\n12/12 [==============================] - 0s 7ms/step - loss: 0.0124 - val_loss: 0.0215\nEpoch 90/100\n12/12 [==============================] - 0s 8ms/step - loss: 0.0120 - val_loss: 0.0215\nEpoch 91/100\n12/12 [==============================] - 0s 8ms/step - loss: 0.0121 - val_loss: 0.0214\nEpoch 92/100\n12/12 [==============================] - 0s 7ms/step - loss: 0.0117 - val_loss: 0.0214\nEpoch 93/100\n12/12 [==============================] - 0s 7ms/step - loss: 0.0123 - val_loss: 0.0215\nEpoch 94/100\n12/12 [==============================] - 0s 7ms/step - loss: 0.0121 - val_loss: 0.0215\nEpoch 95/100\n12/12 [==============================] - 0s 7ms/step - loss: 0.0121 - val_loss: 0.0215\nEpoch 96/100\n12/12 [==============================] - 0s 7ms/step - loss: 0.0121 - val_loss: 0.0215\nEpoch 97/100\n12/12 [==============================] - 0s 8ms/step - loss: 0.0121 - val_loss: 0.0218\nEpoch 98/100\n12/12 [==============================] - 0s 8ms/step - loss: 0.0120 - val_loss: 0.0216\nEpoch 99/100\n12/12 [==============================] - 0s 8ms/step - loss: 0.0126 - val_loss: 0.0214\nEpoch 100/100\n12/12 [==============================] - 0s 8ms/step - loss: 0.0120 - val_loss: 0.0217\n","output_type":"stream"}]},{"cell_type":"code","source":"    # Plot training history\n    plt.plot(history.history['loss'], label='Training loss')\n    plt.plot(history.history['val_loss'], label='Validation loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-05T14:04:27.903425Z","iopub.execute_input":"2024-07-05T14:04:27.904103Z","iopub.status.idle":"2024-07-05T14:04:28.126267Z","shell.execute_reply.started":"2024-07-05T14:04:27.904069Z","shell.execute_reply":"2024-07-05T14:04:28.125342Z"},"trusted":true},"execution_count":37,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"def mean_magnitude_relative_error(y_true, y_pred):\n    # Calculate relative error\n    relative_error = np.abs((y_true - y_pred) / y_true)\n    \n    # Calculate mean magnitude of relative error (MMRE)\n    mmre = np.mean(relative_error)\n    \n    return mmre\n\nif __name__ == \"__main__\":\n    # Load or generate your data\n    X, y = data_china()\n\n    # Assuming trainX and trainY are defined appropriately\n    trainX = X[:400]  # Example: using first 400 samples for training\n    trainY = y[:400]  # Example: using first 400 targets for training\n    testX = X[400:]   # Example: using remaining samples for testing\n    testY = y[400:]   # Example: using remaining targets for testing\n\n    # Create the Wavelet Neural Network\n    input_shape = trainX.shape[1:]  # Assuming trainX.shape is (496, 2, 18), input_shape should be (2, 18)\n    wnn = Sequential([\n        BatchNormalization(input_shape=input_shape),\n        Dense(64, activation='relu'),\n        Dropout(0.1),\n        Dense(32, activation='relu'),\n        MorletWaveletLayer(10),  # Replace with appropriate units for your use case\n        Dense(trainY.shape[1])  # Adjust output dim to match trainY.shape[1]\n    ])\n\n    # Compile the model\n    wnn.compile(optimizer='adam', loss='mse')\n    wnn.summary()\n\n    # Train the model\n    history = wnn.fit(trainX, trainY, epochs=50, batch_size=32, validation_split=0.1, verbose=1)\n\n    # Evaluate the model on test data\n    predictions = wnn.predict(testX)\n\n    # Calculate evaluation metrics\n    mae = mean_absolute_error(testY, predictions)\n    mse = mean_squared_error(testY, predictions)\n    r2 = r2_score(testY, predictions)\n    mmre = mean_magnitude_relative_error(testY, predictions)\n\n    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n    print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n    print(f\"R-squared (R2) Score: {r2:.4f}\")\n    print(f\"Mean Magnitude of Relative Error (MMRE): {mmre:.4f}\")\n\n    # Plot training history\n    plt.plot(history.history['loss'], label='Training loss')\n    plt.plot(history.history['val_loss'], label='Validation loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-05T14:07:45.659562Z","iopub.execute_input":"2024-07-05T14:07:45.659931Z","iopub.status.idle":"2024-07-05T14:07:52.841141Z","shell.execute_reply.started":"2024-07-05T14:07:45.659900Z","shell.execute_reply":"2024-07-05T14:07:52.839930Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"Model: \"sequential_20\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n batch_normalization_13 (Bat  (None, 2, 18)            72        \n chNormalization)                                                \n                                                                 \n dense_27 (Dense)            (None, 2, 64)             1216      \n                                                                 \n dropout_9 (Dropout)         (None, 2, 64)             0         \n                                                                 \n dense_28 (Dense)            (None, 2, 32)             2080      \n                                                                 \n morlet_wavelet_layer_15 (Mo  (None, 2, 10)            320       \n rletWaveletLayer)                                               \n                                                                 \n dense_29 (Dense)            (None, 2, 1)              11        \n                                                                 \n=================================================================\nTotal params: 3,699\nTrainable params: 3,663\nNon-trainable params: 36\n_________________________________________________________________\nEpoch 1/50\n12/12 [==============================] - 2s 29ms/step - loss: 0.3330 - val_loss: 0.5497\nEpoch 2/50\n12/12 [==============================] - 0s 8ms/step - loss: 0.1973 - val_loss: 0.4921\nEpoch 3/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.1265 - val_loss: 0.3918\nEpoch 4/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.1088 - val_loss: 0.3397\nEpoch 5/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.0959 - val_loss: 0.3384\nEpoch 6/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.0948 - val_loss: 0.3064\nEpoch 7/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.0805 - val_loss: 0.2805\nEpoch 8/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.0743 - val_loss: 0.2470\nEpoch 9/50\n12/12 [==============================] - 0s 8ms/step - loss: 0.0662 - val_loss: 0.2232\nEpoch 10/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.0690 - val_loss: 0.2102\nEpoch 11/50\n12/12 [==============================] - 0s 8ms/step - loss: 0.0564 - val_loss: 0.1843\nEpoch 12/50\n12/12 [==============================] - 0s 8ms/step - loss: 0.0596 - val_loss: 0.1568\nEpoch 13/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.0524 - val_loss: 0.1534\nEpoch 14/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.0589 - val_loss: 0.1454\nEpoch 15/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.0554 - val_loss: 0.1232\nEpoch 16/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.0407 - val_loss: 0.1148\nEpoch 17/50\n12/12 [==============================] - 0s 8ms/step - loss: 0.0499 - val_loss: 0.1026\nEpoch 18/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.0483 - val_loss: 0.0998\nEpoch 19/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.0444 - val_loss: 0.0946\nEpoch 20/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.0398 - val_loss: 0.0815\nEpoch 21/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.0492 - val_loss: 0.0846\nEpoch 22/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.0426 - val_loss: 0.0827\nEpoch 23/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.0401 - val_loss: 0.0713\nEpoch 24/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.0408 - val_loss: 0.0689\nEpoch 25/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.0351 - val_loss: 0.0685\nEpoch 26/50\n12/12 [==============================] - 0s 8ms/step - loss: 0.0326 - val_loss: 0.0586\nEpoch 27/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.0355 - val_loss: 0.0548\nEpoch 28/50\n12/12 [==============================] - 0s 10ms/step - loss: 0.0330 - val_loss: 0.0569\nEpoch 29/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.0359 - val_loss: 0.0590\nEpoch 30/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.0340 - val_loss: 0.0521\nEpoch 31/50\n12/12 [==============================] - 0s 8ms/step - loss: 0.0329 - val_loss: 0.0514\nEpoch 32/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.0290 - val_loss: 0.0523\nEpoch 33/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.0301 - val_loss: 0.0483\nEpoch 34/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.0326 - val_loss: 0.0550\nEpoch 35/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.0261 - val_loss: 0.0549\nEpoch 36/50\n12/12 [==============================] - 0s 8ms/step - loss: 0.0312 - val_loss: 0.0516\nEpoch 37/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.0285 - val_loss: 0.0499\nEpoch 38/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.0239 - val_loss: 0.0482\nEpoch 39/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.0249 - val_loss: 0.0437\nEpoch 40/50\n12/12 [==============================] - 0s 8ms/step - loss: 0.0259 - val_loss: 0.0438\nEpoch 41/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.0254 - val_loss: 0.0445\nEpoch 42/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.0259 - val_loss: 0.0470\nEpoch 43/50\n12/12 [==============================] - 0s 8ms/step - loss: 0.0253 - val_loss: 0.0404\nEpoch 44/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.0233 - val_loss: 0.0387\nEpoch 45/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.0271 - val_loss: 0.0410\nEpoch 46/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.0240 - val_loss: 0.0377\nEpoch 47/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.0229 - val_loss: 0.0341\nEpoch 48/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.0224 - val_loss: 0.0324\nEpoch 49/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.0225 - val_loss: 0.0328\nEpoch 50/50\n12/12 [==============================] - 0s 7ms/step - loss: 0.0229 - val_loss: 0.0342\n3/3 [==============================] - 0s 3ms/step\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[38], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m predictions \u001b[38;5;241m=\u001b[39m wnn\u001b[38;5;241m.\u001b[39mpredict(testX)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Calculate evaluation metrics\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m mae \u001b[38;5;241m=\u001b[39m \u001b[43mmean_absolute_error\u001b[49m(testY, predictions)\n\u001b[1;32m     43\u001b[0m mse \u001b[38;5;241m=\u001b[39m mean_squared_error(testY, predictions)\n\u001b[1;32m     44\u001b[0m r2 \u001b[38;5;241m=\u001b[39m r2_score(testY, predictions)\n","\u001b[0;31mNameError\u001b[0m: name 'mean_absolute_error' is not defined"],"ename":"NameError","evalue":"name 'mean_absolute_error' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"# LSTM Model","metadata":{}},{"cell_type":"code","source":"trainX, trainY = data_china()\n\n\nprint('trainX shape == {}.'.format(trainX.shape))\nprint('trainY shape == {}.'.format(trainY.shape))\n\n# define the Autoencoder model\n\nmodel = Sequential()\nmodel.add(BatchNormalization(input_shape=(trainX.shape[1], trainX.shape[2])))\nmodel.add(Bidirectional(LSTM(64, activation='relu', return_sequences=True)))\nmodel.add(LSTM(32, activation='relu', return_sequences=False))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(trainY.shape[1]))\n\nmodel.compile(optimizer='sgd', loss='mse')\nmodel.summary()\n\n\n# fit the model\nhistory = model.fit(trainX, trainY, epochs=50, batch_size=32, validation_split=0.1, verbose=1)\n\nplt.plot(history.history['loss'], label='Training loss')\nplt.plot(history.history['val_loss'], label='Validation loss')\n","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:28:29.569473Z","iopub.execute_input":"2024-07-05T13:28:29.570182Z","iopub.status.idle":"2024-07-05T13:28:55.714705Z","shell.execute_reply.started":"2024-07-05T13:28:29.570135Z","shell.execute_reply":"2024-07-05T13:28:55.713720Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"trainX shape == (496, 2, 18).\ntrainY shape == (496, 1).\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n batch_normalization (BatchN  (None, 2, 18)            72        \n ormalization)                                                   \n                                                                 \n bidirectional (Bidirectiona  (None, 2, 128)           42496     \n l)                                                              \n                                                                 \n lstm_1 (LSTM)               (None, 32)                20608     \n                                                                 \n dropout (Dropout)           (None, 32)                0         \n                                                                 \n dense (Dense)               (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 63,209\nTrainable params: 63,173\nNon-trainable params: 36\n_________________________________________________________________\nEpoch 1/50\n14/14 [==============================] - 6s 59ms/step - loss: 0.0169 - val_loss: 0.0143\nEpoch 2/50\n14/14 [==============================] - 0s 20ms/step - loss: 0.0159 - val_loss: 0.0133\nEpoch 3/50\n14/14 [==============================] - 0s 20ms/step - loss: 0.0152 - val_loss: 0.0127\nEpoch 4/50\n14/14 [==============================] - 0s 20ms/step - loss: 0.0151 - val_loss: 0.0123\nEpoch 5/50\n14/14 [==============================] - 0s 20ms/step - loss: 0.0151 - val_loss: 0.0121\nEpoch 6/50\n14/14 [==============================] - 0s 21ms/step - loss: 0.0150 - val_loss: 0.0120\nEpoch 7/50\n14/14 [==============================] - 0s 20ms/step - loss: 0.0146 - val_loss: 0.0119\nEpoch 8/50\n14/14 [==============================] - 0s 20ms/step - loss: 0.0147 - val_loss: 0.0118\nEpoch 9/50\n14/14 [==============================] - 0s 23ms/step - loss: 0.0146 - val_loss: 0.0118\nEpoch 10/50\n14/14 [==============================] - 0s 25ms/step - loss: 0.0146 - val_loss: 0.0118\nEpoch 11/50\n14/14 [==============================] - 0s 24ms/step - loss: 0.0149 - val_loss: 0.0117\nEpoch 12/50\n14/14 [==============================] - 0s 27ms/step - loss: 0.0149 - val_loss: 0.0117\nEpoch 13/50\n14/14 [==============================] - 0s 24ms/step - loss: 0.0146 - val_loss: 0.0117\nEpoch 14/50\n14/14 [==============================] - 0s 21ms/step - loss: 0.0148 - val_loss: 0.0117\nEpoch 15/50\n14/14 [==============================] - 0s 20ms/step - loss: 0.0146 - val_loss: 0.0117\nEpoch 16/50\n14/14 [==============================] - 0s 21ms/step - loss: 0.0148 - val_loss: 0.0117\nEpoch 17/50\n14/14 [==============================] - 0s 21ms/step - loss: 0.0146 - val_loss: 0.0117\nEpoch 18/50\n14/14 [==============================] - 0s 21ms/step - loss: 0.0147 - val_loss: 0.0117\nEpoch 19/50\n14/14 [==============================] - 0s 21ms/step - loss: 0.0148 - val_loss: 0.0117\nEpoch 20/50\n14/14 [==============================] - 0s 20ms/step - loss: 0.0146 - val_loss: 0.0117\nEpoch 21/50\n14/14 [==============================] - 0s 21ms/step - loss: 0.0147 - val_loss: 0.0117\nEpoch 22/50\n14/14 [==============================] - 0s 23ms/step - loss: 0.0146 - val_loss: 0.0117\nEpoch 23/50\n14/14 [==============================] - 0s 21ms/step - loss: 0.0145 - val_loss: 0.0117\nEpoch 24/50\n14/14 [==============================] - 0s 22ms/step - loss: 0.0147 - val_loss: 0.0117\nEpoch 25/50\n14/14 [==============================] - 0s 21ms/step - loss: 0.0146 - val_loss: 0.0116\nEpoch 26/50\n14/14 [==============================] - 0s 20ms/step - loss: 0.0146 - val_loss: 0.0116\nEpoch 27/50\n14/14 [==============================] - 0s 22ms/step - loss: 0.0145 - val_loss: 0.0116\nEpoch 28/50\n14/14 [==============================] - 0s 20ms/step - loss: 0.0147 - val_loss: 0.0116\nEpoch 29/50\n14/14 [==============================] - 0s 20ms/step - loss: 0.0147 - val_loss: 0.0116\nEpoch 30/50\n14/14 [==============================] - 0s 21ms/step - loss: 0.0145 - val_loss: 0.0116\nEpoch 31/50\n14/14 [==============================] - 0s 20ms/step - loss: 0.0145 - val_loss: 0.0116\nEpoch 32/50\n14/14 [==============================] - 0s 21ms/step - loss: 0.0147 - val_loss: 0.0116\nEpoch 33/50\n14/14 [==============================] - 0s 21ms/step - loss: 0.0146 - val_loss: 0.0116\nEpoch 34/50\n14/14 [==============================] - 0s 21ms/step - loss: 0.0145 - val_loss: 0.0116\nEpoch 35/50\n14/14 [==============================] - 0s 20ms/step - loss: 0.0146 - val_loss: 0.0116\nEpoch 36/50\n14/14 [==============================] - 0s 20ms/step - loss: 0.0145 - val_loss: 0.0116\nEpoch 37/50\n14/14 [==============================] - 0s 21ms/step - loss: 0.0146 - val_loss: 0.0116\nEpoch 38/50\n14/14 [==============================] - 0s 21ms/step - loss: 0.0147 - val_loss: 0.0116\nEpoch 39/50\n14/14 [==============================] - 0s 21ms/step - loss: 0.0146 - val_loss: 0.0116\nEpoch 40/50\n14/14 [==============================] - 0s 20ms/step - loss: 0.0146 - val_loss: 0.0116\nEpoch 41/50\n14/14 [==============================] - 0s 20ms/step - loss: 0.0146 - val_loss: 0.0116\nEpoch 42/50\n14/14 [==============================] - 0s 20ms/step - loss: 0.0144 - val_loss: 0.0116\nEpoch 43/50\n14/14 [==============================] - 0s 21ms/step - loss: 0.0145 - val_loss: 0.0116\nEpoch 44/50\n14/14 [==============================] - 0s 19ms/step - loss: 0.0146 - val_loss: 0.0116\nEpoch 45/50\n14/14 [==============================] - 0s 21ms/step - loss: 0.0145 - val_loss: 0.0116\nEpoch 46/50\n14/14 [==============================] - 0s 23ms/step - loss: 0.0146 - val_loss: 0.0116\nEpoch 47/50\n14/14 [==============================] - 0s 19ms/step - loss: 0.0146 - val_loss: 0.0116\nEpoch 48/50\n14/14 [==============================] - 0s 21ms/step - loss: 0.0145 - val_loss: 0.0116\nEpoch 49/50\n14/14 [==============================] - 0s 20ms/step - loss: 0.0145 - val_loss: 0.0116\nEpoch 50/50\n14/14 [==============================] - 0s 21ms/step - loss: 0.0145 - val_loss: 0.0116\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"[<matplotlib.lines.Line2D at 0x7b0c23f7dde0>]"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAjUAAAGdCAYAAADqsoKGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAABSaElEQVR4nO3de1xUZeIG8GdmYBjuyEUuAoJXvHBRFMQ0NQkqK6l2Q2tXM7fatcyi3f1p24rtDd1ys9LN3NLayjRbNVOzFE1LMRRBJRWvCMpd5DbAADPn98fAwMigDMzMgeH5fj7nM8OZd2beOZHz8F4lgiAIICIiIurlpGJXgIiIiMgUGGqIiIjIKjDUEBERkVVgqCEiIiKrwFBDREREVoGhhoiIiKwCQw0RERFZBYYaIiIisgo2YlfAUjQaDQoKCuDs7AyJRCJ2dYiIiKgTBEFAdXU1/Pz8IJXevi2mz4SagoICBAQEiF0NIiIi6oL8/Hz4+/vftkyfCTXOzs4AtBfFxcVF5NoQERFRZ1RVVSEgIED3PX47fSbUtHQ5ubi4MNQQERH1Mp0ZOsKBwkRERGQVGGqIiIjIKjDUEBERkVXoUqhZs2YNgoKCoFAoEB0djfT09NuW37JlC0JCQqBQKBAaGordu3frPb5161bExcXBw8MDEokEWVlZeo/n5uZCIpEYPLZs2dKVj0BERERWxuhQs3nzZiQlJSE5ORknTpxAeHg44uPjUVJSYrD8kSNHMHv2bMyfPx+ZmZlISEhAQkICsrOzdWWUSiUmTZqEFStWGHyNgIAAFBYW6h2vv/46nJyccP/99xv7EYiIiMgKSQRBEIx5QnR0NMaPH4/Vq1cD0C5qFxAQgIULF2Lx4sXtyicmJkKpVGLnzp26cxMmTEBERATWrl2rVzY3NxfBwcHIzMxERETEbesxZswYjB07Fh9++GGn6l1VVQVXV1dUVlZy9hMREVEvYcz3t1EtNQ0NDcjIyEBsbGzrC0iliI2NRVpamsHnpKWl6ZUHgPj4+A7Ld0ZGRgaysrIwf/78DsuoVCpUVVXpHURERGS9jAo1ZWVlUKvV8Pb21jvv7e2NoqIig88pKioyqnxnfPjhhxgxYgQmTpzYYZmUlBS4urrqDq4mTEREZN163eynuro6bNy48batNACwZMkSVFZW6o78/HwL1ZCIiIjEYNSKwp6enpDJZCguLtY7X1xcDB8fH4PP8fHxMar8nXz55Zeora3FnDlzblvOzs4OdnZ2XXoPIiIi6n2MaqmRy+WIjIxEamqq7pxGo0FqaipiYmIMPicmJkavPADs3bu3w/J38uGHH+Lhhx+Gl5dXl55PRERE1snovZ+SkpIwd+5cjBs3DlFRUVi1ahWUSiXmzZsHAJgzZw4GDBiAlJQUAMCiRYswZcoUrFy5EjNmzMCmTZtw/PhxrFu3Tvea5eXlyMvLQ0FBAQAgJycHgLaVp22LzsWLF3Ho0KF269wQERERGR1qEhMTUVpaiqVLl6KoqAgRERHYs2ePbjBwXl4epNLWBqCJEydi48aNeO211/Dqq69i6NCh2L59O0aPHq0rs2PHDl0oAoBZs2YBAJKTk7Fs2TLd+fXr18Pf3x9xcXFGf1BzKaiow2c/XUWTWsCSB0aIXR0iIqI+y+h1anorc61Tc7GkGrH/OgQnOxucXhbXqV1EiYiIqHPMtk4NtRfg7gCpBKhRNaG0RiV2dYiIiPoshppusrORwb+fAwDgSqlS5NoQERH1XQw1JhDs6QgAuFLGUENERCQWhhoTYKghIiISH0ONCQzy0oaayww1REREomGoMQG21BAREYmPocYEWkJN3o1aqDV9YoY8ERFRj8NQYwJ+rvaQ20jRoNagoKJO7OoQERH1SQw1JiCVShDkoZ3WzXE1RERE4mCoMRHduJrSGpFrQkRE1Dcx1JhIsKcTAA4WJiIiEgtDjYkM8uS0biIiIjEx1JhIsBendRMREYmJocZEWsbUXK+og6pJLXJtiIiI+h6GGhPxcJTD2c4GgqBdr4aIiIgsi6HGRCQSia4LiuNqiIiILI+hxoS4XQIREZF4GGpMqHWtGoYaIiIiS2OoMSG21BAREYmHocaEBjUvwMcxNURERJbHUGNCQZ7a/Z/KalSorm8UuTZERER9C0ONCTkrbOHpZAcAyC3jtG4iIiJLYqgxsdbtErixJRERkSUx1JgYBwsTERGJg6HGxLgHFBERkTgYakyMLTVERETiYKgxsUFtFuATBEHk2hAREfUdDDUmFujhAIkEqFY1oaymQezqEBER9RkMNSZmZyODfz97AOyCIiIisiSGGjMI8tB2QeUy1BAREVkMQ40ZtK5Vw1BDRERkKQw1ZtA6A4oL8BEREVkKQ40ZBHtpN7bkmBoiIiLLYagxg5bup9wbtVBrOK2biIjIEhhqzMDPzR5ymRQNTRoUVNSJXR0iIqI+gaHGDGRSCQZ6OABgFxQREZGlMNSYSZCuC4qhhoiIyBIYasxEN627lKGGiIjIEhhqzIQbWxIREVkWQ42ZMNQQERFZFkONmQR7aUPNtZu1UDWpRa4NERGR9WOoMRMvJzs42dlAIwD55bViV4eIiMjqMdSYiUQi0XVBcbAwERGR+THUmBHH1RAREVkOQ40Zca0aIiIiy2GoMSOuVUNERGQ5DDVmxO4nIiIiy2GoMaOW7qeSahVqVE0i14aIiMi6MdSYkau9LTyd5ACAXLbWEBERmRVDjZnppnUz1BAREZkVQ42Z6cbVcLAwERGRWTHUmFmQbrBwjcg1ISIism5dCjVr1qxBUFAQFAoFoqOjkZ6eftvyW7ZsQUhICBQKBUJDQ7F79269x7du3Yq4uDh4eHhAIpEgKyvL4OukpaXhnnvugaOjI1xcXHD33Xejrq6uKx/BYlqmdV+5wa0SiIiIzMnoULN582YkJSUhOTkZJ06cQHh4OOLj41FSUmKw/JEjRzB79mzMnz8fmZmZSEhIQEJCArKzs3VllEolJk2ahBUrVnT4vmlpabjvvvsQFxeH9PR0HDt2DC+88AKk0p7d2BTs6QQAuFJaA0EQRK4NERGR9ZIIRn7TRkdHY/z48Vi9ejUAQKPRICAgAAsXLsTixYvblU9MTIRSqcTOnTt15yZMmICIiAisXbtWr2xubi6Cg4ORmZmJiIgIvccmTJiAe++9F3/961+Nqa5OVVUVXF1dUVlZCRcXly69RlfUN6oxYukeCAKQ8VosPJzsLPbeREREvZ0x399GNXM0NDQgIyMDsbGxrS8glSI2NhZpaWkGn5OWlqZXHgDi4+M7LG9ISUkJfvrpJ/Tv3x8TJ06Et7c3pkyZgh9//NGY6otCYSuDn6s9AC7CR0REZE5GhZqysjKo1Wp4e3vrnff29kZRUZHB5xQVFRlV3pDLly8DAJYtW4ZnnnkGe/bswdixYzF9+nRcuHDB4HNUKhWqqqr0DrEM8uK0biIiInPr2QNSmmk0GgDAc889h3nz5mHMmDF46623MHz4cKxfv97gc1JSUuDq6qo7AgICLFllPdwugYiIyPyMCjWenp6QyWQoLi7WO19cXAwfHx+Dz/Hx8TGqvCG+vr4AgJEjR+qdHzFiBPLy8gw+Z8mSJaisrNQd+fn5nX4/Uwvy4Fo1RERE5mZUqJHL5YiMjERqaqrunEajQWpqKmJiYgw+JyYmRq88AOzdu7fD8oYEBQXBz88POTk5eufPnz+PgQMHGnyOnZ0dXFxc9A6xBHuxpYaIiMjcbIx9QlJSEubOnYtx48YhKioKq1atglKpxLx58wAAc+bMwYABA5CSkgIAWLRoEaZMmYKVK1dixowZ2LRpE44fP45169bpXrO8vBx5eXkoKCgAAF148fHxgY+PDyQSCf7whz8gOTkZ4eHhiIiIwMcff4xz587hyy+/7PZFMLeWtWpybyih0QiQSiUi14iIiMj6GB1qEhMTUVpaiqVLl6KoqAgRERHYs2ePbjBwXl6e3toxEydOxMaNG/Haa6/h1VdfxdChQ7F9+3aMHj1aV2bHjh26UAQAs2bNAgAkJydj2bJlAICXXnoJ9fX1ePnll1FeXo7w8HDs3bsXgwcP7tIHt6QBbvawlUmgatLgekUdAtwdxK4SERGR1TF6nZreSqx1alo8+O4PyL5ehdVPjMGDYX4Wf38iIqLeyGzr1FDXjQnoBwDIyqsQtyJERERWiqHGQiIC3AAAWfkVotaDiIjIWjHUWEhEoBsA4PT1SjSqNeJWhoiIyAox1FhIsIcjXO1toWrS4FxhtdjVISIisjoMNRYilUoQruuCuiluZYiIiKwQQ40FtYyryeS4GiIiIpNjqLGgMS0tNZwBRUREZHIMNRbU0lJzuUyJytpGcStDRERkZRhqLKifoxxBHtrVhLOuVYhbGSIiIivDUGNhEeyCIiIiMguGGgtrHSzMGVBERESmxFBjYWMCtdslnMyvQB/ZdouIiMgiGGosbISvC+Q2UtysbcTVG7ViV4eIiMhqMNRYmNxGilF+2l1G2QVFRERkOgw1IuBgYSIiItNjqBFBy7ga7thNRERkOgw1ImhZWfhMYRXqG9XiVoaIiMhKMNSIwL+fPTwc5WhUC/i5oErs6hAREVkFhhoRSCQSjAl0A8AuKCIiIlNhqBGJbrAwQw0REZFJMNSIJCKgZbAwp3UTERGZAkONSMICXCGRAPnldSirUYldHSIiol6PoUYkLgpbDPFyAsD1aoiIiEyBoUZEHFdDRERkOgw1IopongHF7RKIiIi6j6FGRC0tNafyK6HRcMduIiKi7mCoEdFwb2fY28pQrWrCpdIasatDRETUqzHUiMhGJkWovysAIJPjaoiIiLqFoUZkLftAZXIGFBERUbcw1IiMM6CIiIhMg6FGZGMCtSsL5xRVobahSeTaEBER9V4MNSLzcVXAx0UBjQCcvlYpdnWIiIh6LYaaHqClC4qDhYmIiLqOoaYHaFmEj9slEBERdR1DTQ8whoOFiYiIuo2hpgcI9XeFTCpBUVU9CivrxK4OERFRr8RQ0wM4yG0wzNsZALugiIiIuoqhpocY0zKuhl1QREREXcJQ00NwBhQREVH3MNT0EC2DhU9fq0STWiNuZYiIiHohhpoeYrCXE5ztbFDXqEZ6brnY1SEiIup1GGp6CKlUgolDPAAAT60/hvcPXoJaI4hcKyIiot6DoaYH+VtCKKYN90KDWoOUb87h8ffTcKVMKXa1iIiIegWGmh7Ey9kO658ajxWPhcLJzgYZV2/i/rcP4aPDV6Bhqw0REdFtMdT0MBKJBInjA7HnpcmYONgD9Y0aLPv6DJ784Cfkl9eKXT0iIqIei6Gmh/Lv54BP50fjLzNHwd5WhrTLN3DfqkP4PD0PgsBWGyIiolsx1PRgUqkEc2KC8M2iyRg3sB+UDWos2XoaT204hsulNWJXj4iIqEeRCH3kz/6qqiq4urqisrISLi4uYlfHaGqNgPU/XsEb3+WgoUkDqQR4INQXC6YOwUi/3vd5iIiIOsOY72+Gml7mYkk1UnafQ+q5Et25e0L64/lpQxA5sJ+INSMiIjI9hhoDrCXUtDhTUIX3Dl7CrlMFaJkYNWGQO56fNgSThnhCIpGIW0EiIiITYKgxwNpCTYsrZUqs/f4StmZeQ6Na+58yzN8VC6YOQdxIb0ilDDdERNR7MdQYYK2hpkVBRR3+88NlfJ6eh/pG7d5Rnk5yTBnWH9NCvDB5qBdc7W1FriUREZFxGGoMsPZQ0+JGjQrrD1/BJ2lXUVXfpDsvk0oQObAf7gnpj2nD+2OYtxO7qIiIqMcz5vu7S1O616xZg6CgICgUCkRHRyM9Pf225bds2YKQkBAoFAqEhoZi9+7deo9v3boVcXFx8PDwgEQiQVZWVrvXmDp1KiQSid7x29/+tivVt2oeTnb4Q3wIjr92LzY+E41nJgdjSH8nqDUC0q+UY/k35xC/6hAmrTiAP207jdPXKsWuMhERkUkYHWo2b96MpKQkJCcn48SJEwgPD0d8fDxKSkoMlj9y5Ahmz56N+fPnIzMzEwkJCUhISEB2draujFKpxKRJk7BixYrbvvczzzyDwsJC3fHPf/7T2Or3GXIbKSYO9sSfZozEvqQp+OGP0/CXmaMwdbgX7GykuF5Rh89+ysOv1/8EVZNa7OoSERF1m9HdT9HR0Rg/fjxWr14NANBoNAgICMDChQuxePHiduUTExOhVCqxc+dO3bkJEyYgIiICa9eu1Subm5uL4OBgZGZmIiIiQu+xqVOnIiIiAqtWrTKmujp9pfupM+oa1Ei7XIY/fnkaZTUqfDh3HKaP8Ba7WkRERO2YrfupoaEBGRkZiI2NbX0BqRSxsbFIS0sz+Jy0tDS98gAQHx/fYfnb+eyzz+Dp6YnRo0djyZIlqK3teC8klUqFqqoqvYO07OUy3BPijYfCfQEAu04VilwjIiKi7jMq1JSVlUGtVsPbW/+vem9vbxQVFRl8TlFRkVHlO/LEE0/g008/xYEDB7BkyRJ88skn+NWvftVh+ZSUFLi6uuqOgIAAo96vL3gwTBtq9p4pRn0ju6CIiKh3sxG7Ap317LPP6u6HhobC19cX06dPx6VLlzB48OB25ZcsWYKkpCTdz1VVVQw2txgT0A++rgoUVtbjhwtluHcku6CIiKj3MqqlxtPTEzKZDMXFxXrni4uL4ePjY/A5Pj4+RpXvrOjoaADAxYsXDT5uZ2cHFxcXvYP0SaUSPBCqba3ZeapA5NoQERF1j1GhRi6XIzIyEqmpqbpzGo0GqampiImJMficmJgYvfIAsHfv3g7Ld1bLtG9fX99uvU5fN6O5C2ofu6CIiKiXM7r7KSkpCXPnzsW4ceMQFRWFVatWQalUYt68eQCAOXPmYMCAAUhJSQEALFq0CFOmTMHKlSsxY8YMbNq0CcePH8e6det0r1leXo68vDwUFGhbC3JycgBoW3l8fHxw6dIlbNy4EQ888AA8PDxw6tQpvPzyy7j77rsRFhbW7YvQl40JcMMAN3tcr6jD9zmluG9091rQiIiIxGL0OjWJiYl48803sXTpUkRERCArKwt79uzRDQbOy8tDYWHrbJqJEydi48aNWLduHcLDw/Hll19i+/btGD16tK7Mjh07MGbMGMyYMQMAMGvWLIwZM0Y35Vsul2Pfvn2Ii4tDSEgIXnnlFTz22GP4+uuvu/XhCZBIJHggVBtkdp3mLCgiIuq9uE0CISu/AglrDsNBLkPGa/fCXi4Tu0odqm9U49lPMlBYUYeBHg4IdHdEkKcDAt0dMNDDEf797GEr69JC2URE1AMZ8/3da2Y/kfmE+7u26YIqwf2hPXec0vrDV3DofCkA4EJJTbvHZVIJ/NwUGOjuiPFB7lh4zxDuVE5E1Ecw1BAkEgkeDPPF+4cuY+fpwh4basqVDXjvwCUAwKLpQ+HpJMfVG7W4Wl6LqzeUyCuvRX2jBvnldcgvr8OPF8sQ5OmAmREDuvyegiDgQkkNhvbnBqBERD0dQw0B0M6Cev/QZew/W4LahiY4yHver8bq/RdRrWrCSF8XLJo+tF0LjCAIKKlW4eqNWmzLvI7P0/Pwxrc5uG+0D+xsutal9nbqBazadwFPTQzCsodHmeJjEBGRmXDwAQEAQge4IsDdHnWNahw4Vyp2ddrJL6/FJ0dzAQBLHggx2KUkkUjg7aJAVLA7/vzgCPR3tsO1m3X49Ghel94zt0yJfze3DH10JBeHL5Z1uf4VtQ1IWHMYv/n4OBrVmi6/DhERdYyhhgBoA8GMUD8AwK7TPW8hvje+zUGjWsDkoZ6YPNTrjuUd5DZ4+d5hAIDV+y+gqr7R6Pf8684zaFBroLDV/m/yxy9PoboLryMIAn6/5RSy8iuw72wx3tp73ujXICKiO2OoIZ2WvaD2nyuBUtUkcm1anb5WiR0ntUHr/+4L6fTzfhnpjyH9nXCzthFrv79k1Humni1G6rkS2Mok2PLcRAS4awdS/33XWaNeBwDWH87FvrPFsGluXXrv4KVutfoQEZFhDDWkM8rPBQM9HFDfqEHquRKxqwNA28qR8o02SDwyZgBGD3Dt9HNtZFJdCPrwxysorKzr1PPqG9X4y84zAICnJwUj1N8Vb/wiHACw6Vg+DuR0/tqczK/A8ub6L31oJGZHBUIQgJc2Z6GsRtXp1yEiojtjqCGdlllQALCrh+wFdfB8KY5cugG5TIqk5u4kY8SO6I/xQf2gatJ0utvnwx+v4OqNWvR3tsPCe4YCACYM8sC8u4IAAIv/dwqVtXfuhqqsa8QLn59Ao1rAfaN88OsJA7H0wZEY5u2E0moVXvniJDQa0y4TpVQ14dS1Cmw9cQ3rf7yCkqp6k74+EVFPxlBDelrG1RzIKUWNyF1Qao2A5d+cAwDMiRmIAHcHo19DIpFg8f0jAABfZlzD+eLq25YvqKjD6v3aTVL/NGMEnOxaZ4H9MT4EwZ6OKK5S4fWdP9/2dQRBwOL/nUJ+eR0C3O2x4hdhkEgksJfL8O7ssbCzkeLg+VJ8+OMVoz8ToA1MGVdvYvOxPPxt5xnMXZ+Ou5bvx6jkb/Hw6sNI+uIk/rLzDBLWHMal0vbr+VDv10fWTSUyCkMN6Rnh64xBno5oaNIg9WzxnZ9gRtsyr+NcUTVcFDZ44Z4hXX6dyIH9cN8oH2gEYEVzSOrI33efRV2jGlFB7ng43E/vMXu5DG/+MgxSCbD1xHXsPdPx9fnk6FV8k10EW5kEq2ePhau9re6x4T7OWPrQSADAP789h5P5FZ3+LJW1jVj4eSbCX/8Oj713BP/3v9P44McrOHi+FNcrtN1rnk5yRAe7I9DdAQWV9Xh8bRqyr1d2+j2oZxMEAc/+9zii/5GKnKLbh3SivoahhvRIJBLdzt07T4m3F1R9oxorv9NubLpg2hC4Oci79Xp/uG84ZFIJUs+V4OjlGwbLHLlYhl2nCiGVAMseHmVwsb3Ige54ZvIgAMCSradxU9nQrkz29Ur8bad2HM3i+0cgPMCtXZknogJx/2gfNKoFLPw8s1Ozqo7lluOBd37A182Dpn1dFZg81BPz7grCPx4JxRfPxeDEn+/F8dfuxebnYrBtwUSMHuCCG8oGzF53FOlXyu/4HmJrVGuwJ7sQNzjeqEM7ThbguzPFKKlW4akN6SiqZBcjUQuGGmqnJdQczCnt9BRmU48N+ehILgor6+HnqsBTE4O6/XqDvZwwOyoAAJDyzbl2TfeNag2Wfa3tUvr1hIEY6dfx/iIv3zsMQ/s7oaxGhT9/la33WHV9I57feAINag1iR3jj6bsM110ikWD5o2EY4GaPvPJavLY9u8PuhCa1djxQ4vtpuN6859VXz9+FtCXT8cn8aCQ/NApPRAciKtgd7o6t4c/DyQ4bn5mAqGB3VKua8OsPf8KBHjIAvCOvf/0zfvvpCUz/10FsPXGNXSy3qK5vxN+aZ+ApbKUorKzHvI+Oid5VbCqCIOCrrOtY8FkGjlziDEEyHkMNtTPc2xmDvRzRoNZg3x26oDLzbmLWujSMSv4Wb+09j/pGdbff/6ayAWsOaMe1JMUNh8LWNBtsLpo+DA5yGU7mV2D36SK9x/6bdhXni2vg7ihH0r3Db/s6ClsZVj4eDplUgp2nCrGruUVLEAQs2XoaV2/UYoCbPd78Zdhtt1ZwdbDFO7MjIJNK8FVWAb7MuNauzLWbtZi17ijeTr0AjQA8OnYAdr042WDrjyEuClv89+ko3BPSH6omDZ7573Hd9Pie5nxxNTb+pF0osaK2EUlfnMTcDcdw7WatyDW7M0EQ0GSBRRXf3ncBpdUqBHs6YufCyfB0kuNsYRUWfHai1y/q2NK1umhTFnafLsIT//kJv/0kA/nlPf+/P/Uc3KWbDPrX3vN4J/UCYkf0xwdzx7d7/HJpDd78LqddOBjo4YBlD4/CtOH9u/zef9t5Bh/8eAUhPs7Y9eJkyEy4IeVbe8/j7dQLCPJwwN6kKbCVSVFarcI9b36PalUTlj8aillRgZ16rZXf5eDd/RfRz8EW3708BXvPFOPVbadhI5Vg83MxiBzYr1Ovs+bARbzxbQ7sbWXY+eIkDPZyAgB8fbIAr247jer6Jjjb2eBvj4zu8j5WjWoNfr/lJL7KKoBEAvwtYTSejB7YYfmGJg0OXyzD16cKsO9MMfo5yrFg6mA8OtbfbLugz12fjoPnSxE7whtjAt3wduoFNDRp4CCX4fdxwzF3YpBJfxc6q75RjbTLN1BarUK5sgE3alS4oWxovt98q1RBowHmTQrC/8UbXvG6u84XV+P+t3+AWiPgo3njMXV4f5y6VoHE94+irlGNxHEBWP5YaK/co+zwxTK88sVJFFXVQyaVYNpwLxzIKYVaI0BuI8Wzkwfhd1MHw9Gu523fQuZnzPc3Qw0ZdL64GnFvHYJcJsWx12J1A11LquvxTuoFfJ6eD7VGgEQC/GKsP8YHu2PldzkortKOhYgf5Y2lD43CADd7o943v7wW01ceRINao/uH25RqVE2Y+sYBlNU04C8zR2FOTBB+v+Ukvsy4hjB/V2xfcFenv5AamjR4ePWPOFdUjaggd5y8VgFVkwZL7g/Bc1MGd7pOao2AX3/4E45cuoERvi7Y+Jto/GP3WWxpbrkZE+iGd2aN6dLsr7Y0GgFLd2Trto34433DsWBq6wDsJrUGRy7dwM5TBfj252JU1rXvehzo4YBF04diZsQAkwaM73NK8NSGY7CVSfDdy1MQ7OmIS6U1WPK/00jP1Y4FGhPohhWPhWGYt/NtX6tG1YST+RU4da0Sg70cce9I7y5/0RdX1WPu+nScM2JA7oNhvlj5eHiX9xszRBAEzP7PURy9XI64kd5YN2ec7rF9Z4rx7CfHoRGAV+4dhoXTh5rsfc2tvlGNN7/NwQfNswAHeTrircQIhAe4IaeoGq9//TOOXNKOgfN2scPi+0MwM3yAWUIj9VwMNQYw1Bjv3n8dxIWSGqz8ZTjiR/tg3aHL+OCHy6ht0HYxTQ/pjz/eF4LhPtovmRpVE97edx7rD+dCrRFgbyvDi9OHYv6kYMhtOvfX/UubMrE9qwATB3vgs99Em+Wvzk/ScvHnr36Gh6McbyVGYM76dADAtgUTMSawc60rLX4uqMTM1YfR1DymaNpwL3w4d7zR/+gWV9Xj/rd/QLmyAQ5yGWob1JBKgBemDcGL04fCxkStI4Ig4M3vcrCmeU+r56YMwpRhXth5qhB7sotQ3mbgs5ezHWaE+uL+0T44fb0S731/CTeaHx/k5YiXYofhwVDfbn/BNKk1eOCdH3C+uAbzJwXjzw+O1D2m0Qj4/Fgelu8+h2pVE2xlEiyYOgQLpg2GnY0MgiDg6o1aZFy9iRN5N3EirwI5RVVoO8RrdlQgXn94VKd/B1tcLKnG3PXHcL2iDv0cbBHm7wYPJzk8HOVwd7SDh6McHk5yuDvK4eFoh2O55Vi89RQa1QKig92xbs44vVlv3bHjZAFe/DwTdjZS7Eua0i7gtvxOA8DKX4bjsUh/k7yvOZ0rqsJLm7J0gfHJ6ED8acYIvc10BUHAd2eK8fddZ5HX3A01JtANyQ+NQkQnu2Cp92OoMYChxnir9p3Hqn0XMMjLEVV1jSir0X6hhQe4Ycn9IZgwyMPg884VVWHp9p91f2EP6e+Ev8wchYmDPXVl1BoBV28ocb64BheKq3G+RHvb8g/c1y9MQqh/51cPNkajWoO4tw7hSpkScpkUDWoNfhnpjzd+Gd6l13sn9QL+tfc8fFwU2L1ost5gXWMcOFeCeR8dA6Cd2bQqMQLRHVzj7lp36BL+sbv99HZ3RznuH+2DB8P8EBXsrtcao1Q14b9pV/H+oUuoaF58cLi3M16+dyjiR/l0OYB+evQqXtueDTcHWxz8/TS4OrQPAkWV9Xhte7ZujNfQ/k4Y6OGAE3kVekGsxQA3ewz3ccaBnBIIAjA+qB/+/WQkvJztOlWnY7nl+M3Hx1FZ14hBXo74eF5Up1rKDl8sw3OfZKBG1YRh3k74aF4U/IxsrbxVjaoJ01d+j+Iq1W1bYlK+OYv3D16GjVSCj5+Owl1DPA2WE5tGI2D94Sv4554cNKg18HSSY8VjYZg+wrvD59Q3qrH+8BWs3n9R90fVLyL98eI9QxHo0b0WTOr5GGoMYKgx3oXiatz71iHdz8GejvhD/HDcP/rOX2CCIGDrietI+easLgzFj/KGnY0MF0pqcKm0Bg1Nhgc2Pn1XsG4dF3P55nQhfvfZCQCAs8IG+1+Z2ukvvFupNQK+PlmAiAA3BHk6dqteXxzPx8WSGiyYOrjb09jvZFN6Hv60PRtOdja4b5QPHgz3Rcwgjzu2ClXXN2LD4Vz854fLqK7XzroZ5eeC38cPN3osVVV9I6a98T1uKBuw7KGReOqu4A7LCoKAXacLsWzHz7rfKQCQ20gROsAVkQP7YWygG8YG9kN/FwUAbVB8cVMmquub4OeqwLo54+641cae7CIs2pQJVZMGYwPd8OHc8ehnRFA9U1CFpzako6RaBR8XBT56ejxCfLr+b07K7rN4/9BlDPRwwLcv3d3hwHmNRsCizVn4+mQBnO1s8OXvJupaUXuKwso6vPLFSV2XUuyI/lj+WBg8nTr3/15xVT1W7DmHrSeu687dNcQDs6MCce9I7y51+VXVN8JWKoW93HTdhR0RBAG5N2oR6O5gkfFhJVX1qG1Qd/vfJbEx1BjAUNM1z392AqeuV+DZuwdj1vgAoweJVtY1YuV3Ofj06FXcOutbYSvFkP5OGNbfGUO9nTHM2wnDvJ27PXakMwRBwC/WpiHj6k0kPzQS827zZWrNKmob4CC3MbprBtDOVvngx8tY/+MVKJv/en794VGYa8QU/OXfnMPag5cwyNMR3758d6d+vypqG7AxPQ9ymRSRA/thpJ/Lbb/MLpXW4Jn/HsflUiXsbKT45y/COhxw/d+0XCTv+BmCAMSO8Ma7s8d06cvu2s1aPLXhGC6W1MBZYYN1vx6HmMHGt7pdaB4c3KQRsOGp8ZgWcvvQqGpS49cfpiP9Sjl8XRXYtuAu+LgqjH7fFvWNahw6X4qD50sxYZAHHrplQUpjnMyvwPyPj6GspgH2tjL8+cGRmB0V0KUWvsy8m1i17wIOXShFyzeYu6Mcv4j0R+L4AN1ge0OUqiak55Yj7dINHL5YhjOFVfB1UeCrFyZ1+Q+bO2lSa/BNdhHe+/4SzhRWISLADW/PisBAD9OGjesVdUi/cgM/XS7HT1fKcaVMCQAIHeCKWVEBeDjcD84K03SJWhJDjQEMNeI6fa0S27Ouw8NJjmH9nTHM2xn+/exFHfB3U9mA7IJKTBri2StnjPQU5coGvPldjm46dssA7DtpOyj8gznjEDuy4+6H7qqqb8SizzNxIKcUAPDbKYPxh/jhur+WBUHAG9/m4N/Nu7k/ER2Ivzw8qltjmSpqG/DMf4/jWO5NyGVSrHw83KhQIAgCnvxAO4A8doQ3Ppg77s5Pan7fx947gkulSoT4OGPR9KEI9XfFADf7Tv2eK1VN2H+uBHt+LsKBcyW67h4AmHdXEP70wAijr8u+M8VY+Hkm6hrVGOHrgjVPjMGg2wSPzrp2sxZfHMvHF8evoajNPmdRwe54IioQ9432AQBk5lXgyKUyHLl0AyfzK3Rj4NqaPNQTH8+LMum/SfWNavzvxDWsO3QZV2/oT013lMvw+szReGzsgC79+yMIAvLL63BUF2Ju4NpN/U17JRLARipBo1r7eR3kMjwU5odZUQGICHDrNf/uMdQYwFBDZD6CIGD5nnN4/+BlAMBfE0bj1xM6njIOAC9sPIGdpwpx1xAPfDrfPIPC21JrtIOk32sOLtOGe+Ht2WNgbyvD//3vlK5L45V7h+GFe4aYpD71jWq8vDkL32Rrlz54bcYI/KZ5Reo72XmqAC9s7Hhw8O3kl9fikX8f0dsJvp+DLUYPcEWYvytCB7hi9IDWoFNZ24h9Z4vxTXYRDl0o1esa9nNVIMzfDXt+1n6GiYM9sPqJsZ0eO/ZJc+uXRgDuHuaFfz85Vm9PNVNoUmvwfU4pNh3Lw/5zJbpWYWeFDRqaNFDd0tUd6O6AiYM9EDPYA94uCjy1IR31jRr8IX44np/W9S1ZWlTVN+Kzo3n48Mcruv8G/Rxs8dTEYMSO7I/Xd5zRjTl8MMwXf38ktNODyhuaNNieeR3vH7qES6VKvcdkUglG+7kgepAHooPdMS7IHWqNgK0nruHz9Dy98iE+zpgdFYiEMQNMNqBdEAQ0qoUutfzeDkONAQw1ROYlCAJSvjmHdYe0webvj3S8Fk7G1XI89l4aJBJg18LJt13B2dS+yrqOP355CqomDQZ5OsLXTYHDF29AJpUg5dFQPD4uwKTvp9YI+OvOM/joSC4AbbfWLyL9MXW4V4fjY5SqJkxfeRBFVfV4KXYoXoo1fof6S6U1+OCHyzh1rRI5RdUGWyfcHeUIdHdA9vVKvceDPBxw32jtzLcwf1dIJBLsyS5E0hcnUdughn8/e6z79bjb/nfTaASs+LY16M4aH4C/Jow22zpHLQor67Dl+DVsPpav2w/Ny9kOdw32wMTBnogZ7NEuIH5xLB9//N8pyKQSbH52AsYFuXfpvUuq67HhcC4+TbuK6uZVnv1cFXjm7kFIHB+gm9ml1gh47/uLeGvfBag1Aga42eOtxAhEBXf8vkpVEz5Pz8MHP1zRtUrZyiQI83dDdLA7ogd5IHJgvw4DoyAIOJZ7E5vS87DrdKEu6ClspZgR6ocF0wbfttvuTrKvV+Jvu85glJ+r3gxGU2CoMYChhsj8BEHA33ed1a078o9HQvFEtP5ihhqNgEfeO4KT+RVIHBeAFb8Is3g9T1+rxLOfHEdh875J9rYy/PtXY7u1aOTtCIKAdYcuI6XNhqrOdjaIH+2Dh8P9MHGw/gDtlrFGAe722PvylG6vql3fqEZOUTVOX69E9vVKnLpWifPF+kFnmLeTLsiE+DgbbKnKKarGM/89jrzyWtjbyvDGL8PwYFj7LrX6RjV+v+Wkbv+438cNw/PTTNP61VlqjYCT1yrgbGeDIf2dbvvegiDg5c1Z2J5VAD9XBXa9ONmoweGCIOA/P1zGm9+d17VyDe3vhN9OGYyHI/w6DHKZeTexaFMW8sprO1zC4aayAR+n5eKjI7m6WYf9ne3wm8nBmB0V2KUxMpW1jdiWeQ2fp+cjp1g741QmlWDW+AAsih2K/s6dH4dVVFmPN7/Lwf9OXIMgaH+vDy+5By4mHLvDUGMAQw2RZQiCgL/tOosPm4PNras0f5V1HYs2ZcFBLsP3v5+qm6lkaaXVKiR9kYXcG0qsnj2201tPdMeZgipsy7yGr08W6o0B8XSSY0aoLx6O8IOrvRz3v30IjWrBrGON6hvVOFdUjStlNQjzd+v0X+kVtQ1Y+Hkmfrig3ZtpwdTBeCWudXxSRW0Dnv1vBtJzy2Erk2DFY2F4dGzPXzenRtWEh979EVfKlIgd0R//mTOuUyFMrRHw+tc/479pVwFo19FZMHUIpof079T4nBpVE5K/+hn/O9G62ObbiWMgt5HiPz9cxufpebpxTQM9HPDc3YPxWOQAkyzuKAgCMvMr8O8DF7HvrHZfOAe5DL+ZPAjP3j3ott2EtQ1NWHfoMt4/eBl1zdvjzIzwwx/ih8O/n2knezDUGMBQQ2Q5giDgLzvPYMPhXADAisdCkTg+EPWNakxfeRDXK+rw+7hheOEe8Ve/FQTB4gMmNRoBx3LLseNkAXafLsTN2tbVm21l2oGd94T0x/qn2m9R0hM0qTX457c5uq7GacO9sGrWGFTVNWLuhnRcLlXCWWGD938ViYk9dL0cQ7KvV+LRfx9Bg1qDPz84EvMn3X5WZH2jGos2ZeLbn4shkQB/emAE5k8K7tLvU9ttURzkMjSqNboBviN8XbBg6mA8EOprtqngP12+gX98cw4n8ysAaIP2oulDMSsqUK+lSaMRsDXzOt749pxuBfnIgf3w2owRRi9e2lkMNQYw1BBZliAIeP1r7VgSiQRY8WgYSmtUeOPbHPi5KrD/91NNtllpb9ao1uDHi2X4OqsA3/5cBGWDGnIbKfa+fLfJp/ya2vbM6/i//2nHJwV7OqK6XrtIp5+rAhvmRfW4dXI6479puVj61c+wlUnwv99NRJi/m8FyN5UNmP/xMZzIq4BcJsVbiRGYEebbrfe+drMWL2/OwrHcmwCAqCB3/G7aYEwd5mWR4C0IAr7JLsI/95xDbvNsrbbrkx29XI6/7z6D7OtVAAD/fvZYcv8IPBDa9cU3O4OhxgCGGiLLEwQBy3b8jI/TrkIiAeQyKVRNGqxKjEDCmK5tzmnN6hvV+OFCGXxcFGZbUdvUTl+rxHOfHEdB8/ikkb4u2DBvPLxF6lbsLkEQ8LtPT2DPz0UIdHfAzhcntRsfkl9ei7nr03G5TAkXhQ3+M2ecyVb/VmsE7DxVAP9+Dp3eFNfUGtUafJ6eh7f3XdBtjeLfz143ZdzZzgbP3zMET00MssgfJgw1BjDUEIlDEAQk72gdcxDu74ptRmwcSj1fWY0KyV/9DDtbKf4yc7TJp2xbWmVdI2a88wOu3azDjDBfrJ49RtcSkX29Ek9tOIayGhX8XBX46OmoO26y2lvVqLTjZv5zSDtuRirRruH0UuywTq8CbQoMNQYw1BCJp2Udm2+zi/Du7LG9phWC+q7MvJv45do0NGkE3fIEB8+XYsGnGVA2qBHi44yPn47qtS1SxiiprsfXJwtx91BPDBUhwDHUGMBQQ0RExmjZ+NXORornpgzGmgMXodYIuGuIB9b+KrJXbjnQGxnz/W3eVZCIiIh6qd9MGoRpw72gatLgnVTtQnmPjBmADU9FMdD0UAw1REREBkilEqx8PAI+zV1Mv5s6GP96PNzk2wCQ6fTu0VxERERm5O4ox+5Fk3H9Zh3HgvUCDDVERES34e4o7/QGniQutqERERGRVWCoMQVBAFTVYteCiIioT2Oo6a6CLGD5QGDtJLFrQkRE1KdxTE13OfsAqkqgoRpoagBs2O9KREQkBrbUdJeTN2DrCAgaoOKq2LUhIiLqsxhquksiAdwHae+XXxa3LkRERH0YQ40puAdrb29cErceREREfRhDjSl4DNbesqWGiIhINAw1puDeEmrYUkNERCQWhhpT4JgaIiIi0THUmEJL91NFnnZaNxEREVkcQ40pcFo3ERGR6BhqTIHTuomIiETHUGMqHs2hhtO6iYiIRMFQYypsqSEiIhIVQ42pcFo3ERGRqBhqTMWd3U9ERERiYqgxlZZp3ZX5nNZNREQkgi6FmjVr1iAoKAgKhQLR0dFIT0+/bfktW7YgJCQECoUCoaGh2L17t97jW7duRVxcHDw8PCCRSJCVldXhawmCgPvvvx8SiQTbt2/vSvXNg9O6iYiIRGV0qNm8eTOSkpKQnJyMEydOIDw8HPHx8SgpKTFY/siRI5g9ezbmz5+PzMxMJCQkICEhAdnZ2boySqUSkyZNwooVK+74/qtWrYJEIjG22ubHad1ERESikgiCIBjzhOjoaIwfPx6rV68GAGg0GgQEBGDhwoVYvHhxu/KJiYlQKpXYuXOn7tyECRMQERGBtWvX6pXNzc1FcHAwMjMzERER0e61srKy8OCDD+L48ePw9fXFtm3bkJCQ0Kl6V1VVwdXVFZWVlXBxcen8BzbGF3OAM18B8SlAzALzvAcREVEfYsz3t1EtNQ0NDcjIyEBsbGzrC0iliI2NRVpamsHnpKWl6ZUHgPj4+A7Ld6S2thZPPPEE1qxZAx8fH6OeazG6lhoOFiYiIrI0G2MKl5WVQa1Ww9vbW++8t7c3zp07Z/A5RUVFBssXFRUZVdGXX34ZEydOxMyZMztVXqVSQaVS6X6uqqoy6v26RDetm91PRERElmZUqBHLjh07sH//fmRmZnb6OSkpKXj99dfNWCsDOK2biIhINEZ1P3l6ekImk6G4uFjvfHFxcYddQj4+PkaVN2T//v24dOkS3NzcYGNjAxsbbRZ77LHHMHXqVIPPWbJkCSorK3VHfn5+p9+vyzitm4iISDRGhRq5XI7IyEikpqbqzmk0GqSmpiImJsbgc2JiYvTKA8DevXs7LG/I4sWLcerUKWRlZekOAHjrrbewYcMGg8+xs7ODi4uL3mF2nNZNREQkGqO7n5KSkjB37lyMGzcOUVFRWLVqFZRKJebNmwcAmDNnDgYMGICUlBQAwKJFizBlyhSsXLkSM2bMwKZNm3D8+HGsW7dO95rl5eXIy8tDQUEBACAnJweAtpWn7XGrwMBABAcHG/+pzaVlWnfxae24Gs+hYteIiIiozzB6nZrExES8+eabWLp0KSIiIpCVlYU9e/boBgPn5eWhsLBQV37ixInYuHEj1q1bh/DwcHz55ZfYvn07Ro8erSuzY8cOjBkzBjNmzAAAzJo1C2PGjGk35btX4G7dREREojB6nZreyiLr1ADAvmXAj28B438DzFhpvvchIiLqA8y2Tg11Aqd1ExERiYKhxtRaZkCx+4mIiMiiGGpMrWWtGk7rJiIisiiGGlPjtG4iIiJRMNSYWtvdutkFRUREZDEMNebQMq2bg4WJiIgshqHGHHQzoNhSQ0REZCkMNebgzpYaIiIiS2OoMQdO6yYiIrI4hhpz4LRuIiIii2OoMQdO6yYiIrI4hhpz4LRuIiIii2OoMRdO6yYiIrIohhpz4bRuIiIii2KoMRdO6yYiIrIohhpz4bRuIiIii2KoMRdO6yYiIrIohhpzcfIG5E6c1k1ERGQhDDXmIpEA7sHa++yCIiIiMjuGGnPiYGEiIiKLYagxJ07rJiIishiGGnPiqsJEREQWw1BjTi3Tutn9REREZHYMNebU0v3Ead1ERERmx1BjTk79Oa2biIjIQhhqzInTuomIiCyGocbcOK2biIjIIhhqzI3TuomIiCyCocbcOK2biIjIIhhqzI3TuomIiCyCocbcOK2biIjIIhhqzI3TuomIiCyCocbcOK2biIjIIhhqLEE3rZuhhoiIyFwYaizBnYOFiYiIzI2hxhJaZkCx+4mIiMhsGGosgasKExERmR1DjSVwWjcREZHZMdRYglN/wM5FO627LEfs2hAREVklhhpLkEgA//Ha+1fTxK0LERGRlWKosZSBE7W3Vw+LWw8iIiIrxVBjKQPv0t5ePQIIgrh1ISIiskIMNZYyYCwgswOUJZzaTUREZAYMNZZiY9dmXA27oIiIiEyNocaSdONqjohbDyIiIivEUGNJDDVERERmw1BjSQFRgNQGqMwDKvLErg0REZFVYaixJLkj4Buhvc/1aoiIiEyKocbSuF4NERGRWTDUWFrb9WqIiIjIZBhqLC0wGoAEuHEBqCkRuzZERERWg6HG0uz7Ad6jtffZWkNERGQyDDVi4NRuIiIik+tSqFmzZg2CgoKgUCgQHR2N9PT025bfsmULQkJCoFAoEBoait27d+s9vnXrVsTFxcHDwwMSiQRZWVntXuO5557D4MGDYW9vDy8vL8ycORPnzp3rSvXFx1BDRERkckaHms2bNyMpKQnJyck4ceIEwsPDER8fj5ISw+NDjhw5gtmzZ2P+/PnIzMxEQkICEhISkJ2drSujVCoxadIkrFixosP3jYyMxIYNG3D27Fl8++23EAQBcXFxUKvVxn4E8bWEmuJsoO6muHUhIiKyEhJBMG7L6OjoaIwfPx6rV68GAGg0GgQEBGDhwoVYvHhxu/KJiYlQKpXYuXOn7tyECRMQERGBtWvX6pXNzc1FcHAwMjMzERERcdt6nDp1CuHh4bh48SIGDx58x3pXVVXB1dUVlZWVcHFx6cQnNbN3x2kHC8/eDAy/T+zaEBER9UjGfH8b1VLT0NCAjIwMxMbGtr6AVIrY2FikpRleTC4tLU2vPADEx8d3WL4zlEolNmzYgODgYAQEBBgso1KpUFVVpXf0KFyvhoiIyKSMCjVlZWVQq9Xw9vbWO+/t7Y2ioiKDzykqKjKq/O38+9//hpOTE5ycnPDNN99g7969kMvlBsumpKTA1dVVd3QUfkTD9WqIiIhMqlfNfnryySeRmZmJgwcPYtiwYXj88cdRX19vsOySJUtQWVmpO/Lz8y1c2ztoaakpzAJUNaJWhYiIyBrYGFPY09MTMpkMxcXFeueLi4vh4+Nj8Dk+Pj5Glb+dllaXoUOHYsKECejXrx+2bduG2bNntytrZ2cHOzs7o9/DYtwCANdA7eaW144Bg6eJXSMiIqJezaiWGrlcjsjISKSmpurOaTQapKamIiYmxuBzYmJi9MoDwN69ezss31mCIEAQBKhUqm69jqg4tZuIiMhkjGqpAYCkpCTMnTsX48aNQ1RUFFatWgWlUol58+YBAObMmYMBAwYgJSUFALBo0SJMmTIFK1euxIwZM7Bp0yYcP34c69at071meXk58vLyUFBQAADIyckBoG3l8fHxweXLl7F582bExcXBy8sL165dw/Lly2Fvb48HHnig2xdBNAMnAqc2MdQQERGZgNGhJjExEaWlpVi6dCmKiooQERGBPXv26AYD5+XlQSptbQCaOHEiNm7ciNdeew2vvvoqhg4diu3bt2P06NG6Mjt27NCFIgCYNWsWACA5ORnLli2DQqHADz/8gFWrVuHmzZvw9vbG3XffjSNHjqB///5d/vCiaxksfO0Y0KQCbHpwdxkREVEPZ/Q6Nb1Vj1unBgAEAXhzGKAsAebtAQZ2r0uOiIjI2phtnRoyMYmE69UQERGZCEON2HTr1TDUEBERdQdDjdhaWmryfgLUTeLWhYiIqBdjqBFb/5GAwhVoVAJFJ8WuDRERUa/FUCM2qRQI5Ho1RERE3cVQ0xNwET4iIqJuY6jpCdpubqnRiFsXIiKiXoqhpifwDQNsHYH6CqD0rNi1ISIi6pUYanoCmS0QEKW9zy4oIiKiLmGo6Sm4Xg0REVG3MNT0FG0HC/eNnSuIiIhMiqGmpxgQCcjkQE0xUH5Z7NoQERH1Ogw1PYWtAvAfr71/bpe4dSEiIuqFGGp6ktBfam9PfMwuKCIiIiMx1PQkob8A5E7AjYtA7g9i14aIiKhXYajpSeycW1trjm8Qty5ERES9DENNTzNunvb27NeAskzcuhAREfUiDDU9jW844DcW0DQCWZ+JXRsiIqJeg6GmJ2pprcn4iHtBERERdRJDTU806lFA7qxdryb3kNi1ISIi6hUYanoiOycg7HHtfQ4YJiIi6hSGmp6qpQvq3E6gpkTcuhAREfUCDDU9lU8oMGAcoGkCMj8VuzZEREQ9HkNNT9bSWnPiYw4YJiIiugOGmp5s1COAnQtwMxe48r3YtSEiIurRGGp6MrkjEJaovc8Bw0RERLfFUNPTtXRB5ewGqovFrQsREVEPxlDT03mPAvyjmgcMfyJ2bYiIiHoshpregAOGiYiI7oihpjcY9QigcAUq8oBL+8WuDRERUY/EUNMb2NoDYbO09zM4YJiIiMgQhpreQjdg+BugqlDcuhAREfVADDW9Rf8RQMAEQFBzhWEiIiIDGGp6E70Bw2px60JERNTDMNT0JiNnAgo3oDIfuLhP7NoQERH1KAw1vYmtPRDxpPb+vmWAulHU6hAREfUkDDW9zeRXAAcPoOQMcPhtsWtDRETUYzDU9DaOHkB8ivb+wX8CZRfFrQ8REVEPwVDTG4U9DgyeDqhVwM6XAEEQu0ZERESiY6jpjSQS4MF/ATb2QO4PnOJNREQEhpreq18QMO1V7f3vXgNqSkStDhERkdgYanqzCQsA33CgvgLYs1js2hAREYmKoaY3k9kAD70DSGRA9v+A89+JXSMiIiLRMNT0dn4RQMwC7f1dSYCqRtTqEBERiYWhxhpMXQK4BWpXGj7wd7FrQ0REJAqGGmsgdwQeXKW9/9Na4HqGqNUhIiISA0ONtRgyHQhLBAQNsONFbqFARER9DkONNYn/B2DvDhRnA2mrxa4NERGRRTHUWBNHT22wAYDvlwM3LolbHyIiIgtiqLE24bOAQdOApnpg86+B6iKxa0RERGQRDDXWRiIBHloFOHkDJT8DH97LFhsiIuoTGGqsUb8g4OlvAfdBQEUe8GEccP2E2LUiIiIyqy6FmjVr1iAoKAgKhQLR0dFIT0+/bfktW7YgJCQECoUCoaGh2L17t97jW7duRVxcHDw8PCCRSJCVlaX3eHl5ORYuXIjhw4fD3t4egYGBePHFF1FZWdmV6vcN7sHA098BvhFAbRnw0YPAxVSxa0VERGQ2RoeazZs3IykpCcnJyThx4gTCw8MRHx+PkhLDGyoeOXIEs2fPxvz585GZmYmEhAQkJCQgOztbV0apVGLSpElYsWKFwdcoKChAQUEB3nzzTWRnZ+Ojjz7Cnj17MH/+fGOr37c4eQFP7dSOsWlUAhsfB05tEbtWREREZiERBEEw5gnR0dEYP348Vq/WThnWaDQICAjAwoULsXhx+00VExMToVQqsXPnTt25CRMmICIiAmvXrtUrm5ubi+DgYGRmZiIiIuK29diyZQt+9atfQalUwsbG5o71rqqqgqurKyorK+Hi4tKJT2pFmhqA7b8Dsr/U/hz/DyDmeXHrRERE1AnGfH8b1VLT0NCAjIwMxMbGtr6AVIrY2FikpaUZfE5aWppeeQCIj4/vsHxntXy4zgSaPs9GDjz6HyD6d9qfv30V2LsUMC7PEhER9WhGhZqysjKo1Wp4e3vrnff29kZRkeGpw0VFRUaV72w9/vrXv+LZZ5/tsIxKpUJVVZXe0adJpcB9KUDsMu3Ph98Gti/gysNERGQ1et3sp6qqKsyYMQMjR47EsmXLOiyXkpICV1dX3REQEGC5SvZUEgkw6WVg5hpAIgNObgQ+fUw7Q4qIiKiXMyrUeHp6QiaTobi4WO98cXExfHx8DD7Hx8fHqPK3U11djfvuuw/Ozs7Ytm0bbG1tOyy7ZMkSVFZW6o78/Hyj389qjfkVMGsjYGMPXDkIrJkAHF0LaNRi14yIiKjLjAo1crkckZGRSE1tnRqs0WiQmpqKmJgYg8+JiYnRKw8Ae/fu7bB8R6qqqhAXFwe5XI4dO3ZAoVDctrydnR1cXFz0Dmpj+H3Ac4eAwInamVF7/g9YHw+UnBW7ZkRERF1i9CjbpKQkzJ07F+PGjUNUVBRWrVoFpVKJefPmAQDmzJmDAQMGICUlBQCwaNEiTJkyBStXrsSMGTOwadMmHD9+HOvWrdO9Znl5OfLy8lBQUAAAyMnJAaBt5fHx8dEFmtraWnz66ad6Y2S8vLwgk8m6dxX6Kq9hwFO7gIwNwN5k4NoxYO1kYHISMPkVwMZO7BoSERF1ntAF7777rhAYGCjI5XIhKipKOHr0qO6xKVOmCHPnztUr/8UXXwjDhg0T5HK5MGrUKGHXrl16j2/YsEEA0O5ITk4WBEEQDhw4YPBxAMKVK1c6VefKykoBgFBZWdmVj2z9Kq8LwsZZgpDsoj3eHS8IV4/e+XlERERmZMz3t9Hr1PRWfXqdms4SBODMdmD3HwBlKQAJMP43QGwyYOcsdu2IiKgPMts6NWTlJBJg1CPA8+nawcQQgGP/Ad4ZC+z/G1DBwdZERNRzsaWGOnb5e+Drl4CbV7Q/S6TA0Dhg3NPAkFhAyrFMRERkXsZ8fzPU0O01NQA5u4Dj64Erh1rPuwYAY+cCY38NOBs/PZ+IiKgzGGoMYKgxgbILQMZHQNZnQN1N7TmpDTD8AW3AGRgDyB1FrSIREVkXhhoDGGpMqLEeOPOVtvUm/2jreYkU6D8K8B8H+I/XHh5DtFs0EBERdQFDjQEMNWZSfEYbbs7tAqoL2j9u5wr4R7aGnAGRgIO75etJRES9EkONAQw1FlBVAFw7rl3E79pxoCATaKprX859cHPIGac9vEcDso63vCAior6LocYAhhoRqBuBkjOtIefaMeDGxfblbBSAb0RryPEKAdwCOT6HiIgYagxhqOkhasuB6ye0Aed6c9CprzRc1sFTG276DdTeug3UHv0Gamdf2d5+/y8iIur9GGoMYKjpoTQaoPxSa2vO9Qyg/Aqg6iDotOXYH3AL0AYct0Dt4RrQek7B/85ERL0dQ40BDDW9TF0FUJEHVFxtvs0Dbl5tPddQc+fXkDsDDv0Ae3fAvp92gLL9LT8r3AA7J21Xl9yp+XDUHlxckIhIdMZ8fxu9SzeRRdi7aQ/fsPaPCYJ2nZyKPKAyX7t9g+5+823dTaChWntU5HWtDjb22nBj1xx27Jybb1vuO7d5zElbXmYLyOTaHc5b7svkzffttEFJIm0+JAAk+j9LpIDUVvu+tvbNZYiIqDMYaqj3kUi0rSwO7oBfhOEyqmqgulgbburKtbe15e1/rq8AGpTNRw2gqgEEtfY1muq0R22ZpT7ZLSTNLUcOra1Hts23cgdtSLKxaw1ONvL25yQSbQjUab7f9pytPWDnog1qds7abjs759ZznJlGRL0EQw1Zp5YvaGMJAtCkag05LUGnobr5tu3Pt5xrqgfUDdpZX7pbVev9JpU2MAlC86HRHmhzX9AAmqaWyrS2NonJRgEoXJu77m493FrvK1z1u+9auvPY4kREFsJQQ9SWRKKdVWWrABw9xKmDRgM01rYGq7b3G5RAQy3QqNTuy6VWtd6qG24516D/uVp/aD0nCNrXV1W3Oaq0t4212nJN9UBNPVBT3MUPJGkNO7bNXXRSm/aHrOW+rTaQ2rtpg5Ki+fbWn23sAIms+TnS5vuy1lupTffDlEat/fyaJm13I1fHJurRGGqIehqptHncjhMAb/HqoW7SthLVV2mn3dfdbD3qK/R/rqu4pStP2WYwt4gtThLZLeOa5PpjnWzk2nJNDc3djSptiGm51bWaAYBE2zWncDUQtprv29prW7ZsFLfcV2jHXNk2/2xjp39rigBGRAw1RNQBmU1r11JXaDTaoNCg1Lb8NCi1rT+apjaHWts1p/ezSlu+rkIbpuqbb2/9Wd1wS+gwQFC3jo3qNqH5/Tux3ICxJNJbQo5tm4HjLYPL2wwyb2kx0jR3Vwpq7bXTNLV2YWrUzd2dLV2b0O/mbNv9CUnr+0mk0B/AjtZzuvoaaPlruUYtXau6+4J+N6tEph0TZts8VszWofnnNuPFbB0Nz0q0c9Lv4rSx1x9LxhmLfR5DDRGZh1Ta+uXj1N9876PRtH6p33qrabplnNMt95saAAjtW05sbmlRkUi13XLtwlVFm58rta07jXVtblXaQNVY33q/pRWobfeg0Nzl2NLlZ+0aleZ5XalNc8CRa/+7yeStoVAX1CTtf4bQPIa+ZXzbrcFM0/oeulmLHd02B0G915fov6/UprWLVHfI9M/pQizaBMxbDqDNoP9bJwE032rUzb/rquZu6ubxfXrnmm75bG0/wy31b1eHWx5z9Qdmrjblf1WjMNQQUe8mlQKQmn+Wlq3CtOFMo2ke/1Sv3+3VWNcczDStLS2ati0ubQabt/0ibDu+SPdz8xgjyS1f6Ld+IQFtXl/Qvw9B+/46bWbOtZtZZ+B9AP1lCzTq5nFizWPD2o4Ta7hlLFnLQHy9MWUtg/WV2mvWtj4tLX7mCk10Z57DRH17hhoiIjFIpYDUXjv2hrpGELQhpqm+zSD55vstrWEaNfRaYG5tjRGE9i0rt7ZAtNzXPf9Ot5r2LT9tW30EdWsX4a3dsZombWtKu9mRt96qoTfoHzD8s0TapouuZXyZnf45acsfBLd8FkOfz1AXZtt62TmZ6792pzDUEBFR7ySRNA/8tgXsxK4M9QScn0hERERWgaGGiIiIrAJDDREREVkFhhoiIiKyCgw1REREZBUYaoiIiMgqMNQQERGRVWCoISIiIqvAUENERERWgaGGiIiIrAJDDREREVkFhhoiIiKyCgw1REREZBX6zC7dgiAAAKqqqkSuCREREXVWy/d2y/f47fSZUFNdXQ0ACAgIELkmREREZKzq6mq4urretoxE6Ez0sQIajQYFBQVwdnaGRCIx6WtXVVUhICAA+fn5cHFxMelrU3u83pbF621ZvN6WxettWV253oIgoLq6Gn5+fpBKbz9qps+01EilUvj7+5v1PVxcXPg/hQXxelsWr7dl8XpbFq+3ZRl7ve/UQtOCA4WJiIjIKjDUEBERkVVgqDEBOzs7JCcnw87OTuyq9Am83pbF621ZvN6WxettWea+3n1moDARERFZN7bUEBERkVVgqCEiIiKrwFBDREREVoGhhoiIiKwCQ003rVmzBkFBQVAoFIiOjkZ6errYVbIahw4dwkMPPQQ/Pz9IJBJs375d73FBELB06VL4+vrC3t4esbGxuHDhgjiV7eVSUlIwfvx4ODs7o3///khISEBOTo5emfr6ejz//PPw8PCAk5MTHnvsMRQXF4tU497tvffeQ1hYmG4BspiYGHzzzTe6x3mtzWv58uWQSCR46aWXdOd4zU1n2bJlkEgkekdISIjucXNea4aabti8eTOSkpKQnJyMEydOIDw8HPHx8SgpKRG7alZBqVQiPDwca9asMfj4P//5T7zzzjtYu3YtfvrpJzg6OiI+Ph719fUWrmnvd/DgQTz//PM4evQo9u7di8bGRsTFxUGpVOrKvPzyy/j666+xZcsWHDx4EAUFBXj00UdFrHXv5e/vj+XLlyMjIwPHjx/HPffcg5kzZ+Lnn38GwGttTseOHcP777+PsLAwvfO85qY1atQoFBYW6o4ff/xR95hZr7VAXRYVFSU8//zzup/VarXg5+cnpKSkiFgr6wRA2LZtm+5njUYj+Pj4CG+88YbuXEVFhWBnZyd8/vnnItTQupSUlAgAhIMHDwqCoL22tra2wpYtW3Rlzp49KwAQ0tLSxKqmVenXr5/wwQcf8FqbUXV1tTB06FBh7969wpQpU4RFixYJgsDfb1NLTk4WwsPDDT5m7mvNlpouamhoQEZGBmJjY3XnpFIpYmNjkZaWJmLN+oYrV66gqKhI7/q7uroiOjqa198EKisrAQDu7u4AgIyMDDQ2Nupd75CQEAQGBvJ6d5NarcamTZugVCoRExPDa21Gzz//PGbMmKF3bQH+fpvDhQsX4Ofnh0GDBuHJJ59EXl4eAPNf6z6zoaWplZWVQa1Ww9vbW++8t7c3zp07J1Kt+o6ioiIAMHj9Wx6jrtFoNHjppZdw1113YfTo0QC011sul8PNzU2vLK93150+fRoxMTGor6+Hk5MTtm3bhpEjRyIrK4vX2gw2bdqEEydO4NixY+0e4++3aUVHR+Ojjz7C8OHDUVhYiNdffx2TJ09Gdna22a81Qw0R6Xn++eeRnZ2t1wdOpjd8+HBkZWWhsrISX375JebOnYuDBw+KXS2rlJ+fj0WLFmHv3r1QKBRiV8fq3X///br7YWFhiI6OxsCBA/HFF1/A3t7erO/N7qcu8vT0hEwmazdiu7i4GD4+PiLVqu9ouca8/qb1wgsvYOfOnThw4AD8/f115318fNDQ0ICKigq98rzeXSeXyzFkyBBERkYiJSUF4eHhePvtt3mtzSAjIwMlJSUYO3YsbGxsYGNjg4MHD+Kdd96BjY0NvL29ec3NyM3NDcOGDcPFixfN/vvNUNNFcrkckZGRSE1N1Z3TaDRITU1FTEyMiDXrG4KDg+Hj46N3/auqqvDTTz/x+neBIAh44YUXsG3bNuzfvx/BwcF6j0dGRsLW1lbveufk5CAvL4/X20Q0Gg1UKhWvtRlMnz4dp0+fRlZWlu4YN24cnnzySd19XnPzqampwaVLl+Dr62v+3+9uDzXuwzZt2iTY2dkJH330kXDmzBnh2WefFdzc3ISioiKxq2YVqqurhczMTCEzM1MAIPzrX/8SMjMzhatXrwqCIAjLly8X3NzchK+++ko4deqUMHPmTCE4OFioq6sTuea9z+9+9zvB1dVV+P7774XCwkLdUVtbqyvz29/+VggMDBT2798vHD9+XIiJiRFiYmJErHXvtXjxYuHgwYPClStXhFOnTgmLFy8WJBKJ8N133wmCwGttCW1nPwkCr7kpvfLKK8L3338vXLlyRTh8+LAQGxsreHp6CiUlJYIgmPdaM9R007vvvisEBgYKcrlciIqKEo4ePSp2lazGgQMHBADtjrlz5wqCoJ3W/ec//1nw9vYW7OzshOnTpws5OTniVrqXMnSdAQgbNmzQlamrqxMWLFgg9OvXT3BwcBAeeeQRobCwULxK92JPP/20MHDgQEEulwteXl7C9OnTdYFGEHitLeHWUMNrbjqJiYmCr6+vIJfLhQEDBgiJiYnCxYsXdY+b81pLBEEQut/eQ0RERCQujqkhIiIiq8BQQ0RERFaBoYaIiIisAkMNERERWQWGGiIiIrIKDDVERERkFRhqiIiIyCow1BAREZFVYKghIiIiq8BQQ0RERFaBoYaIiIisAkMNERERWYX/B6cZ4VWf6IlRAAAAAElFTkSuQmCC"},"metadata":{}}]},{"cell_type":"code","source":"def root_mean_squared_error(y_true, y_pred):\n    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n","metadata":{"execution":{"iopub.status.busy":"2023-12-03T18:40:48.855939Z","iopub.execute_input":"2023-12-03T18:40:48.85649Z","iopub.status.idle":"2023-12-03T18:40:48.86059Z","shell.execute_reply.started":"2023-12-03T18:40:48.856458Z","shell.execute_reply":"2023-12-03T18:40:48.859544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# trainY.shape[1]","metadata":{"execution":{"iopub.status.busy":"2023-12-03T18:40:48.862382Z","iopub.execute_input":"2023-12-03T18:40:48.863037Z","iopub.status.idle":"2023-12-03T18:40:48.871846Z","shell.execute_reply.started":"2023-12-03T18:40:48.86301Z","shell.execute_reply":"2023-12-03T18:40:48.870747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainX, trainY = data_china()\n# def create_and_train_autoencoder(trainX, trainY, epochs, batch_size, validation_split, \n#                                  learning_rate, dropout_rate, activation, \n#                                  optimizer, loss):\n    \n#     model = Sequential()\n#     model.add(BatchNormalization(input_shape=(trainX.shape[1], trainX.shape[2])))\n#     model.add(Bidirectional(LSTM(64, activation=activation, return_sequences=True)))\n#     model.add(LSTM(32, activation=activation, return_sequences=False))\n#     model.add(Dropout(dropout_rate))\n#     model.add(Dense(trainY.shape[1]))\n\n#     model.compile(optimizer=optimizer, loss=loss)\n#     model.summary()\n\n\n#     # fit the model\n#     history = model.fit(trainX, trainY, epochs=epochs, batch_size=batch_size, validation_split=validation_split, verbose=1)\n\n#     plt.plot(history.history['loss'], label='Training loss')\n#     plt.plot(history.history['val_loss'], label='Validation loss')\n\n#     return model, history\n\ndef create_and_train_autoencoder(trainX, trainY, epochs, batch_size, validation_split, \n                                 learning_rate, dropout_rate, lstm_units, activation, \n                                 optimizer, loss):\n    \n    model = Sequential()\n    model.add(BatchNormalization(input_shape=(trainX.shape[1], trainX.shape[2])))\n    model.add(Bidirectional(LSTM(lstm_units, activation=activation, return_sequences=True)))\n    model.add(LSTM(lstm_units // 2, activation=activation, return_sequences=True))\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(trainY.shape[1]))\n\n\n\n    \n    model.compile(optimizer=optimizer, loss=loss)\n    model.summary()\n\n    # Define early stopping\n    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n\n    # fit the model with early stopping\n    history = model.fit(trainX, trainY, epochs=epochs, batch_size=batch_size, \n                        validation_split=validation_split, verbose=1, callbacks=[early_stopping])\n   \n    plt.plot(history.history['loss'], label='Training loss')\n    plt.plot(history.history['val_loss'], label='Validation loss')\n\n    return model, history\n\n\n# Example usage:\n# Adjust hyperparameters as needed\n# autoencoder_model, training_history = create_and_train_autoencoder(trainX, trainY, \n#                                                                     epochs=500, batch_size=32, \n#                                                                     validation_split=0.1, \n#                                                                     learning_rate=best_hyperparameters[0], \n#                                                                     dropout_rate=best_hyperparameters[1], \n#                                                                     lstm_units=int(best_hyperparameters[2]),\n#                                                                     activation='relu', \n#                                                                     optimizer='adam', \n#                                                                     loss='mse')\n\nautoencoder_model, training_history = create_and_train_autoencoder(trainX, trainY, \n                                                                    epochs=100, batch_size=16, \n                                                                    validation_split=0.1, \n                                                                    learning_rate=0.01, \n                                                                    dropout_rate=0.1,\n                                                                    lstm_units= 64,\n                                                                    activation='sigmoid', \n                                                                    optimizer='adam', \n                                                                    loss= 'mae',\n                                                                    \n                                                                  )","metadata":{"execution":{"iopub.status.busy":"2023-12-03T18:40:48.873077Z","iopub.execute_input":"2023-12-03T18:40:48.873343Z","iopub.status.idle":"2023-12-03T18:40:56.321274Z","shell.execute_reply.started":"2023-12-03T18:40:48.873322Z","shell.execute_reply":"2023-12-03T18:40:56.320228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\ndef harmonic_search(objective_function, search_space, num_iterations):\n    # Initialize parameters\n    num_harmonics = 10\n    pitch_adjustment_rate = 0.01\n\n    # Initialize random solutions within the search space\n    solutions = np.random.uniform(low=search_space[:, 0], high=search_space[:, 1], size=(num_harmonics, len(search_space)))\n\n    for iteration in range(num_iterations):\n        # Evaluate the performance of each solution\n        scores = [objective_function(solution) for solution in solutions]\n\n        # Select the top-performing solutions as parents\n        parents = solutions[np.argsort(scores)[:2]]\n\n        # Generate new candidate solutions by combining and modifying parents\n        new_solutions = parents[0] + np.random.uniform(low=-pitch_adjustment_rate, high=pitch_adjustment_rate, size=parents.shape)\n\n        # Clip new solutions to the search space\n        new_solutions = np.clip(new_solutions, search_space[:, 0], search_space[:, 1])\n\n        # Replace the worst solutions with the new ones\n        worst_index = np.argmax(scores)\n        solutions[worst_index] = new_solutions[0]  # Take the first parent as the new solution\n\n    # Return the best solution found\n    best_solution = solutions[np.argmin(scores)]\n    return best_solution\n\n# Example usage:\n# Define the search space for hyperparameters\nsearch_space = np.array([\n    [0.001, 0.1],  # Learning Rate\n    [0.1, 0.9],    # Dropout Rate\n    [16, 64],     # Number of LSTM units\n])\n\n# Define your objective function (replace with your actual training and evaluation logic)\ndef objective_function(hyperparameters):\n    learning_rate, dropout_rate, lstm_units = hyperparameters\n    \n    try:\n        # Create and train LSTM model with the given hyperparameters\n        # Return the performance metric to be minimized (e.g., validation loss)\n        autoencoder_model, training_history = create_and_train_autoencoder(trainX, trainY, \n                                                                            epochs=100, batch_size=16, \n                                                                            validation_split=0.1, \n                                                                            learning_rate=learning_rate, \n                                                                            dropout_rate=dropout_rate, \n                                                                            lstm_units=int(lstm_units),\n                                                                            activation='sigmoid', \n                                                                            optimizer='adam')\n        \n        # Retrieve the performance metric (e.g., validation loss) from the training history\n        metric = min(training_history.history['val_loss'])  # Assuming 'val_loss' is the relevant metric\n\n        return metric\n    except Exception as e:\n        # Return a large value in case of an error\n        return float('inf')\n\n# Run harmonic search\nbest_hyperparameters = harmonic_search(objective_function, search_space, num_iterations=50)\n\n# Print or log the best hyperparameters\nprint(\"Best Hyperparameters:\")\nprint(\"Learning Rate:\", best_hyperparameters[0])\nprint(\"Dropout Rate:\", best_hyperparameters[1])\nprint(\"LSTM Units:\", int(best_hyperparameters[2]))\n\n# Update your LSTM model with the best hyperparameters\nautoencoder_model, training_history = create_and_train_autoencoder(trainX, trainY, \n                                                                    epochs=100, batch_size=16, \n                                                                    validation_split=0.1, \n                                                                    learning_rate=best_hyperparameters[0], \n                                                                    dropout_rate=best_hyperparameters[1], \n                                                                    lstm_units=int(best_hyperparameters[2]),\n                                                                    activation='sigmoid', \n                                                                    optimizer='adam', \n                                                                    loss='mae',\n                                                                    \n                                                                  )\n\n# Optionally, you can also print or log other relevant information, such as the best performance metric\nbest_metric = objective_function(best_hyperparameters)\nprint(\"Best Performance Metric:\", best_metric)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-03T18:40:56.324666Z","iopub.execute_input":"2023-12-03T18:40:56.325943Z","iopub.status.idle":"2023-12-03T18:41:08.408575Z","shell.execute_reply.started":"2023-12-03T18:40:56.325879Z","shell.execute_reply":"2023-12-03T18:41:08.407902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#     from sklearn.preprocessing import MinMaxScaler\n#     df = pd.read_csv(CFG[1])\n# #     x = df.drop(columns=['id','ID'])\n#     # Assuming your dataset is stored in a DataFrame named 'df'\n#     # If not, replace 'df' with the actual name of your DataFrame\n\n#     # Extract the target variable\n#     target_variable = 'Effort'\n#     y = df[target_variable]\n\n#     # Extract features (excluding target variable)\n#     X = df.drop(columns=['id','ID', 'Effort'])\n\n#     # Normalize the dataset\n#     scaler = MinMaxScaler()\n#     X_normalized = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n\n#     # Split the dataset into training and testing sets\n#     X_train, X_test, y_train, y_test = train_test_split(X_normalized, y, test_size=0.2, random_state=42)\n\n#     # Check the shapes of the resulting sets\n#     print(\"X_train shape:\", X_train.shape)\n#     print(\"X_test shape:\", X_test.shape)\n#     print(\"y_train shape:\", y_train.shape)\n#     print(\"y_test shape:\", y_test.shape)\n# df","metadata":{"execution":{"iopub.status.busy":"2023-12-03T18:41:08.409879Z","iopub.execute_input":"2023-12-03T18:41:08.410479Z","iopub.status.idle":"2023-12-03T18:41:08.415774Z","shell.execute_reply.started":"2023-12-03T18:41:08.410454Z","shell.execute_reply":"2023-12-03T18:41:08.414586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn import datasets\n# from sklearn.model_selection import train_test_split\n# from sklearn.svm import SVR\n# from sklearn.metrics import  mean_absolute_error\n\n\n\n# # Create an SVM regressor\n# svm_regressor = SVR(kernel='linear', C=1)\n\n# svm_regressor.fit(X_train, y_train)\n\n# # Make predictions on the test set\n# y_pred = svm_regressor.predict(X_test)\n\n# # Evaluate mean squared error\n# mae = mean_absolute_error(y_test, y_pred)\n# print(f\"Mean Squared Error: {mae}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-12-03T18:41:08.41725Z","iopub.execute_input":"2023-12-03T18:41:08.417544Z","iopub.status.idle":"2023-12-03T18:41:08.425814Z","shell.execute_reply.started":"2023-12-03T18:41:08.417521Z","shell.execute_reply":"2023-12-03T18:41:08.424822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# trainX.shape","metadata":{"execution":{"iopub.status.busy":"2023-12-03T18:41:08.427028Z","iopub.execute_input":"2023-12-03T18:41:08.427281Z","iopub.status.idle":"2023-12-03T18:41:08.435802Z","shell.execute_reply.started":"2023-12-03T18:41:08.42726Z","shell.execute_reply":"2023-12-03T18:41:08.434271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Attention, Activation\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport matplotlib.pyplot as plt\ndf = pd.read_csv(CFG[1])\n# Load your dataset (assuming it's in a DataFrame named 'df')\n# Replace 'Effort' with the actual column name you want to predict\ntarget_column = 'Effort'\n\n# Separate features (X) and target variable (y)\nX = df.drop(target_column, axis=1)\ny = df[target_column]\n\n# Data Preprocessing\nscaler_X = MinMaxScaler(feature_range=(0, 1))\nscaler_y = MinMaxScaler(feature_range=(0, 1))\n\nX_normalized = scaler_X.fit_transform(X.values)\ny_normalized = scaler_y.fit_transform(y.values.reshape(-1, 1))\n\n# Reshape X for LSTM input (assuming you have a sequence length)\nsequence_length = 1  # Adjust based on your dataset\nX_reshaped = X_normalized.reshape((X_normalized.shape[0], sequence_length, X_normalized.shape[1]))\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_reshaped, y_normalized, test_size=0.2, random_state=42)\n\n# Model Construction and Training\ninp = Input(shape=(X_reshaped.shape[1], X_reshaped.shape[2]))\ne = Sequential()(inp)\ng = LSTM(units=64, return_sequences=True)(e)\nh = LSTM(units=32, return_sequences=True)(g)\ns = Attention(use_scale=True)([h, h])\nf = Activation('relu')(s)\nout = Dense(1)(f)  # Assuming a regression task\n\nmodel = Model(inputs=inp, outputs=out)\nmodel.compile(optimizer='adam', loss='mean_squared_error')  # You can use other loss functions for regression\n\n# Implement Early Stopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n\n# Train the model and monitor the training history\nhistory = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.1, callbacks=[early_stopping])\n\n# Plot the training and validation loss\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\nplt.show()\n\n\n# Prediction\ny_pred = model.predict(X_test)\n\n# Rescale predictions to the original scale\ny_pred_rescaled = scaler_y.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n\n# Rescale true labels to the original scale\ny_test_rescaled = scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten()\n\n# Evaluate mean absolute error\nmae = np.mean(np.abs(y_test_rescaled - y_pred_rescaled))\nprint(f\"Mean Absolute Error: {mae}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-12-03T18:41:08.437576Z","iopub.execute_input":"2023-12-03T18:41:08.438828Z","iopub.status.idle":"2023-12-03T18:41:22.808409Z","shell.execute_reply.started":"2023-12-03T18:41:08.438755Z","shell.execute_reply":"2023-12-03T18:41:22.807583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(CFG[1])\n# df = df.drop(columns=['id','ID'])\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2023-12-03T18:41:22.810752Z","iopub.execute_input":"2023-12-03T18:41:22.811034Z","iopub.status.idle":"2023-12-03T18:41:22.823386Z","shell.execute_reply.started":"2023-12-03T18:41:22.811013Z","shell.execute_reply":"2023-12-03T18:41:22.822388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Attention, Activation\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.metrics import mean_absolute_error\nimport optuna\n\ndf = pd.read_csv(CFG[7])\n# df = df.drop(columns=['id','ID'])\n# Load your dataset (assuming it's in a DataFrame named 'df')\n# Replace 'Effort' with the actual column name you want to predict\ntarget_column = 'Effort'\n\n# Separate features (X) and target variable (y)\nX = df.drop(target_column, axis=1)\ny = df[target_column]\n\n# Data Preprocessing\nscaler_X = MinMaxScaler(feature_range=(0, 1))\nscaler_y = MinMaxScaler(feature_range=(0, 1))\n\nX_normalized = scaler_X.fit_transform(X.values)\ny_normalized = scaler_y.fit_transform(y.values.reshape(-1, 1))\n\n# Reshape X for LSTM input (assuming you have a sequence length)\nsequence_length = 1  # Adjust based on your dataset\nX_reshaped = X_normalized.reshape((X_normalized.shape[0], sequence_length, X_normalized.shape[1]))\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_reshaped, y_normalized, test_size=0.2, random_state=42)\n\n# Model Construction and Training\ndef build_model(units1, units2, learning_rate):\n    inp = Input(shape=(X_reshaped.shape[1], X_reshaped.shape[2]))\n    e = Sequential()(inp)\n    g = LSTM(units=units1, return_sequences=True)(e)\n    h = LSTM(units=units2, return_sequences=True)(g)\n    s = Attention(use_scale=True)([h, h])\n    f = Activation('relu')(s)\n    out = Dense(1)(f)  # Assuming a regression task\n\n    model = Model(inputs=inp, outputs=out)\n    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n    model.compile(optimizer=optimizer, loss='mean_squared_error')  # You can use other loss functions for regression\n    \n    return model\n\n# Implement Early Stopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n\n# Objective function for Optuna\ndef objective(trial):\n    units1 = trial.suggest_int('units1', 32, 128)\n    units2 = trial.suggest_int('units2', 16, 64)\n    learning_rate = trial.suggest_loguniform('learning_rate', 0.001, 0.1)\n\n    model = build_model(units1=units1, units2=units2, learning_rate=learning_rate)\n    \n    history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.1, callbacks=[early_stopping], verbose=0)\n    \n    return mean_absolute_error(scaler_y.inverse_transform(y_test.reshape(-1, 1)), scaler_y.inverse_transform(model.predict(X_test).reshape(-1, 1)))\n\n# Perform hyperparameter tuning with Optuna\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=10)\n\n# Get the best hyperparameters\nbest_params = study.best_params\n\n# Train the final model with the best hyperparameters\nbest_model = build_model(**best_params)\nbest_model.fit(X_train, y_train, epochs=200, batch_size=32, validation_split=0.1, callbacks=[early_stopping])\n\n# Plot the training and validation loss\nplt.plot(best_model.history.history['loss'], label='Train Loss')\nplt.plot(best_model.history.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\nplt.show()\n\n# Prediction\ny_pred = best_model.predict(X_test)\n\n# Rescale predictions to the original scale\ny_pred_rescaled = scaler_y.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n\n# Rescale true labels to the original scale\ny_test_rescaled = scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten()\n\n# Evaluate mean absolute error\nmae = mean_absolute_error(y_test_rescaled, y_pred_rescaled)\nprint(f\"Mean Absolute Error: {mae}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-12-03T18:41:22.824773Z","iopub.execute_input":"2023-12-03T18:41:22.825062Z","iopub.status.idle":"2023-12-03T18:42:31.7374Z","shell.execute_reply.started":"2023-12-03T18:41:22.82504Z","shell.execute_reply":"2023-12-03T18:42:31.736504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Attention, Activation\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\n\n# Load your dataset (assuming it's in a DataFrame named 'df')\n# Replace 'Effort' with the actual column name you want to predict\ntarget_column = 'Effort'\n\n# Separate features (X) and target variable (y)\nX = df.drop(target_column, axis=1)\ny = df[target_column]\n\n# Data Preprocessing\nscaler_X = MinMaxScaler(feature_range=(0, 1))\nscaler_y = MinMaxScaler(feature_range=(0, 1))\n\nX_normalized = scaler_X.fit_transform(X.values)\ny_normalized = scaler_y.fit_transform(y.values.reshape(-1, 1))\n\n# Reshape X for LSTM input (assuming you have a sequence length)\nsequence_length = 1  # Adjust based on your dataset\nX_reshaped = X_normalized.reshape((X_normalized.shape[0], sequence_length, X_normalized.shape[1]))\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_reshaped, y_normalized, test_size=0.2, random_state=42)\n\n# Model Construction and Training\ninp = Input(shape=(X_reshaped.shape[1], X_reshaped.shape[2]))\ne = Sequential()(inp)\ng = LSTM(units=64, return_sequences=True)(e)\nh = LSTM(units=32, return_sequences=True)(g)\ns = Attention(use_scale=True)([h, h])\nf = Activation('relu')(s)\nout = Dense(1)(f)  # Assuming a regression task\n\nmodel = Model(inputs=inp, outputs=out)\nmodel.compile(optimizer='adam', loss='mean_squared_error')  # You can use other loss functions for regression\n\n# Implement Early Stopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n\n# Train the model and monitor the training history\nhistory = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.1, callbacks=[early_stopping])\n\n# Plot the training and validation loss\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\nplt.show()\n\n# Prediction\ny_pred = model.predict(X_test)\n\n# Rescale predictions to the original scale\ny_pred_rescaled = scaler_y.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n\n# Rescale true labels to the original scale\ny_test_rescaled = scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten()\n\n# Evaluate root mean squared error (RMSE)\nrmse = np.sqrt(mean_squared_error(y_test_rescaled, y_pred_rescaled))\nprint(f\"Root Mean Squared Error (RMSE): {rmse}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-12-03T18:42:31.739012Z","iopub.execute_input":"2023-12-03T18:42:31.739328Z","iopub.status.idle":"2023-12-03T18:42:37.258957Z","shell.execute_reply.started":"2023-12-03T18:42:31.7393Z","shell.execute_reply":"2023-12-03T18:42:37.258028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Attention, Activation\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.metrics import mean_squared_error\nimport optuna\n\n# Load your dataset (assuming it's in a DataFrame named 'df')\n# Replace 'Effort' with the actual column name you want to predict\ntarget_column = 'Effort'\n\n# Separate features (X) and target variable (y)\nX = df.drop(target_column, axis=1)\ny = df[target_column]\n\n# Data Preprocessing\nscaler_X = MinMaxScaler(feature_range=(0, 1))\nscaler_y = MinMaxScaler(feature_range=(0, 1))\n\nX_normalized = scaler_X.fit_transform(X.values)\ny_normalized = scaler_y.fit_transform(y.values.reshape(-1, 1))\n\n# Reshape X for LSTM input (assuming you have a sequence length)\nsequence_length = 1  # Adjust based on your dataset\nX_reshaped = X_normalized.reshape((X_normalized.shape[0], sequence_length, X_normalized.shape[1]))\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_reshaped, y_normalized, test_size=0.2, random_state=42)\n\n# Model Construction and Training\ndef build_model(units1, units2, learning_rate):\n    inp = Input(shape=(X_reshaped.shape[1], X_reshaped.shape[2]))\n    e = Sequential()(inp)\n    g = LSTM(units=units1, return_sequences=True)(e)\n    h = LSTM(units=units2, return_sequences=True)(g)\n    s = Attention(use_scale=True)([h, h])\n    f = Activation('relu')(s)\n    out = Dense(1)(f)  # Assuming a regression task\n\n    model = Model(inputs=inp, outputs=out)\n    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n    model.compile(optimizer=optimizer, loss='mean_squared_error')  # You can use other loss functions for regression\n    \n    return model\n\n# Implement Early Stopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n\n# Objective function for Optuna\ndef objective(trial):\n    units1 = trial.suggest_int('units1', 32, 128)\n    units2 = trial.suggest_int('units2', 16, 64)\n    learning_rate = trial.suggest_loguniform('learning_rate', 0.001, 0.1)\n\n    model = build_model(units1=units1, units2=units2, learning_rate=learning_rate)\n    \n    history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.1, callbacks=[early_stopping], verbose=0)\n    \n    return mean_absolute_error(scaler_y.inverse_transform(y_test.reshape(-1, 1)), scaler_y.inverse_transform(model.predict(X_test).reshape(-1, 1)))\n\n# Perform hyperparameter tuning with Optuna\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=10)\n\n# Get the best hyperparameters\nbest_params = study.best_params\n\n# Train the final model with the best hyperparameters\nbest_model = build_model(**best_params)\nbest_model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.1, callbacks=[early_stopping])\n\n# Plot the training and validation loss\nplt.plot(best_model.history.history['loss'], label='Train Loss')\nplt.plot(best_model.history.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\nplt.show()\n\n# Prediction\ny_pred = best_model.predict(X_test)\n\n# Rescale predictions to the original scale\ny_pred_rescaled = scaler_y.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n\n# Rescale true labels to the original scale\ny_test_rescaled = scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten()\n\n# Evaluate root mean squared error (RMSE)\nrmse = np.sqrt(mean_squared_error(y_test_rescaled, y_pred_rescaled))\nprint(f\"Root Mean Squared Error (RMSE): {rmse}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-12-03T18:42:37.260272Z","iopub.execute_input":"2023-12-03T18:42:37.260566Z","iopub.status.idle":"2023-12-03T18:43:50.480486Z","shell.execute_reply.started":"2023-12-03T18:42:37.260543Z","shell.execute_reply":"2023-12-03T18:43:50.479477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Attention, Activation\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport optuna\n\n# Load your dataset (assuming it's in a DataFrame named 'df')\n# Replace 'Effort' with the actual column name you want to predict\ntarget_column = 'Effort'\n\n# Separate features (X) and target variable (y)\nX = df.drop(target_column, axis=1)\ny = df[target_column]\n\n# Data Preprocessing\nscaler_X = MinMaxScaler(feature_range=(0, 1))\nscaler_y = MinMaxScaler(feature_range=(0, 1))\n\nX_normalized = scaler_X.fit_transform(X.values)\ny_normalized = scaler_y.fit_transform(y.values.reshape(-1, 1))\n\n# Reshape X for LSTM input (assuming you have a sequence length)\nsequence_length = 1  # Adjust based on your dataset\nX_reshaped = X_normalized.reshape((X_normalized.shape[0], sequence_length, X_normalized.shape[1]))\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_reshaped, y_normalized, test_size=0.2, random_state=42)\n\n# Model Construction and Training\ndef build_model(units1, units2, learning_rate):\n    inp = Input(shape=(X_reshaped.shape[1], X_reshaped.shape[2]))\n    e = Sequential()(inp)\n    g = LSTM(units=units1, return_sequences=True)(e)\n    h = LSTM(units=units2, return_sequences=True)(g)\n    s = Attention(use_scale=True)([h, h])\n    f = Activation('relu')(s)\n    out = Dense(1)(f)  # Assuming a regression task\n\n    model = Model(inputs=inp, outputs=out)\n    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n    model.compile(optimizer=optimizer, loss='mean_squared_error')  # You can use other loss functions for regression\n    \n    return model\n\n# Implement Early Stopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n\n# Objective function for Optuna\ndef objective(trial):\n    units1 = trial.suggest_int('units1', 32, 128)\n    units2 = trial.suggest_int('units2', 16, 64)\n    learning_rate = trial.suggest_loguniform('learning_rate', 0.001, 0.1)\n\n    model = build_model(units1=units1, units2=units2, learning_rate=learning_rate)\n    \n    history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.1, callbacks=[early_stopping], verbose=0)\n    \n    return mean_absolute_error(scaler_y.inverse_transform(y_test.reshape(-1, 1)), scaler_y.inverse_transform(model.predict(X_test).reshape(-1, 1)))\n\n# Perform hyperparameter tuning with Optuna\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=10)\n\n# Get the best hyperparameters\nbest_params = study.best_params\n\n# Train the final model with the best hyperparameters\nbest_model = build_model(**best_params)\nbest_model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.1, callbacks=[early_stopping])\n\n# Plot the training and validation loss\nplt.plot(best_model.history.history['loss'], label='Train Loss')\nplt.plot(best_model.history.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\nplt.show()\n\n# Prediction\ny_pred = best_model.predict(X_test)\n\n# Rescale predictions to the original scale\ny_pred_rescaled = scaler_y.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n\n# Rescale true labels to the original scale\ny_test_rescaled = scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten()\n\n# Evaluate R-squared\nr_squared = r2_score(y_test_rescaled, y_pred_rescaled)\nprint(f\"R-squared: {r_squared}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-12-03T18:43:50.483512Z","iopub.execute_input":"2023-12-03T18:43:50.483912Z","iopub.status.idle":"2023-12-03T18:44:59.982447Z","shell.execute_reply.started":"2023-12-03T18:43:50.483883Z","shell.execute_reply":"2023-12-03T18:44:59.981439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}